{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12304923,"sourceType":"datasetVersion","datasetId":7756022}],"dockerImageVersionId":31040,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"72bb890d-130d-4f05-917b-d6d408e465da","cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nfrom skimage.transform import resize\nfrom pathlib import Path\nimport nibabel as nib\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F","metadata":{"id":"72bb890d-130d-4f05-917b-d6d408e465da","executionInfo":{"status":"ok","timestamp":1750915382689,"user_tz":300,"elapsed":3062,"user":{"displayName":"Yazan Bakdash","userId":"15411906646133184411"}},"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T03:56:33.564839Z","iopub.execute_input":"2025-07-03T03:56:33.565375Z","iopub.status.idle":"2025-07-03T03:56:33.569451Z","shell.execute_reply.started":"2025-07-03T03:56:33.565349Z","shell.execute_reply":"2025-07-03T03:56:33.568790Z"}},"outputs":[],"execution_count":1},{"id":"c011f188","cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c011f188","executionInfo":{"status":"ok","timestamp":1750915382742,"user_tz":300,"elapsed":54,"user":{"displayName":"Yazan Bakdash","userId":"15411906646133184411"}},"outputId":"0b2f6213-7cc6-4b25-c652-b495649284fa","trusted":true,"execution":{"iopub.status.busy":"2025-07-03T03:56:33.571991Z","iopub.execute_input":"2025-07-03T03:56:33.572193Z","iopub.status.idle":"2025-07-03T03:56:33.584711Z","shell.execute_reply.started":"2025-07-03T03:56:33.572177Z","shell.execute_reply":"2025-07-03T03:56:33.583912Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":2},{"id":"5f02cacf-07ac-4ef5-a998-f84999c5c751","cell_type":"code","source":"data_dir = Path(\"/kaggle/input/brainmetshare-t1-gd/BrainMetShare_t1_gd\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T03:56:33.585796Z","iopub.execute_input":"2025-07-03T03:56:33.586006Z","iopub.status.idle":"2025-07-03T03:56:33.597292Z","shell.execute_reply.started":"2025-07-03T03:56:33.585988Z","shell.execute_reply":"2025-07-03T03:56:33.596481Z"}},"outputs":[],"execution_count":3},{"id":"5b78160f","cell_type":"markdown","source":"### U-NET Model","metadata":{"id":"5b78160f"}},{"id":"b597948f","cell_type":"markdown","source":"- Consider adding batchnorm to doubleConv","metadata":{"id":"b597948f"}},{"id":"f49d069e","cell_type":"code","source":"class DoubleConv(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n\n    def forward(self, x):\n        x = F.relu(self.bn1(self.conv1(x)), inplace=True)\n        x = F.relu(self.bn2(self.conv2(x)), inplace=True)\n        return x\n","metadata":{"id":"f49d069e","executionInfo":{"status":"ok","timestamp":1750915383447,"user_tz":300,"elapsed":23,"user":{"displayName":"Yazan Bakdash","userId":"15411906646133184411"}},"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T03:56:33.598273Z","iopub.execute_input":"2025-07-03T03:56:33.598547Z","iopub.status.idle":"2025-07-03T03:56:33.610986Z","shell.execute_reply.started":"2025-07-03T03:56:33.598523Z","shell.execute_reply":"2025-07-03T03:56:33.610374Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":4},{"id":"446bbc76","cell_type":"markdown","source":"- consider adding other encoder layer","metadata":{"id":"446bbc76"}},{"id":"debe009b","cell_type":"code","source":"class UNETModel(nn.Module):\n    def __init__(self, in_channels=1, out_channels=1):\n        super().__init__()\n        # Down\n        self.down1 = DoubleConv(in_channels,64)\n        self.down2 = DoubleConv(64,128)\n        self.down3 = DoubleConv(128,256)\n        self.bottleneck = DoubleConv(256,512)\n\n        self.pool = nn.MaxPool2d(2)\n\n        # Up\n        self.up1 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n        self.right1 = DoubleConv(512, 256) # 256 (up) + 256 (encoder3)\n        self.up2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n        self.right2 = DoubleConv(256, 128) # 128 (up) + 128 (encoder2)\n        self.up3 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n        self.right3 = DoubleConv(128, 64) # 64 (up) + 64 (encoder1)\n\n        # Out\n        self.out = nn.Conv2d(64, out_channels, kernel_size=1)\n\n    def forward(self, x):\n        # Down\n        encoder1 = self.down1(x)\n        encoder2 = self.down2(self.pool(encoder1))\n        encoder3 = self.down3(self.pool(encoder2))\n\n        # Bottleneck\n        bneck = self.bottleneck(self.pool(encoder3))\n\n        # Up\n        decoder1 = self.up1(bneck)\n        decoder1 = self.right1(torch.cat([decoder1, encoder3], dim=1))\n        decoder2 = self.up2(decoder1)\n        decoder2 = self.right2(torch.cat([decoder2, encoder2], dim=1))\n        decoder3 = self.up3(decoder2)\n        decoder3 = self.right3(torch.cat([decoder3, encoder1], dim=1))\n\n        # Out\n        out = self.out(decoder3)\n        return out\n","metadata":{"id":"debe009b","executionInfo":{"status":"ok","timestamp":1750915383493,"user_tz":300,"elapsed":38,"user":{"displayName":"Yazan Bakdash","userId":"15411906646133184411"}},"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T03:56:33.612180Z","iopub.execute_input":"2025-07-03T03:56:33.612411Z","iopub.status.idle":"2025-07-03T03:56:33.627151Z","shell.execute_reply.started":"2025-07-03T03:56:33.612395Z","shell.execute_reply":"2025-07-03T03:56:33.626418Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":5},{"id":"bf30c77e-f03d-4f5a-93c7-d3bca384d135","cell_type":"code","source":"class DoubleConv3D(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.conv1 = nn.Conv3d(in_channels, out_channels, kernel_size=3, padding=1)\n        self.bn1 = nn.BatchNorm3d(out_channels)\n\n        self.conv2 = nn.Conv3d(out_channels, out_channels, kernel_size=3, padding=1)\n        self.bn2 = nn.BatchNorm3d(out_channels)\n\n    def forward(self, x):\n        x = F.relu(self.bn1(self.conv1(x)), inplace=True)\n        x = F.relu(self.bn2(self.conv2(x)), inplace=True)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T03:56:33.627790Z","iopub.execute_input":"2025-07-03T03:56:33.627993Z","iopub.status.idle":"2025-07-03T03:56:33.639075Z","shell.execute_reply.started":"2025-07-03T03:56:33.627978Z","shell.execute_reply":"2025-07-03T03:56:33.638500Z"}},"outputs":[],"execution_count":6},{"id":"360ff44a-2602-4dd8-8ec6-8c7eb3c7795e","cell_type":"code","source":"class UNET3D(nn.Module):\n    def __init__(self, in_channels=1, out_channels=1):\n        super().__init__()\n        self.pool = nn.MaxPool3d(2)\n        # Down\n        self.down1 = DoubleConv3D(in_channels,32)\n        self.down2 = DoubleConv3D(32,64)\n        self.down3 = DoubleConv3D(64,128)\n\n        #  Bottleneck\n        self.bottleneck = DoubleConv3D(128,256)\n\n        # Up\n        self.up1 = nn.ConvTranspose3d(256, 128, kernel_size=2, stride=2)\n        self.right1 = DoubleConv3D(256, 128)\n        self.up2 = nn.ConvTranspose3d(128, 64, kernel_size=2, stride=2)\n        self.right2 = DoubleConv3D(128, 64)\n        self.up3 = nn.ConvTranspose3d(64, 32, kernel_size=2, stride=2)\n        self.right3 = DoubleConv3D(64, 32)\n\n        # Out\n        self.out = nn.Conv3d(32, out_channels, kernel_size=1)\n\n    def forward(self, x):\n        # Down\n        encoder1 = self.down1(x)\n        encoder2 = self.down2(self.pool(encoder1))\n        encoder3 = self.down3(self.pool(encoder2))\n\n        # Bottleneck\n        bneck = self.bottleneck(self.pool(encoder3))\n\n        # Up\n        decoder1 = self.up1(bneck)\n        decoder1 = self.right1(torch.cat([decoder1, encoder3], dim=1))\n        decoder2 = self.up2(decoder1)\n        decoder2 = self.right2(torch.cat([decoder2, encoder2], dim=1))\n        decoder3 = self.up3(decoder2)\n        decoder3 = self.right3(torch.cat([decoder3, encoder1], dim=1))\n\n        # Out\n        out = self.out(decoder3)\n        return out\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T03:56:33.639846Z","iopub.execute_input":"2025-07-03T03:56:33.640336Z","iopub.status.idle":"2025-07-03T03:56:33.656227Z","shell.execute_reply.started":"2025-07-03T03:56:33.640308Z","shell.execute_reply":"2025-07-03T03:56:33.655390Z"}},"outputs":[],"execution_count":7},{"id":"a7922a0b","cell_type":"markdown","source":"### Preparing Data","metadata":{"id":"a7922a0b"}},{"id":"tAlSGIhIvyo1","cell_type":"code","source":"def largest_slice(mask):\n    slice_sums = mask.sum(axis=(0, 1))\n    z = int(np.argmax(slice_sums))\n    return z","metadata":{"id":"tAlSGIhIvyo1","executionInfo":{"status":"ok","timestamp":1750915383507,"user_tz":300,"elapsed":12,"user":{"displayName":"Yazan Bakdash","userId":"15411906646133184411"}},"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T03:56:33.732783Z","iopub.execute_input":"2025-07-03T03:56:33.733051Z","iopub.status.idle":"2025-07-03T03:56:33.736994Z","shell.execute_reply.started":"2025-07-03T03:56:33.733032Z","shell.execute_reply":"2025-07-03T03:56:33.736169Z"}},"outputs":[],"execution_count":8},{"id":"229ca389","cell_type":"code","source":"from torch.utils.data import Dataset\nimport torchvision.transforms.functional as TF\n\nclass MRIDataset(Dataset):\n    def __init__(self, dir_list):\n        self.sample_dirs = dir_list  # Already a list of Path objects\n\n    def __len__(self):\n        return len(self.sample_dirs)\n\n    def __getitem__(self, idx):\n        sample_dir = self.sample_dirs[idx]\n        image = np.load(sample_dir / \"image.npy\")\n        mask = np.load(sample_dir / \"mask.npy\")\n\n        # Conver to tensor\n        image = torch.tensor(image, dtype=torch.float32)\n        mask = torch.tensor(mask, dtype=torch.float32)\n        \n        # Take slice\n        z = largest_slice(mask)\n        image = image[:,:,z].unsqueeze(0)\n        #image = torch.stack([image[z,:,:],image[:,z,:],image[:,:,z]], dim=0)\n        mask = mask[:,:,z].unsqueeze(0)\n\n        \n\n        return image, mask, sample_dir.name","metadata":{"id":"229ca389","executionInfo":{"status":"ok","timestamp":1750915386526,"user_tz":300,"elapsed":3010,"user":{"displayName":"Yazan Bakdash","userId":"15411906646133184411"}},"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T03:56:33.738225Z","iopub.execute_input":"2025-07-03T03:56:33.738508Z","iopub.status.idle":"2025-07-03T03:56:33.751120Z","shell.execute_reply.started":"2025-07-03T03:56:33.738491Z","shell.execute_reply":"2025-07-03T03:56:33.750212Z"}},"outputs":[],"execution_count":9},{"id":"55f692d2-7bfc-475d-a371-6ad0893452d3","cell_type":"code","source":"from torch.utils.data import Dataset\n\nclass MRIDataset3D(Dataset):\n    def __init__(self, dir_list, patch_size=(96, 96, 96), stride=(96, 96, 96)):\n        self.patch_size = patch_size\n        self.stride = stride\n        self.sample_dirs = dir_list\n        self.patch_indices = []  # list of (sample_idx, z, y, x)\n\n        # Precompute all patch indices\n        for sample_idx, sample_dir in enumerate(self.sample_dirs):\n            image = np.load(sample_dir / \"image.npy\")  # assume shape (D, H, W)\n            D, H, W = image.shape\n            pd, ph, pw = patch_size\n            sd, sh, sw = stride\n\n            for z in range(0, max(1, D - pd + 1), sd):\n                for y in range(0, max(1, H - ph + 1), sh):\n                    for x in range(0, max(1, W - pw + 1), sw):\n                        self.patch_indices.append((sample_idx, z, y, x))\n\n    def __len__(self):\n        return len(self.patch_indices)\n\n    def __getitem__(self, idx):\n        sample_idx, z, y, x = self.patch_indices[idx]\n        sample_dir = self.sample_dirs[sample_idx]\n\n        image = np.load(sample_dir / \"image.npy\")\n        mask = np.load(sample_dir / \"mask.npy\")\n\n        patch_img = image[z:z+self.patch_size[0], y:y+self.patch_size[1], x:x+self.patch_size[2]]\n        patch_mask = mask[z:z+self.patch_size[0], y:y+self.patch_size[1], x:x+self.patch_size[2]]\n\n        # Add channel dim and convert to torch\n        patch_img = torch.tensor(patch_img, dtype=torch.float32).unsqueeze(0)\n        patch_mask = torch.tensor(patch_mask, dtype=torch.float32).unsqueeze(0)\n\n        return patch_img, patch_mask, sample_dir.name","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T03:56:33.752135Z","iopub.execute_input":"2025-07-03T03:56:33.752384Z","iopub.status.idle":"2025-07-03T03:56:33.764314Z","shell.execute_reply.started":"2025-07-03T03:56:33.752368Z","shell.execute_reply":"2025-07-03T03:56:33.763492Z"}},"outputs":[],"execution_count":10},{"id":"0548131f","cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Split data into train and test\ndirs = sorted([p for p in data_dir.iterdir() if p.is_dir()])\ntrain_dirs, test_dirs = train_test_split(dirs, test_size=0.2, random_state=12)\ntrain_data = MRIDataset3D(train_dirs)\ntest_data = MRIDataset3D(test_dirs)","metadata":{"id":"0548131f","executionInfo":{"status":"ok","timestamp":1750915391584,"user_tz":300,"elapsed":5059,"user":{"displayName":"Yazan Bakdash","userId":"15411906646133184411"}},"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T03:56:33.765025Z","iopub.execute_input":"2025-07-03T03:56:33.765246Z","iopub.status.idle":"2025-07-03T03:57:03.953230Z","shell.execute_reply.started":"2025-07-03T03:56:33.765229Z","shell.execute_reply":"2025-07-03T03:57:03.952473Z"}},"outputs":[],"execution_count":11},{"id":"1fe2d4de","cell_type":"code","source":"from torch.utils.data import DataLoader\n\ntrain_loader = DataLoader(train_data, batch_size=2, shuffle=True)\ntest_loader = DataLoader(test_data, batch_size=2, shuffle=False)","metadata":{"id":"1fe2d4de","executionInfo":{"status":"ok","timestamp":1750915391613,"user_tz":300,"elapsed":19,"user":{"displayName":"Yazan Bakdash","userId":"15411906646133184411"}},"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T03:57:03.955101Z","iopub.execute_input":"2025-07-03T03:57:03.955484Z","iopub.status.idle":"2025-07-03T03:57:03.959435Z","shell.execute_reply.started":"2025-07-03T03:57:03.955465Z","shell.execute_reply":"2025-07-03T03:57:03.958747Z"}},"outputs":[],"execution_count":12},{"id":"PELKt9z1bz90","cell_type":"code","source":"class DiceLoss(nn.Module):\n    def __init__(self, smooth=1e-6):\n        super().__init__()\n        self.smooth = smooth\n\n    def forward(self, logits, targets):\n        # Apply sigmoid to logits\n        probs = torch.sigmoid(logits)\n        targets = targets.float()\n\n        intersection = (probs * targets).sum(dim=(2, 3))\n        union = probs.sum(dim=(2, 3)) + targets.sum(dim=(2, 3))\n        dice = (2 * intersection + self.smooth) / (union + self.smooth)\n        return 1 - dice.mean()\n\n","metadata":{"id":"PELKt9z1bz90","executionInfo":{"status":"ok","timestamp":1750915391629,"user_tz":300,"elapsed":14,"user":{"displayName":"Yazan Bakdash","userId":"15411906646133184411"}},"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T03:57:03.960244Z","iopub.execute_input":"2025-07-03T03:57:03.960500Z","iopub.status.idle":"2025-07-03T03:57:03.978735Z","shell.execute_reply.started":"2025-07-03T03:57:03.960478Z","shell.execute_reply":"2025-07-03T03:57:03.978078Z"}},"outputs":[],"execution_count":13},{"id":"gr-AWBf-bsQm","cell_type":"code","source":"def combined_loss(logits, targets):\n    return DiceLoss()(logits, targets) + nn.BCEWithLogitsLoss()(logits, targets)","metadata":{"id":"gr-AWBf-bsQm","executionInfo":{"status":"ok","timestamp":1750915391661,"user_tz":300,"elapsed":30,"user":{"displayName":"Yazan Bakdash","userId":"15411906646133184411"}},"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T03:57:03.979407Z","iopub.execute_input":"2025-07-03T03:57:03.979585Z","iopub.status.idle":"2025-07-03T03:57:03.992162Z","shell.execute_reply.started":"2025-07-03T03:57:03.979572Z","shell.execute_reply":"2025-07-03T03:57:03.991479Z"}},"outputs":[],"execution_count":14},{"id":"2UrGvfgVekeA","cell_type":"code","source":"def safe_dice_loss(logits, targets, smooth=1e-6):\n    probs = torch.sigmoid(logits)\n    targets = targets.float()\n    valid = targets.sum(dim=(2, 3)) > 0  # skip blank targets\n    intersection = (probs * targets).sum(dim=(2, 3))\n    union = probs.sum(dim=(2, 3)) + targets.sum(dim=(2, 3))\n    dice = (2 * intersection + smooth) / (union + smooth)\n    return 1 - dice[valid].mean() if valid.any() else torch.tensor(0.0, device=logits.device)","metadata":{"id":"2UrGvfgVekeA","executionInfo":{"status":"ok","timestamp":1750915391703,"user_tz":300,"elapsed":40,"user":{"displayName":"Yazan Bakdash","userId":"15411906646133184411"}},"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T03:57:03.993062Z","iopub.execute_input":"2025-07-03T03:57:03.993324Z","iopub.status.idle":"2025-07-03T03:57:04.006174Z","shell.execute_reply.started":"2025-07-03T03:57:03.993302Z","shell.execute_reply":"2025-07-03T03:57:04.005478Z"}},"outputs":[],"execution_count":15},{"id":"3b7ffe37","cell_type":"code","source":"model = UNET3D().to(device)\ncriterion = combined_loss\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\nepochs = 30\ntrain_losses = [0] * epochs\ntest_losses = [0] * epochs\nfor i in range(epochs):\n    model.train()\n    train_loss_total = 0\n    for b, (img_train, mask_train, names) in enumerate(train_loader):\n        img_train = img_train.to(device)\n        mask_train = mask_train.to(device)\n\n        mask_pred = model(img_train)\n        loss = criterion(mask_pred, mask_train)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        train_loss_total += loss.item()\n        print(f'  Batch: {b+1} Loss: {loss.item()}')\n\n    train_losses[i] = train_loss_total / len(train_loader)\n    print(f'Epoch: {i+1} Loss: {train_loss_total / len(train_loader)}')\n\n    # testing\n    model.eval()\n    test_loss_total = 0\n    with torch.no_grad():\n        for b, (img_test, mask_test, names) in enumerate(test_loader):\n            img_test = img_test.to(device)\n            mask_test = mask_test.to(device)\n\n            mask_pred = model(img_test)\n            loss = criterion(mask_pred, mask_test)\n            test_loss_total += loss.item()\n\n    test_losses[i] = test_loss_total / len(test_loader)","metadata":{"id":"3b7ffe37","outputId":"be764959-ff36-4d8c-a2fa-770f82cb61d9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1750918098225,"user_tz":300,"elapsed":117484,"user":{"displayName":"Yazan Bakdash","userId":"15411906646133184411"}},"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T03:57:28.236368Z","iopub.execute_input":"2025-07-03T03:57:28.236895Z","iopub.status.idle":"2025-07-03T04:51:32.705427Z","shell.execute_reply.started":"2025-07-03T03:57:28.236873Z","shell.execute_reply":"2025-07-03T04:51:32.703936Z"}},"outputs":[{"name":"stdout","text":"  Batch: 1 Loss: 1.8306078910827637\n  Batch: 2 Loss: 1.805379867553711\n  Batch: 3 Loss: 1.8331947326660156\n  Batch: 4 Loss: 1.7702512741088867\n  Batch: 5 Loss: 1.7654271125793457\n  Batch: 6 Loss: 1.8454076051712036\n  Batch: 7 Loss: 1.8151237964630127\n  Batch: 8 Loss: 1.7543878555297852\n  Batch: 9 Loss: 1.7673753499984741\n  Batch: 10 Loss: 1.7393085956573486\n  Batch: 11 Loss: 1.7362914085388184\n  Batch: 12 Loss: 1.7301521301269531\n  Batch: 13 Loss: 1.7241778373718262\n  Batch: 14 Loss: 1.7145717144012451\n  Batch: 15 Loss: 1.7050930261611938\n  Batch: 16 Loss: 1.6970632076263428\n  Batch: 17 Loss: 1.7423415184020996\n  Batch: 18 Loss: 1.718427062034607\n  Batch: 19 Loss: 1.7028329372406006\n  Batch: 20 Loss: 1.7444605827331543\n  Batch: 21 Loss: 1.6775184869766235\n  Batch: 22 Loss: 1.673449158668518\n  Batch: 23 Loss: 1.701025366783142\n  Batch: 24 Loss: 1.6849285364151\n  Batch: 25 Loss: 1.717123031616211\n  Batch: 26 Loss: 1.6441752910614014\n  Batch: 27 Loss: 1.69108247756958\n  Batch: 28 Loss: 1.628481388092041\n  Batch: 29 Loss: 1.6407928466796875\n  Batch: 30 Loss: 1.7279939651489258\n  Batch: 31 Loss: 1.645919680595398\n  Batch: 32 Loss: 1.6805460453033447\n  Batch: 33 Loss: 1.6383559703826904\n  Batch: 34 Loss: 1.611375331878662\n  Batch: 35 Loss: 1.6348178386688232\n  Batch: 36 Loss: 1.6895935535430908\n  Batch: 37 Loss: 1.627568006515503\n  Batch: 38 Loss: 1.635056734085083\n  Batch: 39 Loss: 1.6812660694122314\n  Batch: 40 Loss: 1.6631920337677002\n  Batch: 41 Loss: 1.581047773361206\n  Batch: 42 Loss: 1.6118347644805908\n  Batch: 43 Loss: 1.5708882808685303\n  Batch: 44 Loss: 1.5861892700195312\n  Batch: 45 Loss: 1.5887607336044312\n  Batch: 46 Loss: 1.6794390678405762\n  Batch: 47 Loss: 1.632269263267517\n  Batch: 48 Loss: 1.6006252765655518\n  Batch: 49 Loss: 1.5630357265472412\n  Batch: 50 Loss: 1.5502710342407227\n  Batch: 51 Loss: 1.5509722232818604\n  Batch: 52 Loss: 1.5470744371414185\n  Batch: 53 Loss: 1.6063666343688965\n  Batch: 54 Loss: 1.5431114435195923\n  Batch: 55 Loss: 1.5336909294128418\n  Batch: 56 Loss: 1.5763676166534424\n  Batch: 57 Loss: 1.5883039236068726\n  Batch: 58 Loss: 1.5249269008636475\n  Batch: 59 Loss: 1.5851025581359863\n  Batch: 60 Loss: 1.5228512287139893\n  Batch: 61 Loss: 1.5534895658493042\n  Batch: 62 Loss: 1.5453739166259766\n  Batch: 63 Loss: 1.522555947303772\n  Batch: 64 Loss: 1.532979965209961\n  Batch: 65 Loss: 1.5209550857543945\n  Batch: 66 Loss: 1.5000149011611938\n  Batch: 67 Loss: 1.5640560388565063\n  Batch: 68 Loss: 1.5131765604019165\n  Batch: 69 Loss: 1.5546900033950806\n  Batch: 70 Loss: 1.5487452745437622\n  Batch: 71 Loss: 1.6232528686523438\n  Batch: 72 Loss: 1.4976212978363037\n  Batch: 73 Loss: 1.6355862617492676\n  Batch: 74 Loss: 1.526828646659851\n  Batch: 75 Loss: 1.560737133026123\n  Batch: 76 Loss: 1.5073024034500122\n  Batch: 77 Loss: 1.4618043899536133\n  Batch: 78 Loss: 1.5559146404266357\n  Batch: 79 Loss: 1.518270492553711\n  Batch: 80 Loss: 1.5878745317459106\n  Batch: 81 Loss: 1.5024359226226807\n  Batch: 82 Loss: 1.599231243133545\n  Batch: 83 Loss: 1.5155104398727417\n  Batch: 84 Loss: 1.5958404541015625\n  Batch: 85 Loss: 1.496660828590393\n  Batch: 86 Loss: 1.5761631727218628\n  Batch: 87 Loss: 1.4736467599868774\n  Batch: 88 Loss: 1.4785832166671753\n  Batch: 89 Loss: 1.5638728141784668\n  Batch: 90 Loss: 1.5006005764007568\n  Batch: 91 Loss: 1.4960929155349731\n  Batch: 92 Loss: 1.5461456775665283\n  Batch: 93 Loss: 1.5061323642730713\n  Batch: 94 Loss: 1.4723551273345947\n  Batch: 95 Loss: 1.5309051275253296\n  Batch: 96 Loss: 1.5551015138626099\n  Batch: 97 Loss: 1.492325782775879\n  Batch: 98 Loss: 1.4887242317199707\n  Batch: 99 Loss: 1.4787532091140747\n  Batch: 100 Loss: 1.4747323989868164\n  Batch: 101 Loss: 1.5600624084472656\n  Batch: 102 Loss: 1.468766689300537\n  Batch: 103 Loss: 1.4730279445648193\n  Batch: 104 Loss: 1.5774085521697998\n  Batch: 105 Loss: 1.5183970928192139\n  Batch: 106 Loss: 1.5333575010299683\n  Batch: 107 Loss: 1.502882957458496\n  Batch: 108 Loss: 1.49130380153656\n  Batch: 109 Loss: 1.4834680557250977\n  Batch: 110 Loss: 1.5318377017974854\n  Batch: 111 Loss: 1.531029224395752\n  Batch: 112 Loss: 1.4954843521118164\n  Batch: 113 Loss: 1.485349416732788\n  Batch: 114 Loss: 1.5683081150054932\n  Batch: 115 Loss: 1.553222894668579\n  Batch: 116 Loss: 1.4626867771148682\n  Batch: 117 Loss: 1.456251859664917\n  Batch: 118 Loss: 1.5472204685211182\n  Batch: 119 Loss: 1.4811429977416992\n  Batch: 120 Loss: 1.4592571258544922\n  Batch: 121 Loss: 1.526811122894287\n  Batch: 122 Loss: 1.5023771524429321\n  Batch: 123 Loss: 1.4554578065872192\n  Batch: 124 Loss: 1.450771450996399\n  Batch: 125 Loss: 1.4478802680969238\n  Batch: 126 Loss: 1.48920738697052\n  Batch: 127 Loss: 1.460983395576477\n  Batch: 128 Loss: 1.5147535800933838\n  Batch: 129 Loss: 1.449038028717041\n  Batch: 130 Loss: 1.5084824562072754\n  Batch: 131 Loss: 1.510212779045105\n  Batch: 132 Loss: 1.5050591230392456\n  Batch: 133 Loss: 1.4963878393173218\n  Batch: 134 Loss: 1.479805588722229\n  Batch: 135 Loss: 1.4613800048828125\n  Batch: 136 Loss: 1.440640926361084\n  Batch: 137 Loss: 1.5055288076400757\n  Batch: 138 Loss: 1.455967903137207\n  Batch: 139 Loss: 1.4542244672775269\n  Batch: 140 Loss: 1.509516954421997\n  Batch: 141 Loss: 1.47837495803833\n  Batch: 142 Loss: 1.4879627227783203\n  Batch: 143 Loss: 1.4870182275772095\n  Batch: 144 Loss: 1.497987985610962\n  Batch: 145 Loss: 1.447712779045105\n  Batch: 146 Loss: 1.4572157859802246\n  Batch: 147 Loss: 1.5002844333648682\n  Batch: 148 Loss: 1.4444410800933838\n  Batch: 149 Loss: 1.5006561279296875\n  Batch: 150 Loss: 1.4748802185058594\n  Batch: 151 Loss: 1.4770512580871582\n  Batch: 152 Loss: 1.4581010341644287\n  Batch: 153 Loss: 1.4447236061096191\n  Batch: 154 Loss: 1.4361732006072998\n  Batch: 155 Loss: 1.5378413200378418\n  Batch: 156 Loss: 1.468610167503357\n  Batch: 157 Loss: 1.4523485898971558\n  Batch: 158 Loss: 1.456484317779541\n  Batch: 159 Loss: 1.4847958087921143\n  Batch: 160 Loss: 1.439448356628418\n  Batch: 161 Loss: 1.4849146604537964\n  Batch: 162 Loss: 1.468963861465454\n  Batch: 163 Loss: 1.4462876319885254\n  Batch: 164 Loss: 1.4812498092651367\n  Batch: 165 Loss: 1.459843635559082\n  Batch: 166 Loss: 1.4398201704025269\n  Batch: 167 Loss: 1.486586332321167\n  Batch: 168 Loss: 1.4636681079864502\n  Batch: 169 Loss: 1.4482446908950806\n  Batch: 170 Loss: 1.4914164543151855\n  Batch: 171 Loss: 1.456329107284546\n  Batch: 172 Loss: 1.4547488689422607\n  Batch: 173 Loss: 1.4582165479660034\n  Batch: 174 Loss: 1.4507718086242676\n  Batch: 175 Loss: 1.440724492073059\n  Batch: 176 Loss: 1.4321701526641846\n  Batch: 177 Loss: 1.466928243637085\n  Batch: 178 Loss: 1.4982526302337646\n  Batch: 179 Loss: 1.4248239994049072\n  Batch: 180 Loss: 1.4322984218597412\n  Batch: 181 Loss: 1.4644025564193726\n  Batch: 182 Loss: 1.4954065084457397\n  Batch: 183 Loss: 1.4499235153198242\n  Batch: 184 Loss: 1.4414405822753906\n  Batch: 185 Loss: 1.4312973022460938\n  Batch: 186 Loss: 1.4466514587402344\n  Batch: 187 Loss: 1.4463800191879272\n  Batch: 188 Loss: 1.4398186206817627\n  Batch: 189 Loss: 1.4499534368515015\n  Batch: 190 Loss: 1.4402573108673096\n  Batch: 191 Loss: 1.4315531253814697\n  Batch: 192 Loss: 1.4491190910339355\n  Batch: 193 Loss: 1.4489120244979858\n  Batch: 194 Loss: 1.4434070587158203\n  Batch: 195 Loss: 1.4452275037765503\n  Batch: 196 Loss: 1.4260485172271729\n  Batch: 197 Loss: 1.4235689640045166\n  Batch: 198 Loss: 1.4172788858413696\n  Batch: 199 Loss: 1.4356091022491455\n  Batch: 200 Loss: 1.4258555173873901\n  Batch: 201 Loss: 1.4295752048492432\n  Batch: 202 Loss: 1.439612627029419\n  Batch: 203 Loss: 1.4281688928604126\n  Batch: 204 Loss: 1.423264980316162\n  Batch: 205 Loss: 1.4322586059570312\n  Batch: 206 Loss: 1.4278185367584229\n  Batch: 207 Loss: 1.431486964225769\n  Batch: 208 Loss: 1.4456692934036255\n  Batch: 209 Loss: 1.4287164211273193\n  Batch: 210 Loss: 1.4313029050827026\n  Batch: 211 Loss: 1.4193209409713745\n  Batch: 212 Loss: 1.432060718536377\n  Batch: 213 Loss: 1.45207679271698\n  Batch: 214 Loss: 1.4453024864196777\n  Batch: 215 Loss: 1.4313267469406128\n  Batch: 216 Loss: 1.4249494075775146\n  Batch: 217 Loss: 1.4213004112243652\n  Batch: 218 Loss: 1.4668617248535156\n  Batch: 219 Loss: 1.422085165977478\n  Batch: 220 Loss: 1.4225667715072632\n  Batch: 221 Loss: 1.4321789741516113\n  Batch: 222 Loss: 1.4084126949310303\n  Batch: 223 Loss: 1.4140715599060059\n  Batch: 224 Loss: 1.4150376319885254\n  Batch: 225 Loss: 1.4460599422454834\n  Batch: 226 Loss: 1.4204378128051758\n  Batch: 227 Loss: 1.4151650667190552\n  Batch: 228 Loss: 1.4244085550308228\n  Batch: 229 Loss: 1.4091336727142334\n  Batch: 230 Loss: 1.4215033054351807\n  Batch: 231 Loss: 1.409705400466919\n  Batch: 232 Loss: 1.417097806930542\n  Batch: 233 Loss: 1.4239840507507324\n  Batch: 234 Loss: 1.4725981950759888\n  Batch: 235 Loss: 1.4347171783447266\n  Batch: 236 Loss: 1.4207407236099243\n  Batch: 237 Loss: 1.4273929595947266\n  Batch: 238 Loss: 1.434171438217163\n  Batch: 239 Loss: 1.4249708652496338\n  Batch: 240 Loss: 1.4169807434082031\n  Batch: 241 Loss: 1.421400547027588\n  Batch: 242 Loss: 1.422012209892273\n  Batch: 243 Loss: 1.4442286491394043\n  Batch: 244 Loss: 1.4111723899841309\n  Batch: 245 Loss: 1.4160915613174438\n  Batch: 246 Loss: 1.40848708152771\n  Batch: 247 Loss: 1.4053508043289185\n  Batch: 248 Loss: 1.4135768413543701\n  Batch: 249 Loss: 1.4023609161376953\n  Batch: 250 Loss: 1.4041094779968262\n  Batch: 251 Loss: 1.4202321767807007\n  Batch: 252 Loss: 1.417065143585205\n  Batch: 253 Loss: 1.4297127723693848\n  Batch: 254 Loss: 1.400496244430542\n  Batch: 255 Loss: 1.411232590675354\n  Batch: 256 Loss: 1.4139935970306396\n  Batch: 257 Loss: 1.4354661703109741\n  Batch: 258 Loss: 1.4062780141830444\n  Batch: 259 Loss: 1.4770824909210205\n  Batch: 260 Loss: 1.4033195972442627\n  Batch: 261 Loss: 1.4061970710754395\n  Batch: 262 Loss: 1.3974817991256714\n  Batch: 263 Loss: 1.4237829446792603\n  Batch: 264 Loss: 1.4030464887619019\n  Batch: 265 Loss: 1.3986923694610596\n  Batch: 266 Loss: 1.4659703969955444\n  Batch: 267 Loss: 1.3924137353897095\n  Batch: 268 Loss: 1.4044286012649536\n  Batch: 269 Loss: 1.4147535562515259\n  Batch: 270 Loss: 1.3986823558807373\n  Batch: 271 Loss: 1.4017788171768188\n  Batch: 272 Loss: 1.391901969909668\n  Batch: 273 Loss: 1.401585578918457\n  Batch: 274 Loss: 1.3985356092453003\n  Batch: 275 Loss: 1.3881250619888306\n  Batch: 276 Loss: 1.3948897123336792\n  Batch: 277 Loss: 1.407274842262268\n  Batch: 278 Loss: 1.3978495597839355\n  Batch: 279 Loss: 1.4019687175750732\n  Batch: 280 Loss: 1.3994104862213135\n  Batch: 281 Loss: 1.3986304998397827\n  Batch: 282 Loss: 1.398535132408142\n  Batch: 283 Loss: 1.3870021104812622\n  Batch: 284 Loss: 1.3908841609954834\n  Batch: 285 Loss: 1.3947844505310059\n  Batch: 286 Loss: 1.3906786441802979\n  Batch: 287 Loss: 1.3867548704147339\n  Batch: 288 Loss: 1.378366470336914\n  Batch: 289 Loss: 1.3907639980316162\n  Batch: 290 Loss: 1.3906128406524658\n  Batch: 291 Loss: 1.3842891454696655\n  Batch: 292 Loss: 1.3818469047546387\n  Batch: 293 Loss: 1.4010217189788818\n  Batch: 294 Loss: 1.3821415901184082\n  Batch: 295 Loss: 1.388733148574829\n  Batch: 296 Loss: 1.4101088047027588\n  Batch: 297 Loss: 1.4011359214782715\n  Batch: 298 Loss: 1.3887834548950195\n  Batch: 299 Loss: 1.3824453353881836\n  Batch: 300 Loss: 1.384421467781067\n  Batch: 301 Loss: 1.3878878355026245\n  Batch: 302 Loss: 1.3854384422302246\n  Batch: 303 Loss: 1.3665690422058105\n  Batch: 304 Loss: 1.4021666049957275\n  Batch: 305 Loss: 1.3834314346313477\n  Batch: 306 Loss: 1.3895509243011475\n  Batch: 307 Loss: 1.3851914405822754\n  Batch: 308 Loss: 1.3816941976547241\n  Batch: 309 Loss: 1.383995532989502\n  Batch: 310 Loss: 1.3795669078826904\n  Batch: 311 Loss: 1.3789509534835815\n  Batch: 312 Loss: 1.377014398574829\n  Batch: 313 Loss: 1.3674633502960205\n  Batch: 314 Loss: 1.379481554031372\n  Batch: 315 Loss: 1.383981466293335\n  Batch: 316 Loss: 1.3766613006591797\n  Batch: 317 Loss: 1.3709533214569092\n  Batch: 318 Loss: 1.378051519393921\n  Batch: 319 Loss: 1.3737716674804688\n  Batch: 320 Loss: 1.3751022815704346\n  Batch: 321 Loss: 1.3754792213439941\n  Batch: 322 Loss: 1.3758677244186401\n  Batch: 323 Loss: 1.3655624389648438\n  Batch: 324 Loss: 1.3653889894485474\n  Batch: 325 Loss: 1.3690680265426636\n  Batch: 326 Loss: 1.4179680347442627\n  Batch: 327 Loss: 1.372513771057129\n  Batch: 328 Loss: 1.3704135417938232\n  Batch: 329 Loss: 1.3701913356781006\n  Batch: 330 Loss: 1.3586722612380981\n  Batch: 331 Loss: 1.3647375106811523\n  Batch: 332 Loss: 1.3804270029067993\n  Batch: 333 Loss: 1.383217215538025\n  Batch: 334 Loss: 1.3630139827728271\n  Batch: 335 Loss: 1.38212251663208\n  Batch: 336 Loss: 1.3655922412872314\nEpoch: 1 Loss: 1.4863862487531843\n  Batch: 1 Loss: 1.3599082231521606\n  Batch: 2 Loss: 1.3868882656097412\n  Batch: 3 Loss: 1.3599261045455933\n  Batch: 4 Loss: 1.3593679666519165\n  Batch: 5 Loss: 1.3579761981964111\n  Batch: 6 Loss: 1.3717972040176392\n  Batch: 7 Loss: 1.3630797863006592\n  Batch: 8 Loss: 1.375867486000061\n  Batch: 9 Loss: 1.375117540359497\n  Batch: 10 Loss: 1.3586382865905762\n  Batch: 11 Loss: 1.3822283744812012\n  Batch: 12 Loss: 1.3711298704147339\n  Batch: 13 Loss: 1.3714213371276855\n  Batch: 14 Loss: 1.369134545326233\n  Batch: 15 Loss: 1.3639543056488037\n  Batch: 16 Loss: 1.3634521961212158\n  Batch: 17 Loss: 1.3556256294250488\n  Batch: 18 Loss: 1.3633546829223633\n  Batch: 19 Loss: 1.36709725856781\n  Batch: 20 Loss: 1.3528345823287964\n  Batch: 21 Loss: 1.3537859916687012\n  Batch: 22 Loss: 1.3617262840270996\n  Batch: 23 Loss: 1.3613494634628296\n  Batch: 24 Loss: 1.3644628524780273\n  Batch: 25 Loss: 1.3561300039291382\n  Batch: 26 Loss: 1.3518297672271729\n  Batch: 27 Loss: 1.3597168922424316\n  Batch: 28 Loss: 1.3655750751495361\n  Batch: 29 Loss: 1.3548107147216797\n  Batch: 30 Loss: 1.353167176246643\n  Batch: 31 Loss: 1.3589391708374023\n  Batch: 32 Loss: 1.3548072576522827\n  Batch: 33 Loss: 1.353864312171936\n  Batch: 34 Loss: 1.3514056205749512\n  Batch: 35 Loss: 1.3630046844482422\n  Batch: 36 Loss: 1.3497169017791748\n  Batch: 37 Loss: 1.3506134748458862\n  Batch: 38 Loss: 1.3551050424575806\n  Batch: 39 Loss: 1.3562910556793213\n  Batch: 40 Loss: 1.3461366891860962\n  Batch: 41 Loss: 1.3446718454360962\n  Batch: 42 Loss: 1.3517690896987915\n  Batch: 43 Loss: 1.3523857593536377\n  Batch: 44 Loss: 1.35105562210083\n  Batch: 45 Loss: 1.3469576835632324\n  Batch: 46 Loss: 1.34274160861969\n  Batch: 47 Loss: 1.35308837890625\n  Batch: 48 Loss: 1.3571786880493164\n  Batch: 49 Loss: 1.3427202701568604\n  Batch: 50 Loss: 1.3454917669296265\n  Batch: 51 Loss: 1.3427410125732422\n  Batch: 52 Loss: 1.358896255493164\n  Batch: 53 Loss: 1.3550214767456055\n  Batch: 54 Loss: 1.344299554824829\n  Batch: 55 Loss: 1.3600850105285645\n  Batch: 56 Loss: 1.3497824668884277\n  Batch: 57 Loss: 1.3448107242584229\n  Batch: 58 Loss: 1.3347067832946777\n  Batch: 59 Loss: 1.3452068567276\n  Batch: 60 Loss: 1.3337256908416748\n  Batch: 61 Loss: 1.3417136669158936\n  Batch: 62 Loss: 1.3426510095596313\n  Batch: 63 Loss: 1.3414332866668701\n  Batch: 64 Loss: 1.3400529623031616\n  Batch: 65 Loss: 1.3327847719192505\n  Batch: 66 Loss: 1.3316757678985596\n  Batch: 67 Loss: 1.3406615257263184\n  Batch: 68 Loss: 1.3363821506500244\n  Batch: 69 Loss: 1.3366374969482422\n  Batch: 70 Loss: 1.3358540534973145\n  Batch: 71 Loss: 1.3306900262832642\n  Batch: 72 Loss: 1.3289705514907837\n  Batch: 73 Loss: 1.345717191696167\n  Batch: 74 Loss: 1.3334307670593262\n  Batch: 75 Loss: 1.3372454643249512\n  Batch: 76 Loss: 1.3352484703063965\n  Batch: 77 Loss: 1.3305909633636475\n  Batch: 78 Loss: 1.3281805515289307\n  Batch: 79 Loss: 1.370429515838623\n  Batch: 80 Loss: 1.3711193799972534\n  Batch: 81 Loss: 1.3339412212371826\n  Batch: 82 Loss: 1.3289520740509033\n  Batch: 83 Loss: 1.3388557434082031\n  Batch: 84 Loss: 1.326540470123291\n  Batch: 85 Loss: 1.33101487159729\n  Batch: 86 Loss: 1.3327486515045166\n  Batch: 87 Loss: 1.3317317962646484\n  Batch: 88 Loss: 1.3266634941101074\n  Batch: 89 Loss: 1.3316078186035156\n  Batch: 90 Loss: 1.327890157699585\n  Batch: 91 Loss: 1.335434913635254\n  Batch: 92 Loss: 1.3535175323486328\n  Batch: 93 Loss: 1.3309099674224854\n  Batch: 94 Loss: 1.3222886323928833\n  Batch: 95 Loss: 1.3316601514816284\n  Batch: 96 Loss: 1.3221628665924072\n  Batch: 97 Loss: 1.3306224346160889\n  Batch: 98 Loss: 1.3289170265197754\n  Batch: 99 Loss: 1.326859474182129\n  Batch: 100 Loss: 1.3296009302139282\n  Batch: 101 Loss: 1.329412579536438\n  Batch: 102 Loss: 1.31846022605896\n  Batch: 103 Loss: 1.329254388809204\n  Batch: 104 Loss: 1.3277174234390259\n  Batch: 105 Loss: 1.3270214796066284\n  Batch: 106 Loss: 1.3262931108474731\n  Batch: 107 Loss: 1.332122564315796\n  Batch: 108 Loss: 1.3150899410247803\n  Batch: 109 Loss: 1.3253494501113892\n  Batch: 110 Loss: 1.3203508853912354\n  Batch: 111 Loss: 1.319860816001892\n  Batch: 112 Loss: 1.3242653608322144\n  Batch: 113 Loss: 1.3191860914230347\n  Batch: 114 Loss: 1.322410225868225\n  Batch: 115 Loss: 1.3270606994628906\n  Batch: 116 Loss: 1.3214550018310547\n  Batch: 117 Loss: 1.3229210376739502\n  Batch: 118 Loss: 1.3144121170043945\n  Batch: 119 Loss: 1.322295904159546\n  Batch: 120 Loss: 1.3201454877853394\n  Batch: 121 Loss: 1.3159573078155518\n  Batch: 122 Loss: 1.3154135942459106\n  Batch: 123 Loss: 1.3191046714782715\n  Batch: 124 Loss: 1.3140785694122314\n  Batch: 125 Loss: 1.3136918544769287\n  Batch: 126 Loss: 1.338800072669983\n  Batch: 127 Loss: 1.317768931388855\n  Batch: 128 Loss: 1.3172776699066162\n  Batch: 129 Loss: 1.3138303756713867\n  Batch: 130 Loss: 1.3176368474960327\n  Batch: 131 Loss: 1.3181558847427368\n  Batch: 132 Loss: 1.3222609758377075\n  Batch: 133 Loss: 1.3344383239746094\n  Batch: 134 Loss: 1.325613021850586\n  Batch: 135 Loss: 1.3114395141601562\n  Batch: 136 Loss: 1.3107017278671265\n  Batch: 137 Loss: 1.3111343383789062\n  Batch: 138 Loss: 1.3108148574829102\n  Batch: 139 Loss: 1.3102707862854004\n  Batch: 140 Loss: 1.3117918968200684\n  Batch: 141 Loss: 1.3187443017959595\n  Batch: 142 Loss: 1.3106727600097656\n  Batch: 143 Loss: 1.3179090023040771\n  Batch: 144 Loss: 1.3273167610168457\n  Batch: 145 Loss: 1.3149118423461914\n  Batch: 146 Loss: 1.3118584156036377\n  Batch: 147 Loss: 1.3129098415374756\n  Batch: 148 Loss: 1.307466745376587\n  Batch: 149 Loss: 1.3278207778930664\n  Batch: 150 Loss: 1.306936264038086\n  Batch: 151 Loss: 1.3112229108810425\n  Batch: 152 Loss: 1.3095468282699585\n  Batch: 153 Loss: 1.307706594467163\n  Batch: 154 Loss: 1.3186254501342773\n  Batch: 155 Loss: 1.3109943866729736\n  Batch: 156 Loss: 1.3124961853027344\n  Batch: 157 Loss: 1.3079885244369507\n  Batch: 158 Loss: 1.3096891641616821\n  Batch: 159 Loss: 1.3090019226074219\n  Batch: 160 Loss: 1.3058151006698608\n  Batch: 161 Loss: 1.3090929985046387\n  Batch: 162 Loss: 1.307062029838562\n  Batch: 163 Loss: 1.3113634586334229\n  Batch: 164 Loss: 1.3083670139312744\n  Batch: 165 Loss: 1.3135037422180176\n  Batch: 166 Loss: 1.3025569915771484\n  Batch: 167 Loss: 1.3172945976257324\n  Batch: 168 Loss: 1.3036024570465088\n  Batch: 169 Loss: 1.311845064163208\n  Batch: 170 Loss: 1.305403232574463\n  Batch: 171 Loss: 1.3170790672302246\n  Batch: 172 Loss: 1.300105333328247\n  Batch: 173 Loss: 1.3014789819717407\n  Batch: 174 Loss: 1.302851676940918\n  Batch: 175 Loss: 1.3045473098754883\n  Batch: 176 Loss: 1.3007594347000122\n  Batch: 177 Loss: 1.3034889698028564\n  Batch: 178 Loss: 1.3049660921096802\n  Batch: 179 Loss: 1.301206111907959\n  Batch: 180 Loss: 1.304624080657959\n  Batch: 181 Loss: 1.298888087272644\n  Batch: 182 Loss: 1.297998070716858\n  Batch: 183 Loss: 1.2981045246124268\n  Batch: 184 Loss: 1.3097631931304932\n  Batch: 185 Loss: 1.300295352935791\n  Batch: 186 Loss: 1.2996925115585327\n  Batch: 187 Loss: 1.2996422052383423\n  Batch: 188 Loss: 1.3090876340866089\n  Batch: 189 Loss: 1.2948687076568604\n  Batch: 190 Loss: 1.3003779649734497\n  Batch: 191 Loss: 1.3137383460998535\n  Batch: 192 Loss: 1.2934092283248901\n  Batch: 193 Loss: 1.3002452850341797\n  Batch: 194 Loss: 1.2916104793548584\n  Batch: 195 Loss: 1.3078253269195557\n  Batch: 196 Loss: 1.2945888042449951\n  Batch: 197 Loss: 1.2956534624099731\n  Batch: 198 Loss: 1.2974789142608643\n  Batch: 199 Loss: 1.3139876127243042\n  Batch: 200 Loss: 1.2930271625518799\n  Batch: 201 Loss: 1.2910493612289429\n  Batch: 202 Loss: 1.2907013893127441\n  Batch: 203 Loss: 1.2992969751358032\n  Batch: 204 Loss: 1.2928261756896973\n  Batch: 205 Loss: 1.288686752319336\n  Batch: 206 Loss: 1.299267292022705\n  Batch: 207 Loss: 1.2974895238876343\n  Batch: 208 Loss: 1.2963467836380005\n  Batch: 209 Loss: 1.2861804962158203\n  Batch: 210 Loss: 1.2935057878494263\n  Batch: 211 Loss: 1.2956786155700684\n  Batch: 212 Loss: 1.2901633977890015\n  Batch: 213 Loss: 1.2872962951660156\n  Batch: 214 Loss: 1.300270438194275\n  Batch: 215 Loss: 1.2851976156234741\n  Batch: 216 Loss: 1.293190598487854\n  Batch: 217 Loss: 1.2923545837402344\n  Batch: 218 Loss: 1.2849754095077515\n  Batch: 219 Loss: 1.2863951921463013\n  Batch: 220 Loss: 1.2863880395889282\n  Batch: 221 Loss: 1.2865781784057617\n  Batch: 222 Loss: 1.2880886793136597\n  Batch: 223 Loss: 1.2879018783569336\n  Batch: 224 Loss: 1.28799569606781\n  Batch: 225 Loss: 1.2767597436904907\n  Batch: 226 Loss: 1.2891944646835327\n  Batch: 227 Loss: 1.2908408641815186\n  Batch: 228 Loss: 1.287235975265503\n  Batch: 229 Loss: 1.2857282161712646\n  Batch: 230 Loss: 1.2863963842391968\n  Batch: 231 Loss: 1.2857599258422852\n  Batch: 232 Loss: 1.28440523147583\n  Batch: 233 Loss: 1.2844758033752441\n  Batch: 234 Loss: 1.2849289178848267\n  Batch: 235 Loss: 1.2849534749984741\n  Batch: 236 Loss: 1.2833995819091797\n  Batch: 237 Loss: 1.2792233228683472\n  Batch: 238 Loss: 1.283942461013794\n  Batch: 239 Loss: 1.2841887474060059\n  Batch: 240 Loss: 1.2828974723815918\n  Batch: 241 Loss: 1.2817336320877075\n  Batch: 242 Loss: 1.279375433921814\n  Batch: 243 Loss: 1.2779552936553955\n  Batch: 244 Loss: 1.2830933332443237\n  Batch: 245 Loss: 1.2763348817825317\n  Batch: 246 Loss: 1.2804899215698242\n  Batch: 247 Loss: 1.2779803276062012\n  Batch: 248 Loss: 1.2793840169906616\n  Batch: 249 Loss: 1.276836633682251\n  Batch: 250 Loss: 1.2767274379730225\n  Batch: 251 Loss: 1.279037356376648\n  Batch: 252 Loss: 1.276619791984558\n  Batch: 253 Loss: 1.27789306640625\n  Batch: 254 Loss: 1.2748883962631226\n  Batch: 255 Loss: 1.2743948698043823\n  Batch: 256 Loss: 1.270761251449585\n  Batch: 257 Loss: 1.2757928371429443\n  Batch: 258 Loss: 1.278005838394165\n  Batch: 259 Loss: 1.2734289169311523\n  Batch: 260 Loss: 1.2764105796813965\n  Batch: 261 Loss: 1.2738820314407349\n  Batch: 262 Loss: 1.2758234739303589\n  Batch: 263 Loss: 1.271902084350586\n  Batch: 264 Loss: 1.3007258176803589\n  Batch: 265 Loss: 1.2758461236953735\n  Batch: 266 Loss: 1.2750047445297241\n  Batch: 267 Loss: 1.2734432220458984\n  Batch: 268 Loss: 1.2744204998016357\n  Batch: 269 Loss: 1.2734179496765137\n  Batch: 270 Loss: 1.2703919410705566\n  Batch: 271 Loss: 1.2744038105010986\n  Batch: 272 Loss: 1.2695177793502808\n  Batch: 273 Loss: 1.2723116874694824\n  Batch: 274 Loss: 1.2893271446228027\n  Batch: 275 Loss: 1.2723472118377686\n  Batch: 276 Loss: 1.2688380479812622\n  Batch: 277 Loss: 1.2730958461761475\n  Batch: 278 Loss: 1.2692694664001465\n  Batch: 279 Loss: 1.2719614505767822\n  Batch: 280 Loss: 1.2713996171951294\n  Batch: 281 Loss: 1.2718900442123413\n  Batch: 282 Loss: 1.2703748941421509\n  Batch: 283 Loss: 1.2692123651504517\n  Batch: 284 Loss: 1.2686805725097656\n  Batch: 285 Loss: 1.2694482803344727\n  Batch: 286 Loss: 1.2714579105377197\n  Batch: 287 Loss: 1.2695190906524658\n  Batch: 288 Loss: 1.2647780179977417\n  Batch: 289 Loss: 1.2668499946594238\n  Batch: 290 Loss: 1.2685104608535767\n  Batch: 291 Loss: 1.2681257724761963\n  Batch: 292 Loss: 1.2642825841903687\n  Batch: 293 Loss: 1.2567702531814575\n  Batch: 294 Loss: 1.2659642696380615\n  Batch: 295 Loss: 1.266596794128418\n  Batch: 296 Loss: 1.2644065618515015\n  Batch: 297 Loss: 1.2667807340621948\n  Batch: 298 Loss: 1.2674695253372192\n  Batch: 299 Loss: 1.265333890914917\n  Batch: 300 Loss: 1.2642154693603516\n  Batch: 301 Loss: 1.2658706903457642\n  Batch: 302 Loss: 1.2607783079147339\n  Batch: 303 Loss: 1.2623727321624756\n  Batch: 304 Loss: 1.2604737281799316\n  Batch: 305 Loss: 1.2610613107681274\n  Batch: 306 Loss: 1.2633647918701172\n  Batch: 307 Loss: 1.2618467807769775\n  Batch: 308 Loss: 1.2617692947387695\n  Batch: 309 Loss: 1.2609282732009888\n  Batch: 310 Loss: 1.2627934217453003\n  Batch: 311 Loss: 1.2626032829284668\n  Batch: 312 Loss: 1.265457034111023\n  Batch: 313 Loss: 1.263806700706482\n  Batch: 314 Loss: 1.2579221725463867\n  Batch: 315 Loss: 1.2577030658721924\n  Batch: 316 Loss: 1.2582945823669434\n  Batch: 317 Loss: 1.2565234899520874\n  Batch: 318 Loss: 1.2573070526123047\n  Batch: 319 Loss: 1.2608367204666138\n  Batch: 320 Loss: 1.2552599906921387\n  Batch: 321 Loss: 1.2624061107635498\n  Batch: 322 Loss: 1.2600958347320557\n  Batch: 323 Loss: 1.2592926025390625\n  Batch: 324 Loss: 1.2555384635925293\n  Batch: 325 Loss: 1.2571104764938354\n  Batch: 326 Loss: 1.2586897611618042\n  Batch: 327 Loss: 1.2541998624801636\n  Batch: 328 Loss: 1.2695791721343994\n  Batch: 329 Loss: 1.2587370872497559\n  Batch: 330 Loss: 1.2545663118362427\n  Batch: 331 Loss: 1.253873348236084\n  Batch: 332 Loss: 1.254274845123291\n  Batch: 333 Loss: 1.2556817531585693\n  Batch: 334 Loss: 1.2557058334350586\n  Batch: 335 Loss: 1.2552635669708252\n  Batch: 336 Loss: 1.252890944480896\nEpoch: 2 Loss: 1.3074420307363783\n  Batch: 1 Loss: 1.2554813623428345\n  Batch: 2 Loss: 1.2513368129730225\n  Batch: 3 Loss: 1.2544649839401245\n  Batch: 4 Loss: 1.2511351108551025\n  Batch: 5 Loss: 1.2533369064331055\n  Batch: 6 Loss: 1.2525923252105713\n  Batch: 7 Loss: 1.251884937286377\n  Batch: 8 Loss: 1.2538503408432007\n  Batch: 9 Loss: 1.2511415481567383\n  Batch: 10 Loss: 1.2525089979171753\n  Batch: 11 Loss: 1.249944806098938\n  Batch: 12 Loss: 1.2525293827056885\n  Batch: 13 Loss: 1.2493759393692017\n  Batch: 14 Loss: 1.2504997253417969\n  Batch: 15 Loss: 1.2516409158706665\n  Batch: 16 Loss: 1.248122215270996\n  Batch: 17 Loss: 1.24813711643219\n  Batch: 18 Loss: 1.2474448680877686\n  Batch: 19 Loss: 1.260301947593689\n  Batch: 20 Loss: 1.2492843866348267\n  Batch: 21 Loss: 1.2528443336486816\n  Batch: 22 Loss: 1.2464351654052734\n  Batch: 23 Loss: 1.2491668462753296\n  Batch: 24 Loss: 1.246326208114624\n  Batch: 25 Loss: 1.2518678903579712\n  Batch: 26 Loss: 1.2461299896240234\n  Batch: 27 Loss: 1.2486294507980347\n  Batch: 28 Loss: 1.2492315769195557\n  Batch: 29 Loss: 1.2474963665008545\n  Batch: 30 Loss: 1.2483512163162231\n  Batch: 31 Loss: 1.2474801540374756\n  Batch: 32 Loss: 1.241655707359314\n  Batch: 33 Loss: 1.2473931312561035\n  Batch: 34 Loss: 1.2436825037002563\n  Batch: 35 Loss: 1.242867350578308\n  Batch: 36 Loss: 1.2427363395690918\n  Batch: 37 Loss: 1.2431021928787231\n  Batch: 38 Loss: 1.250780463218689\n  Batch: 39 Loss: 1.242513656616211\n  Batch: 40 Loss: 1.2440968751907349\n  Batch: 41 Loss: 1.2445353269577026\n  Batch: 42 Loss: 1.2427022457122803\n  Batch: 43 Loss: 1.241245985031128\n  Batch: 44 Loss: 1.2445729970932007\n  Batch: 45 Loss: 1.243048906326294\n  Batch: 46 Loss: 1.2429651021957397\n  Batch: 47 Loss: 1.2413828372955322\n  Batch: 48 Loss: 1.2415556907653809\n  Batch: 49 Loss: 1.2424038648605347\n  Batch: 50 Loss: 1.2398580312728882\n  Batch: 51 Loss: 1.2391315698623657\n  Batch: 52 Loss: 1.2410340309143066\n  Batch: 53 Loss: 1.2408331632614136\n  Batch: 54 Loss: 1.2383842468261719\n  Batch: 55 Loss: 1.2397187948226929\n  Batch: 56 Loss: 1.2372524738311768\n  Batch: 57 Loss: 1.25288987159729\n  Batch: 58 Loss: 1.2411818504333496\n  Batch: 59 Loss: 1.2414270639419556\n  Batch: 60 Loss: 1.2370657920837402\n  Batch: 61 Loss: 1.2402327060699463\n  Batch: 62 Loss: 1.236652135848999\n  Batch: 63 Loss: 1.2400329113006592\n  Batch: 64 Loss: 1.2378954887390137\n  Batch: 65 Loss: 1.236973524093628\n  Batch: 66 Loss: 1.238053321838379\n  Batch: 67 Loss: 1.2352278232574463\n  Batch: 68 Loss: 1.237512469291687\n  Batch: 69 Loss: 1.2385225296020508\n  Batch: 70 Loss: 1.2376673221588135\n  Batch: 71 Loss: 1.236910343170166\n  Batch: 72 Loss: 1.2356284856796265\n  Batch: 73 Loss: 1.2348577976226807\n  Batch: 74 Loss: 1.2355928421020508\n  Batch: 75 Loss: 1.2376534938812256\n  Batch: 76 Loss: 1.2358472347259521\n  Batch: 77 Loss: 1.2361564636230469\n  Batch: 78 Loss: 1.2325513362884521\n  Batch: 79 Loss: 1.2323466539382935\n  Batch: 80 Loss: 1.2316642999649048\n  Batch: 81 Loss: 1.2307987213134766\n  Batch: 82 Loss: 1.233290195465088\n  Batch: 83 Loss: 1.2318085432052612\n  Batch: 84 Loss: 1.231425404548645\n  Batch: 85 Loss: 1.2309328317642212\n  Batch: 86 Loss: 1.2313095331192017\n  Batch: 87 Loss: 1.2300684452056885\n  Batch: 88 Loss: 1.232527494430542\n  Batch: 89 Loss: 1.2346726655960083\n  Batch: 90 Loss: 1.2307385206222534\n  Batch: 91 Loss: 1.2311809062957764\n  Batch: 92 Loss: 1.231921911239624\n  Batch: 93 Loss: 1.2229877710342407\n  Batch: 94 Loss: 1.230967402458191\n  Batch: 95 Loss: 1.2314740419387817\n  Batch: 96 Loss: 1.2292606830596924\n  Batch: 97 Loss: 1.2286955118179321\n  Batch: 98 Loss: 1.232723593711853\n  Batch: 99 Loss: 1.2313131093978882\n  Batch: 100 Loss: 1.2302093505859375\n  Batch: 101 Loss: 1.2288665771484375\n  Batch: 102 Loss: 1.227708101272583\n  Batch: 103 Loss: 1.235443353652954\n  Batch: 104 Loss: 1.226970911026001\n  Batch: 105 Loss: 1.2291195392608643\n  Batch: 106 Loss: 1.2290592193603516\n  Batch: 107 Loss: 1.2293158769607544\n  Batch: 108 Loss: 1.2291518449783325\n  Batch: 109 Loss: 1.2273199558258057\n  Batch: 110 Loss: 1.2253018617630005\n  Batch: 111 Loss: 1.2281216382980347\n  Batch: 112 Loss: 1.2250275611877441\n  Batch: 113 Loss: 1.2280243635177612\n  Batch: 114 Loss: 1.2245140075683594\n  Batch: 115 Loss: 1.2274837493896484\n  Batch: 116 Loss: 1.2270755767822266\n  Batch: 117 Loss: 1.2254358530044556\n  Batch: 118 Loss: 1.2266024351119995\n  Batch: 119 Loss: 1.224721908569336\n  Batch: 120 Loss: 1.225171446800232\n  Batch: 121 Loss: 1.2259869575500488\n  Batch: 122 Loss: 1.2254011631011963\n  Batch: 123 Loss: 1.224631428718567\n  Batch: 124 Loss: 1.2249743938446045\n  Batch: 125 Loss: 1.2200847864151\n  Batch: 126 Loss: 1.221609115600586\n  Batch: 127 Loss: 1.2236037254333496\n  Batch: 128 Loss: 1.2232658863067627\n  Batch: 129 Loss: 1.2243072986602783\n  Batch: 130 Loss: 1.2226358652114868\n  Batch: 131 Loss: 1.223654866218567\n  Batch: 132 Loss: 1.223639965057373\n  Batch: 133 Loss: 1.2213187217712402\n  Batch: 134 Loss: 1.2286832332611084\n  Batch: 135 Loss: 1.2261574268341064\n  Batch: 136 Loss: 1.2211376428604126\n  Batch: 137 Loss: 1.2193852663040161\n  Batch: 138 Loss: 1.2215023040771484\n  Batch: 139 Loss: 1.2221100330352783\n  Batch: 140 Loss: 1.222666621208191\n  Batch: 141 Loss: 1.220847725868225\n  Batch: 142 Loss: 1.2207889556884766\n  Batch: 143 Loss: 1.220699429512024\n  Batch: 144 Loss: 1.2199651002883911\n  Batch: 145 Loss: 1.2191814184188843\n  Batch: 146 Loss: 1.2196769714355469\n  Batch: 147 Loss: 1.2185782194137573\n  Batch: 148 Loss: 1.2172900438308716\n  Batch: 149 Loss: 1.2168073654174805\n  Batch: 150 Loss: 1.2184357643127441\n  Batch: 151 Loss: 1.2159538269042969\n  Batch: 152 Loss: 1.2169921398162842\n  Batch: 153 Loss: 1.2172691822052002\n  Batch: 154 Loss: 1.2171672582626343\n  Batch: 155 Loss: 1.2175923585891724\n  Batch: 156 Loss: 1.215059757232666\n  Batch: 157 Loss: 1.2164453268051147\n  Batch: 158 Loss: 1.2149333953857422\n  Batch: 159 Loss: 1.214282512664795\n  Batch: 160 Loss: 1.214806318283081\n  Batch: 161 Loss: 1.2275149822235107\n  Batch: 162 Loss: 1.2140487432479858\n  Batch: 163 Loss: 1.2135043144226074\n  Batch: 164 Loss: 1.2136423587799072\n  Batch: 165 Loss: 1.2133835554122925\n  Batch: 166 Loss: 1.2196199893951416\n  Batch: 167 Loss: 1.2120821475982666\n  Batch: 168 Loss: 1.2155404090881348\n  Batch: 169 Loss: 1.2154802083969116\n  Batch: 170 Loss: 1.214386224746704\n  Batch: 171 Loss: 1.22163724899292\n  Batch: 172 Loss: 1.2143268585205078\n  Batch: 173 Loss: 1.2151648998260498\n  Batch: 174 Loss: 1.2143635749816895\n  Batch: 175 Loss: 1.2122308015823364\n  Batch: 176 Loss: 1.2142508029937744\n  Batch: 177 Loss: 1.2115192413330078\n  Batch: 178 Loss: 1.2118457555770874\n  Batch: 179 Loss: 1.2126202583312988\n  Batch: 180 Loss: 1.2105387449264526\n  Batch: 181 Loss: 1.2173447608947754\n  Batch: 182 Loss: 1.2109681367874146\n  Batch: 183 Loss: 1.2122036218643188\n  Batch: 184 Loss: 1.2109264135360718\n  Batch: 185 Loss: 1.2123677730560303\n  Batch: 186 Loss: 1.2099958658218384\n  Batch: 187 Loss: 1.2091108560562134\n  Batch: 188 Loss: 1.2103312015533447\n  Batch: 189 Loss: 1.2081139087677002\n  Batch: 190 Loss: 1.2089259624481201\n  Batch: 191 Loss: 1.2083615064620972\n  Batch: 192 Loss: 1.2075368165969849\n  Batch: 193 Loss: 1.2078155279159546\n  Batch: 194 Loss: 1.209423303604126\n  Batch: 195 Loss: 1.2091246843338013\n  Batch: 196 Loss: 1.2090250253677368\n  Batch: 197 Loss: 1.2068989276885986\n  Batch: 198 Loss: 1.2101521492004395\n  Batch: 199 Loss: 1.2063995599746704\n  Batch: 200 Loss: 1.215968132019043\n  Batch: 201 Loss: 1.2057085037231445\n  Batch: 202 Loss: 1.2132514715194702\n  Batch: 203 Loss: 1.2054105997085571\n  Batch: 204 Loss: 1.2077016830444336\n  Batch: 205 Loss: 1.2086689472198486\n  Batch: 206 Loss: 1.2059931755065918\n  Batch: 207 Loss: 1.204300045967102\n  Batch: 208 Loss: 1.208503007888794\n  Batch: 209 Loss: 1.2100988626480103\n  Batch: 210 Loss: 1.2065982818603516\n  Batch: 211 Loss: 1.2079038619995117\n  Batch: 212 Loss: 1.2201099395751953\n  Batch: 213 Loss: 1.2040505409240723\n  Batch: 214 Loss: 1.2051664590835571\n  Batch: 215 Loss: 1.2029354572296143\n  Batch: 216 Loss: 1.2037869691848755\n  Batch: 217 Loss: 1.2175976037979126\n  Batch: 218 Loss: 1.2043644189834595\n  Batch: 219 Loss: 1.2027757167816162\n  Batch: 220 Loss: 1.2061480283737183\n  Batch: 221 Loss: 1.2035186290740967\n  Batch: 222 Loss: 1.2081297636032104\n  Batch: 223 Loss: 1.202627182006836\n  Batch: 224 Loss: 1.2037779092788696\n  Batch: 225 Loss: 1.201241135597229\n  Batch: 226 Loss: 1.201236605644226\n  Batch: 227 Loss: 1.2058789730072021\n  Batch: 228 Loss: 1.2018945217132568\n  Batch: 229 Loss: 1.2004761695861816\n  Batch: 230 Loss: 1.2011020183563232\n  Batch: 231 Loss: 1.2008267641067505\n  Batch: 232 Loss: 1.2008312940597534\n  Batch: 233 Loss: 1.202483057975769\n  Batch: 234 Loss: 1.1997590065002441\n  Batch: 235 Loss: 1.1987721920013428\n  Batch: 236 Loss: 1.1996935606002808\n  Batch: 237 Loss: 1.198947548866272\n  Batch: 238 Loss: 1.2005733251571655\n  Batch: 239 Loss: 1.199531078338623\n  Batch: 240 Loss: 1.2122087478637695\n  Batch: 241 Loss: 1.2011016607284546\n  Batch: 242 Loss: 1.1978058815002441\n  Batch: 243 Loss: 1.1976782083511353\n  Batch: 244 Loss: 1.1983022689819336\n  Batch: 245 Loss: 1.199686884880066\n  Batch: 246 Loss: 1.2039105892181396\n  Batch: 247 Loss: 1.1971131563186646\n  Batch: 248 Loss: 1.1987829208374023\n  Batch: 249 Loss: 1.1968437433242798\n  Batch: 250 Loss: 1.1961530447006226\n  Batch: 251 Loss: 1.1961967945098877\n  Batch: 252 Loss: 1.199187994003296\n  Batch: 253 Loss: 1.1952259540557861\n  Batch: 254 Loss: 1.197091817855835\n  Batch: 255 Loss: 1.2113434076309204\n  Batch: 256 Loss: 1.1977542638778687\n  Batch: 257 Loss: 1.197434425354004\n  Batch: 258 Loss: 1.195963740348816\n  Batch: 259 Loss: 1.1987086534500122\n  Batch: 260 Loss: 1.1937264204025269\n  Batch: 261 Loss: 1.194669485092163\n  Batch: 262 Loss: 1.193872332572937\n  Batch: 263 Loss: 1.1950271129608154\n  Batch: 264 Loss: 1.197556495666504\n  Batch: 265 Loss: 1.1865943670272827\n  Batch: 266 Loss: 1.1942137479782104\n  Batch: 267 Loss: 1.1943110227584839\n  Batch: 268 Loss: 1.1968779563903809\n  Batch: 269 Loss: 1.1963838338851929\n  Batch: 270 Loss: 1.215823769569397\n  Batch: 271 Loss: 1.1937272548675537\n  Batch: 272 Loss: 1.1946579217910767\n  Batch: 273 Loss: 1.1973923444747925\n  Batch: 274 Loss: 1.1958253383636475\n  Batch: 275 Loss: 1.1916546821594238\n  Batch: 276 Loss: 1.193386435508728\n  Batch: 277 Loss: 1.1941767930984497\n  Batch: 278 Loss: 1.1929285526275635\n  Batch: 279 Loss: 1.1961101293563843\n  Batch: 280 Loss: 1.2004661560058594\n  Batch: 281 Loss: 1.1915007829666138\n  Batch: 282 Loss: 1.193294644355774\n  Batch: 283 Loss: 1.190664529800415\n  Batch: 284 Loss: 1.1928279399871826\n  Batch: 285 Loss: 1.1911919116973877\n  Batch: 286 Loss: 1.191603183746338\n  Batch: 287 Loss: 1.1915881633758545\n  Batch: 288 Loss: 1.19185209274292\n  Batch: 289 Loss: 1.1920689344406128\n  Batch: 290 Loss: 1.1904703378677368\n  Batch: 291 Loss: 1.1909221410751343\n  Batch: 292 Loss: 1.1932858228683472\n  Batch: 293 Loss: 1.1898412704467773\n  Batch: 294 Loss: 1.1878504753112793\n  Batch: 295 Loss: 1.1881734132766724\n  Batch: 296 Loss: 1.186013102531433\n  Batch: 297 Loss: 1.1892036199569702\n  Batch: 298 Loss: 1.1870827674865723\n  Batch: 299 Loss: 1.1894294023513794\n  Batch: 300 Loss: 1.1888455152511597\n  Batch: 301 Loss: 1.1882022619247437\n  Batch: 302 Loss: 1.1884009838104248\n  Batch: 303 Loss: 1.1888328790664673\n  Batch: 304 Loss: 1.1867642402648926\n  Batch: 305 Loss: 1.193289041519165\n  Batch: 306 Loss: 1.1857141256332397\n  Batch: 307 Loss: 1.18570077419281\n  Batch: 308 Loss: 1.1863057613372803\n  Batch: 309 Loss: 1.1850438117980957\n  Batch: 310 Loss: 1.1855977773666382\n  Batch: 311 Loss: 1.186568260192871\n  Batch: 312 Loss: 1.1829736232757568\n  Batch: 313 Loss: 1.18776273727417\n  Batch: 314 Loss: 1.1856281757354736\n  Batch: 315 Loss: 1.183254361152649\n  Batch: 316 Loss: 1.1839858293533325\n  Batch: 317 Loss: 1.1853821277618408\n  Batch: 318 Loss: 1.1867715120315552\n  Batch: 319 Loss: 1.1869486570358276\n  Batch: 320 Loss: 1.1845240592956543\n  Batch: 321 Loss: 1.1869242191314697\n  Batch: 322 Loss: 1.1856515407562256\n  Batch: 323 Loss: 1.184257984161377\n  Batch: 324 Loss: 1.184635877609253\n  Batch: 325 Loss: 1.1854040622711182\n  Batch: 326 Loss: 1.1835027933120728\n  Batch: 327 Loss: 1.1825836896896362\n  Batch: 328 Loss: 1.183186411857605\n  Batch: 329 Loss: 1.1824612617492676\n  Batch: 330 Loss: 1.1791298389434814\n  Batch: 331 Loss: 1.1858011484146118\n  Batch: 332 Loss: 1.1821645498275757\n  Batch: 333 Loss: 1.18204927444458\n  Batch: 334 Loss: 1.1834261417388916\n  Batch: 335 Loss: 1.1804111003875732\n  Batch: 336 Loss: 1.183523178100586\nEpoch: 3 Loss: 1.2159722120988936\n  Batch: 1 Loss: 1.1819117069244385\n  Batch: 2 Loss: 1.1821379661560059\n  Batch: 3 Loss: 1.1810694932937622\n  Batch: 4 Loss: 1.1872848272323608\n  Batch: 5 Loss: 1.1794992685317993\n  Batch: 6 Loss: 1.1750798225402832\n  Batch: 7 Loss: 1.181735873222351\n  Batch: 8 Loss: 1.1807688474655151\n  Batch: 9 Loss: 1.1813946962356567\n  Batch: 10 Loss: 1.178031086921692\n  Batch: 11 Loss: 1.1841139793395996\n  Batch: 12 Loss: 1.180367112159729\n  Batch: 13 Loss: 1.1778371334075928\n  Batch: 14 Loss: 1.1775308847427368\n  Batch: 15 Loss: 1.179152488708496\n  Batch: 16 Loss: 1.1812628507614136\n  Batch: 17 Loss: 1.177281379699707\n  Batch: 18 Loss: 1.1767042875289917\n  Batch: 19 Loss: 1.17806077003479\n  Batch: 20 Loss: 1.1790125370025635\n  Batch: 21 Loss: 1.179634690284729\n  Batch: 22 Loss: 1.1768581867218018\n  Batch: 23 Loss: 1.1774561405181885\n  Batch: 24 Loss: 1.176598310470581\n  Batch: 25 Loss: 1.1759129762649536\n  Batch: 26 Loss: 1.1769280433654785\n  Batch: 27 Loss: 1.1755329370498657\n  Batch: 28 Loss: 1.1750375032424927\n  Batch: 29 Loss: 1.1752734184265137\n  Batch: 30 Loss: 1.17585027217865\n  Batch: 31 Loss: 1.1747080087661743\n  Batch: 32 Loss: 1.174838900566101\n  Batch: 33 Loss: 1.175405502319336\n  Batch: 34 Loss: 1.1739574670791626\n  Batch: 35 Loss: 1.1751600503921509\n  Batch: 36 Loss: 1.1755948066711426\n  Batch: 37 Loss: 1.1747984886169434\n  Batch: 38 Loss: 1.178239107131958\n  Batch: 39 Loss: 1.1734873056411743\n  Batch: 40 Loss: 1.1732815504074097\n  Batch: 41 Loss: 1.1729004383087158\n  Batch: 42 Loss: 1.1744155883789062\n  Batch: 43 Loss: 1.1738629341125488\n  Batch: 44 Loss: 1.1721400022506714\n  Batch: 45 Loss: 1.1737149953842163\n  Batch: 46 Loss: 1.1715019941329956\n  Batch: 47 Loss: 1.1718347072601318\n  Batch: 48 Loss: 1.1713414192199707\n  Batch: 49 Loss: 1.17390775680542\n  Batch: 50 Loss: 1.171937346458435\n  Batch: 51 Loss: 1.1718389987945557\n  Batch: 52 Loss: 1.174760103225708\n  Batch: 53 Loss: 1.170681357383728\n  Batch: 54 Loss: 1.171432375907898\n  Batch: 55 Loss: 1.1694529056549072\n  Batch: 56 Loss: 1.1708871126174927\n  Batch: 57 Loss: 1.172168493270874\n  Batch: 58 Loss: 1.1717549562454224\n  Batch: 59 Loss: 1.1696856021881104\n  Batch: 60 Loss: 1.1710675954818726\n  Batch: 61 Loss: 1.169669270515442\n  Batch: 62 Loss: 1.1700197458267212\n  Batch: 63 Loss: 1.1700669527053833\n  Batch: 64 Loss: 1.170112133026123\n  Batch: 65 Loss: 1.1692270040512085\n  Batch: 66 Loss: 1.1684478521347046\n  Batch: 67 Loss: 1.1696348190307617\n  Batch: 68 Loss: 1.1665149927139282\n  Batch: 69 Loss: 1.1690442562103271\n  Batch: 70 Loss: 1.1678718328475952\n  Batch: 71 Loss: 1.1678123474121094\n  Batch: 72 Loss: 1.170212984085083\n  Batch: 73 Loss: 1.1688156127929688\n  Batch: 74 Loss: 1.1668883562088013\n  Batch: 75 Loss: 1.1676181554794312\n  Batch: 76 Loss: 1.168487548828125\n  Batch: 77 Loss: 1.1631484031677246\n  Batch: 78 Loss: 1.1668189764022827\n  Batch: 79 Loss: 1.1691406965255737\n  Batch: 80 Loss: 1.1673465967178345\n  Batch: 81 Loss: 1.1672556400299072\n  Batch: 82 Loss: 1.1643702983856201\n  Batch: 83 Loss: 1.1666227579116821\n  Batch: 84 Loss: 1.1663812398910522\n  Batch: 85 Loss: 1.1658234596252441\n  Batch: 86 Loss: 1.166527271270752\n  Batch: 87 Loss: 1.1661776304244995\n  Batch: 88 Loss: 1.1652741432189941\n  Batch: 89 Loss: 1.164799690246582\n  Batch: 90 Loss: 1.1649010181427002\n  Batch: 91 Loss: 1.164991855621338\n  Batch: 92 Loss: 1.1659934520721436\n  Batch: 93 Loss: 1.1658766269683838\n  Batch: 94 Loss: 1.1651992797851562\n  Batch: 95 Loss: 1.1661310195922852\n  Batch: 96 Loss: 1.163475513458252\n  Batch: 97 Loss: 1.1642920970916748\n  Batch: 98 Loss: 1.164920687675476\n  Batch: 99 Loss: 1.1695842742919922\n  Batch: 100 Loss: 1.1636593341827393\n  Batch: 101 Loss: 1.1635172367095947\n  Batch: 102 Loss: 1.1631122827529907\n  Batch: 103 Loss: 1.163513422012329\n  Batch: 104 Loss: 1.1626962423324585\n  Batch: 105 Loss: 1.1624016761779785\n  Batch: 106 Loss: 1.1622201204299927\n  Batch: 107 Loss: 1.1639735698699951\n  Batch: 108 Loss: 1.1619192361831665\n  Batch: 109 Loss: 1.1621803045272827\n  Batch: 110 Loss: 1.1499361991882324\n  Batch: 111 Loss: 1.161301612854004\n  Batch: 112 Loss: 1.1636326313018799\n  Batch: 113 Loss: 1.1624479293823242\n  Batch: 114 Loss: 1.1608774662017822\n  Batch: 115 Loss: 1.1609127521514893\n  Batch: 116 Loss: 1.1609091758728027\n  Batch: 117 Loss: 1.1585496664047241\n  Batch: 118 Loss: 1.161633849143982\n  Batch: 119 Loss: 1.1597857475280762\n  Batch: 120 Loss: 1.1566238403320312\n  Batch: 121 Loss: 1.1598129272460938\n  Batch: 122 Loss: 1.1596894264221191\n  Batch: 123 Loss: 1.1606464385986328\n  Batch: 124 Loss: 1.161561369895935\n  Batch: 125 Loss: 1.159462332725525\n  Batch: 126 Loss: 1.1604814529418945\n  Batch: 127 Loss: 1.1598037481307983\n  Batch: 128 Loss: 1.158477783203125\n  Batch: 129 Loss: 1.1589964628219604\n  Batch: 130 Loss: 1.159900426864624\n  Batch: 131 Loss: 1.1581952571868896\n  Batch: 132 Loss: 1.1581006050109863\n  Batch: 133 Loss: 1.157850980758667\n  Batch: 134 Loss: 1.157652735710144\n  Batch: 135 Loss: 1.1580708026885986\n  Batch: 136 Loss: 1.1567941904067993\n  Batch: 137 Loss: 1.157556176185608\n  Batch: 138 Loss: 1.1565030813217163\n  Batch: 139 Loss: 1.1577181816101074\n  Batch: 140 Loss: 1.156542181968689\n  Batch: 141 Loss: 1.157301902770996\n  Batch: 142 Loss: 1.1566766500473022\n  Batch: 143 Loss: 1.1562387943267822\n  Batch: 144 Loss: 1.1560883522033691\n  Batch: 145 Loss: 1.155875325202942\n  Batch: 146 Loss: 1.156951665878296\n  Batch: 147 Loss: 1.1552579402923584\n  Batch: 148 Loss: 1.1565133333206177\n  Batch: 149 Loss: 1.1550558805465698\n  Batch: 150 Loss: 1.1547154188156128\n  Batch: 151 Loss: 1.1549714803695679\n  Batch: 152 Loss: 1.155367374420166\n  Batch: 153 Loss: 1.1547234058380127\n  Batch: 154 Loss: 1.1545566320419312\n  Batch: 155 Loss: 1.1542719602584839\n  Batch: 156 Loss: 1.1547439098358154\n  Batch: 157 Loss: 1.1554075479507446\n  Batch: 158 Loss: 1.1559193134307861\n  Batch: 159 Loss: 1.154323935508728\n  Batch: 160 Loss: 1.1560330390930176\n  Batch: 161 Loss: 1.1501191854476929\n  Batch: 162 Loss: 1.1534181833267212\n  Batch: 163 Loss: 1.1467175483703613\n  Batch: 164 Loss: 1.1491079330444336\n  Batch: 165 Loss: 1.1545329093933105\n  Batch: 166 Loss: 1.1544159650802612\n  Batch: 167 Loss: 1.1531540155410767\n  Batch: 168 Loss: 1.1558561325073242\n  Batch: 169 Loss: 1.154992938041687\n  Batch: 170 Loss: 1.1539801359176636\n  Batch: 171 Loss: 1.154754877090454\n  Batch: 172 Loss: 1.1534100770950317\n  Batch: 173 Loss: 1.152191400527954\n  Batch: 174 Loss: 1.1524523496627808\n  Batch: 175 Loss: 1.1520978212356567\n  Batch: 176 Loss: 1.151691198348999\n  Batch: 177 Loss: 1.1543505191802979\n  Batch: 178 Loss: 1.1507954597473145\n  Batch: 179 Loss: 1.1509838104248047\n  Batch: 180 Loss: 1.153545618057251\n  Batch: 181 Loss: 1.151811957359314\n  Batch: 182 Loss: 1.1507782936096191\n  Batch: 183 Loss: 1.1514835357666016\n  Batch: 184 Loss: 1.1516724824905396\n  Batch: 185 Loss: 1.1501880884170532\n  Batch: 186 Loss: 1.1513330936431885\n  Batch: 187 Loss: 1.1503082513809204\n  Batch: 188 Loss: 1.1502608060836792\n  Batch: 189 Loss: 1.1519746780395508\n  Batch: 190 Loss: 1.1436429023742676\n  Batch: 191 Loss: 1.150605320930481\n  Batch: 192 Loss: 1.1527034044265747\n  Batch: 193 Loss: 1.1507923603057861\n  Batch: 194 Loss: 1.1491628885269165\n  Batch: 195 Loss: 1.1486490964889526\n  Batch: 196 Loss: 1.1491345167160034\n  Batch: 197 Loss: 1.1656053066253662\n  Batch: 198 Loss: 1.1486873626708984\n  Batch: 199 Loss: 1.1497776508331299\n  Batch: 200 Loss: 1.149043321609497\n  Batch: 201 Loss: 1.1477349996566772\n  Batch: 202 Loss: 1.149171233177185\n  Batch: 203 Loss: 1.147387146949768\n  Batch: 204 Loss: 1.1479742527008057\n  Batch: 205 Loss: 1.148470163345337\n  Batch: 206 Loss: 1.1494808197021484\n  Batch: 207 Loss: 1.1473283767700195\n  Batch: 208 Loss: 1.1478440761566162\n  Batch: 209 Loss: 1.1478192806243896\n  Batch: 210 Loss: 1.1476895809173584\n  Batch: 211 Loss: 1.146734595298767\n  Batch: 212 Loss: 1.1462761163711548\n  Batch: 213 Loss: 1.1469571590423584\n  Batch: 214 Loss: 1.1465190649032593\n  Batch: 215 Loss: 1.1446566581726074\n  Batch: 216 Loss: 1.1458319425582886\n  Batch: 217 Loss: 1.1497536897659302\n  Batch: 218 Loss: 1.1445823907852173\n  Batch: 219 Loss: 1.1456680297851562\n  Batch: 220 Loss: 1.145163893699646\n  Batch: 221 Loss: 1.1458169221878052\n  Batch: 222 Loss: 1.1443997621536255\n  Batch: 223 Loss: 1.1458518505096436\n  Batch: 224 Loss: 1.1452590227127075\n  Batch: 225 Loss: 1.1450237035751343\n  Batch: 226 Loss: 1.143837332725525\n  Batch: 227 Loss: 1.144877552986145\n  Batch: 228 Loss: 1.1449311971664429\n  Batch: 229 Loss: 1.146117925643921\n  Batch: 230 Loss: 1.1443414688110352\n  Batch: 231 Loss: 1.1392600536346436\n  Batch: 232 Loss: 1.144659399986267\n  Batch: 233 Loss: 1.143527865409851\n  Batch: 234 Loss: 1.1387548446655273\n  Batch: 235 Loss: 1.1433701515197754\n  Batch: 236 Loss: 1.1435626745224\n  Batch: 237 Loss: 1.1427444219589233\n  Batch: 238 Loss: 1.1424705982208252\n  Batch: 239 Loss: 1.14315927028656\n  Batch: 240 Loss: 1.1429078578948975\n  Batch: 241 Loss: 1.142206072807312\n  Batch: 242 Loss: 1.1419048309326172\n  Batch: 243 Loss: 1.1421006917953491\n  Batch: 244 Loss: 1.141955852508545\n  Batch: 245 Loss: 1.1424680948257446\n  Batch: 246 Loss: 1.1412923336029053\n  Batch: 247 Loss: 1.1421098709106445\n  Batch: 248 Loss: 1.1417312622070312\n  Batch: 249 Loss: 1.1407997608184814\n  Batch: 250 Loss: 1.1438794136047363\n  Batch: 251 Loss: 1.1427829265594482\n  Batch: 252 Loss: 1.141876220703125\n  Batch: 253 Loss: 1.1406641006469727\n  Batch: 254 Loss: 1.1416032314300537\n  Batch: 255 Loss: 1.1402829885482788\n  Batch: 256 Loss: 1.141220211982727\n  Batch: 257 Loss: 1.1399173736572266\n  Batch: 258 Loss: 1.1398563385009766\n  Batch: 259 Loss: 1.140795111656189\n  Batch: 260 Loss: 1.1385622024536133\n  Batch: 261 Loss: 1.1391209363937378\n  Batch: 262 Loss: 1.1366854906082153\n  Batch: 263 Loss: 1.1391887664794922\n  Batch: 264 Loss: 1.140836238861084\n  Batch: 265 Loss: 1.1385524272918701\n  Batch: 266 Loss: 1.1407594680786133\n  Batch: 267 Loss: 1.139646291732788\n  Batch: 268 Loss: 1.1390033960342407\n  Batch: 269 Loss: 1.1387392282485962\n  Batch: 270 Loss: 1.1387200355529785\n  Batch: 271 Loss: 1.1381233930587769\n  Batch: 272 Loss: 1.1377196311950684\n  Batch: 273 Loss: 1.140160322189331\n  Batch: 274 Loss: 1.1389870643615723\n  Batch: 275 Loss: 1.1391730308532715\n  Batch: 276 Loss: 1.138324499130249\n  Batch: 277 Loss: 1.1375172138214111\n  Batch: 278 Loss: 1.1384416818618774\n  Batch: 279 Loss: 1.1376811265945435\n  Batch: 280 Loss: 1.1380057334899902\n  Batch: 281 Loss: 1.1369054317474365\n  Batch: 282 Loss: 1.1367545127868652\n  Batch: 283 Loss: 1.1377487182617188\n  Batch: 284 Loss: 1.1365587711334229\n  Batch: 285 Loss: 1.1354901790618896\n  Batch: 286 Loss: 1.1373236179351807\n  Batch: 287 Loss: 1.1368094682693481\n  Batch: 288 Loss: 1.1358132362365723\n  Batch: 289 Loss: 1.1359275579452515\n  Batch: 290 Loss: 1.1357207298278809\n  Batch: 291 Loss: 1.1368064880371094\n  Batch: 292 Loss: 1.1365927457809448\n  Batch: 293 Loss: 1.1360485553741455\n  Batch: 294 Loss: 1.1359069347381592\n  Batch: 295 Loss: 1.1318804025650024\n  Batch: 296 Loss: 1.1366440057754517\n  Batch: 297 Loss: 1.1352955102920532\n  Batch: 298 Loss: 1.1346590518951416\n  Batch: 299 Loss: 1.1352558135986328\n  Batch: 300 Loss: 1.138157606124878\n  Batch: 301 Loss: 1.1349616050720215\n  Batch: 302 Loss: 1.1332470178604126\n  Batch: 303 Loss: 1.1338467597961426\n  Batch: 304 Loss: 1.1339448690414429\n  Batch: 305 Loss: 1.1336756944656372\n  Batch: 306 Loss: 1.1334127187728882\n  Batch: 307 Loss: 1.1334058046340942\n  Batch: 308 Loss: 1.1327636241912842\n  Batch: 309 Loss: 1.1459448337554932\n  Batch: 310 Loss: 1.132950782775879\n  Batch: 311 Loss: 1.1341242790222168\n  Batch: 312 Loss: 1.1333295106887817\n  Batch: 313 Loss: 1.133730411529541\n  Batch: 314 Loss: 1.1327486038208008\n  Batch: 315 Loss: 1.1328823566436768\n  Batch: 316 Loss: 1.1340463161468506\n  Batch: 317 Loss: 1.1323778629302979\n  Batch: 318 Loss: 1.1354305744171143\n  Batch: 319 Loss: 1.1314643621444702\n  Batch: 320 Loss: 1.1317732334136963\n  Batch: 321 Loss: 1.133148431777954\n  Batch: 322 Loss: 1.1318747997283936\n  Batch: 323 Loss: 1.1325771808624268\n  Batch: 324 Loss: 1.1314947605133057\n  Batch: 325 Loss: 1.1310760974884033\n  Batch: 326 Loss: 1.1305255889892578\n  Batch: 327 Loss: 1.1318624019622803\n  Batch: 328 Loss: 1.1317346096038818\n  Batch: 329 Loss: 1.131277322769165\n  Batch: 330 Loss: 1.1309432983398438\n  Batch: 331 Loss: 1.1304200887680054\n  Batch: 332 Loss: 1.130773663520813\n  Batch: 333 Loss: 1.1305543184280396\n  Batch: 334 Loss: 1.132452130317688\n  Batch: 335 Loss: 1.1300106048583984\n  Batch: 336 Loss: 1.1298508644104004\nEpoch: 4 Loss: 1.1539780384018308\n  Batch: 1 Loss: 1.1307576894760132\n  Batch: 2 Loss: 1.1296712160110474\n  Batch: 3 Loss: 1.1295671463012695\n  Batch: 4 Loss: 1.1292067766189575\n  Batch: 5 Loss: 1.1291754245758057\n  Batch: 6 Loss: 1.1293712854385376\n  Batch: 7 Loss: 1.1254574060440063\n  Batch: 8 Loss: 1.130694031715393\n  Batch: 9 Loss: 1.1289207935333252\n  Batch: 10 Loss: 1.128593921661377\n  Batch: 11 Loss: 1.1284897327423096\n  Batch: 12 Loss: 1.1308739185333252\n  Batch: 13 Loss: 1.1281042098999023\n  Batch: 14 Loss: 1.1294291019439697\n  Batch: 15 Loss: 1.1277964115142822\n  Batch: 16 Loss: 1.1278393268585205\n  Batch: 17 Loss: 1.1288353204727173\n  Batch: 18 Loss: 1.1273932456970215\n  Batch: 19 Loss: 1.1365952491760254\n  Batch: 20 Loss: 1.1281358003616333\n  Batch: 21 Loss: 1.1281589269638062\n  Batch: 22 Loss: 1.1281695365905762\n  Batch: 23 Loss: 1.1270630359649658\n  Batch: 24 Loss: 1.1308565139770508\n  Batch: 25 Loss: 1.132860541343689\n  Batch: 26 Loss: 1.1259541511535645\n  Batch: 27 Loss: 1.1265199184417725\n  Batch: 28 Loss: 1.1263837814331055\n  Batch: 29 Loss: 1.1266831159591675\n  Batch: 30 Loss: 1.1261955499649048\n  Batch: 31 Loss: 1.126699447631836\n  Batch: 32 Loss: 1.126787781715393\n  Batch: 33 Loss: 1.1257539987564087\n  Batch: 34 Loss: 1.1267012357711792\n  Batch: 35 Loss: 1.1198642253875732\n  Batch: 36 Loss: 1.128086805343628\n  Batch: 37 Loss: 1.1254836320877075\n  Batch: 38 Loss: 1.1272412538528442\n  Batch: 39 Loss: 1.125963568687439\n  Batch: 40 Loss: 1.1260309219360352\n  Batch: 41 Loss: 1.1259795427322388\n  Batch: 42 Loss: 1.1249370574951172\n  Batch: 43 Loss: 1.1257671117782593\n  Batch: 44 Loss: 1.1255340576171875\n  Batch: 45 Loss: 1.1253023147583008\n  Batch: 46 Loss: 1.1250437498092651\n  Batch: 47 Loss: 1.1246845722198486\n  Batch: 48 Loss: 1.1242401599884033\n  Batch: 49 Loss: 1.1250344514846802\n  Batch: 50 Loss: 1.1247378587722778\n  Batch: 51 Loss: 1.124383807182312\n  Batch: 52 Loss: 1.1237791776657104\n  Batch: 53 Loss: 1.1230998039245605\n  Batch: 54 Loss: 1.1233469247817993\n  Batch: 55 Loss: 1.1269081830978394\n  Batch: 56 Loss: 1.1243391036987305\n  Batch: 57 Loss: 1.1237006187438965\n  Batch: 58 Loss: 1.122896432876587\n  Batch: 59 Loss: 1.1235864162445068\n  Batch: 60 Loss: 1.1243094205856323\n  Batch: 61 Loss: 1.1225894689559937\n  Batch: 62 Loss: 1.1232306957244873\n  Batch: 63 Loss: 1.1204453706741333\n  Batch: 64 Loss: 1.1472254991531372\n  Batch: 65 Loss: 1.123277187347412\n  Batch: 66 Loss: 1.122346043586731\n  Batch: 67 Loss: 1.1236900091171265\n  Batch: 68 Loss: 1.0958406925201416\n  Batch: 69 Loss: 1.1224640607833862\n  Batch: 70 Loss: 1.1230114698410034\n  Batch: 71 Loss: 1.12504243850708\n  Batch: 72 Loss: 1.1214653253555298\n  Batch: 73 Loss: 1.1222126483917236\n  Batch: 74 Loss: 1.1238082647323608\n  Batch: 75 Loss: 1.1194649934768677\n  Batch: 76 Loss: 1.1224184036254883\n  Batch: 77 Loss: 1.1200482845306396\n  Batch: 78 Loss: 1.121741533279419\n  Batch: 79 Loss: 1.122003197669983\n  Batch: 80 Loss: 1.1228008270263672\n  Batch: 81 Loss: 1.1225003004074097\n  Batch: 82 Loss: 1.1221753358840942\n  Batch: 83 Loss: 1.1201821565628052\n  Batch: 84 Loss: 1.1204054355621338\n  Batch: 85 Loss: 1.119994878768921\n  Batch: 86 Loss: 1.121385931968689\n  Batch: 87 Loss: 1.1138421297073364\n  Batch: 88 Loss: 1.120856761932373\n  Batch: 89 Loss: 1.1209704875946045\n  Batch: 90 Loss: 1.1222511529922485\n  Batch: 91 Loss: 1.1194565296173096\n  Batch: 92 Loss: 1.11897611618042\n  Batch: 93 Loss: 1.119486689567566\n  Batch: 94 Loss: 1.1197477579116821\n  Batch: 95 Loss: 1.1192946434020996\n  Batch: 96 Loss: 1.1160792112350464\n  Batch: 97 Loss: 1.1187609434127808\n  Batch: 98 Loss: 1.1186050176620483\n  Batch: 99 Loss: 1.1185141801834106\n  Batch: 100 Loss: 1.1184130907058716\n  Batch: 101 Loss: 1.1196297407150269\n  Batch: 102 Loss: 1.1227002143859863\n  Batch: 103 Loss: 1.1193671226501465\n  Batch: 104 Loss: 1.1186579465866089\n  Batch: 105 Loss: 1.118047833442688\n  Batch: 106 Loss: 1.119145154953003\n  Batch: 107 Loss: 1.119080662727356\n  Batch: 108 Loss: 1.1182554960250854\n  Batch: 109 Loss: 1.1181331872940063\n  Batch: 110 Loss: 1.1178001165390015\n  Batch: 111 Loss: 1.1178126335144043\n  Batch: 112 Loss: 1.1171917915344238\n  Batch: 113 Loss: 1.1168574094772339\n  Batch: 114 Loss: 1.1173017024993896\n  Batch: 115 Loss: 1.118544340133667\n  Batch: 116 Loss: 1.116441249847412\n  Batch: 117 Loss: 1.1116106510162354\n  Batch: 118 Loss: 1.116176962852478\n  Batch: 119 Loss: 1.117155909538269\n  Batch: 120 Loss: 1.1171079874038696\n  Batch: 121 Loss: 1.1170203685760498\n  Batch: 122 Loss: 1.1154258251190186\n  Batch: 123 Loss: 1.1158143281936646\n  Batch: 124 Loss: 1.1163123846054077\n  Batch: 125 Loss: 1.1242616176605225\n  Batch: 126 Loss: 1.1153932809829712\n  Batch: 127 Loss: 1.1155756711959839\n  Batch: 128 Loss: 1.1175358295440674\n  Batch: 129 Loss: 1.115206003189087\n  Batch: 130 Loss: 1.1172454357147217\n  Batch: 131 Loss: 1.1170237064361572\n  Batch: 132 Loss: 1.1167757511138916\n  Batch: 133 Loss: 1.1163804531097412\n  Batch: 134 Loss: 1.1156065464019775\n  Batch: 135 Loss: 1.1152722835540771\n  Batch: 136 Loss: 1.1156222820281982\n  Batch: 137 Loss: 1.1140649318695068\n  Batch: 138 Loss: 1.1031620502471924\n  Batch: 139 Loss: 1.11397385597229\n  Batch: 140 Loss: 1.114484190940857\n  Batch: 141 Loss: 1.1144211292266846\n  Batch: 142 Loss: 1.1130021810531616\n  Batch: 143 Loss: 1.1154258251190186\n  Batch: 144 Loss: 1.1144154071807861\n  Batch: 145 Loss: 1.1139700412750244\n  Batch: 146 Loss: 1.1088210344314575\n  Batch: 147 Loss: 1.1138712167739868\n  Batch: 148 Loss: 1.110036849975586\n  Batch: 149 Loss: 1.1157729625701904\n  Batch: 150 Loss: 1.1160346269607544\n  Batch: 151 Loss: 1.113316297531128\n  Batch: 152 Loss: 1.112983226776123\n  Batch: 153 Loss: 1.1153111457824707\n  Batch: 154 Loss: 1.108005404472351\n  Batch: 155 Loss: 1.118507981300354\n  Batch: 156 Loss: 1.1101665496826172\n  Batch: 157 Loss: 1.1124588251113892\n  Batch: 158 Loss: 1.1133716106414795\n  Batch: 159 Loss: 1.1141889095306396\n  Batch: 160 Loss: 1.1124917268753052\n  Batch: 161 Loss: 1.09074068069458\n  Batch: 162 Loss: 1.1068787574768066\n  Batch: 163 Loss: 1.1133854389190674\n  Batch: 164 Loss: 1.1106749773025513\n  Batch: 165 Loss: 1.11796236038208\n  Batch: 166 Loss: 1.111818790435791\n  Batch: 167 Loss: 1.1119328737258911\n  Batch: 168 Loss: 1.1149640083312988\n  Batch: 169 Loss: 1.105306625366211\n  Batch: 170 Loss: 1.111580491065979\n  Batch: 171 Loss: 1.1096738576889038\n  Batch: 172 Loss: 1.1104990243911743\n  Batch: 173 Loss: 1.1100040674209595\n  Batch: 174 Loss: 1.1139471530914307\n  Batch: 175 Loss: 1.1111284494400024\n  Batch: 176 Loss: 1.1131690740585327\n  Batch: 177 Loss: 1.1102532148361206\n  Batch: 178 Loss: 1.0974717140197754\n  Batch: 179 Loss: 1.1093862056732178\n  Batch: 180 Loss: 1.1100990772247314\n  Batch: 181 Loss: 1.1098872423171997\n  Batch: 182 Loss: 1.1073626279830933\n  Batch: 183 Loss: 1.1126314401626587\n  Batch: 184 Loss: 1.1092156171798706\n  Batch: 185 Loss: 1.1121841669082642\n  Batch: 186 Loss: 1.110421061515808\n  Batch: 187 Loss: 1.1092040538787842\n  Batch: 188 Loss: 1.1093753576278687\n  Batch: 189 Loss: 1.1104786396026611\n  Batch: 190 Loss: 1.115511417388916\n  Batch: 191 Loss: 1.10993492603302\n  Batch: 192 Loss: 1.1085642576217651\n  Batch: 193 Loss: 1.1085138320922852\n  Batch: 194 Loss: 1.1102879047393799\n  Batch: 195 Loss: 1.1097838878631592\n  Batch: 196 Loss: 1.1085150241851807\n  Batch: 197 Loss: 1.108282208442688\n  Batch: 198 Loss: 1.1079895496368408\n  Batch: 199 Loss: 1.108489990234375\n  Batch: 200 Loss: 1.108439564704895\n  Batch: 201 Loss: 1.1077311038970947\n  Batch: 202 Loss: 1.1082038879394531\n  Batch: 203 Loss: 1.1074004173278809\n  Batch: 204 Loss: 1.1073862314224243\n  Batch: 205 Loss: 1.1070396900177002\n  Batch: 206 Loss: 1.1074081659317017\n  Batch: 207 Loss: 1.101452112197876\n  Batch: 208 Loss: 1.1069567203521729\n  Batch: 209 Loss: 1.1076658964157104\n  Batch: 210 Loss: 1.1061136722564697\n  Batch: 211 Loss: 1.1075897216796875\n  Batch: 212 Loss: 1.1073411703109741\n  Batch: 213 Loss: 1.1062909364700317\n  Batch: 214 Loss: 1.1071957349777222\n  Batch: 215 Loss: 1.106175422668457\n  Batch: 216 Loss: 1.106702208518982\n  Batch: 217 Loss: 1.1058446168899536\n  Batch: 218 Loss: 1.1060107946395874\n  Batch: 219 Loss: 1.1059237718582153\n  Batch: 220 Loss: 1.1063810586929321\n  Batch: 221 Loss: 1.1054054498672485\n  Batch: 222 Loss: 1.105810284614563\n  Batch: 223 Loss: 1.1062092781066895\n  Batch: 224 Loss: 1.105645775794983\n  Batch: 225 Loss: 1.105027437210083\n  Batch: 226 Loss: 1.1076292991638184\n  Batch: 227 Loss: 1.1050288677215576\n  Batch: 228 Loss: 1.1061780452728271\n  Batch: 229 Loss: 1.105157732963562\n  Batch: 230 Loss: 1.1045480966567993\n  Batch: 231 Loss: 1.1030552387237549\n  Batch: 232 Loss: 1.1047074794769287\n  Batch: 233 Loss: 1.104020595550537\n  Batch: 234 Loss: 1.104238748550415\n  Batch: 235 Loss: 1.1058896780014038\n  Batch: 236 Loss: 1.1043777465820312\n  Batch: 237 Loss: 1.105347990989685\n  Batch: 238 Loss: 1.1048474311828613\n  Batch: 239 Loss: 1.1044026613235474\n  Batch: 240 Loss: 1.103611707687378\n  Batch: 241 Loss: 1.1046284437179565\n  Batch: 242 Loss: 1.103150725364685\n  Batch: 243 Loss: 1.1047637462615967\n  Batch: 244 Loss: 1.1029791831970215\n  Batch: 245 Loss: 1.1032376289367676\n  Batch: 246 Loss: 1.103482961654663\n  Batch: 247 Loss: 1.1030187606811523\n  Batch: 248 Loss: 1.101639986038208\n  Batch: 249 Loss: 1.101976752281189\n  Batch: 250 Loss: 1.103495717048645\n  Batch: 251 Loss: 1.1040235757827759\n  Batch: 252 Loss: 1.103736400604248\n  Batch: 253 Loss: 1.1023542881011963\n  Batch: 254 Loss: 1.1033172607421875\n  Batch: 255 Loss: 1.1031169891357422\n  Batch: 256 Loss: 1.1029049158096313\n  Batch: 257 Loss: 1.1030701398849487\n  Batch: 258 Loss: 1.102198600769043\n  Batch: 259 Loss: 1.1024785041809082\n  Batch: 260 Loss: 1.1063573360443115\n  Batch: 261 Loss: 1.1024749279022217\n  Batch: 262 Loss: 1.1014318466186523\n  Batch: 263 Loss: 1.101738452911377\n  Batch: 264 Loss: 1.1012861728668213\n  Batch: 265 Loss: 1.101333737373352\n  Batch: 266 Loss: 1.101536750793457\n  Batch: 267 Loss: 1.0731537342071533\n  Batch: 268 Loss: 1.1015818119049072\n  Batch: 269 Loss: 1.0926592350006104\n  Batch: 270 Loss: 1.101086974143982\n  Batch: 271 Loss: 1.1009249687194824\n  Batch: 272 Loss: 1.1030083894729614\n  Batch: 273 Loss: 1.0855035781860352\n  Batch: 274 Loss: 1.1000442504882812\n  Batch: 275 Loss: 1.1028443574905396\n  Batch: 276 Loss: 1.1008191108703613\n  Batch: 277 Loss: 1.1006017923355103\n  Batch: 278 Loss: 1.10121750831604\n  Batch: 279 Loss: 1.1017590761184692\n  Batch: 280 Loss: 1.101560115814209\n  Batch: 281 Loss: 1.1019933223724365\n  Batch: 282 Loss: 1.0989704132080078\n  Batch: 283 Loss: 1.1011234521865845\n  Batch: 284 Loss: 1.1002851724624634\n  Batch: 285 Loss: 1.0999351739883423\n  Batch: 286 Loss: 1.0996285676956177\n  Batch: 287 Loss: 1.0995240211486816\n  Batch: 288 Loss: 1.0991657972335815\n  Batch: 289 Loss: 1.0991582870483398\n  Batch: 290 Loss: 1.0993729829788208\n  Batch: 291 Loss: 1.0993027687072754\n  Batch: 292 Loss: 1.0990281105041504\n  Batch: 293 Loss: 1.1006008386611938\n  Batch: 294 Loss: 1.0978004932403564\n  Batch: 295 Loss: 1.098720908164978\n  Batch: 296 Loss: 1.0988237857818604\n  Batch: 297 Loss: 1.09821355342865\n  Batch: 298 Loss: 1.0985913276672363\n  Batch: 299 Loss: 1.0989259481430054\n  Batch: 300 Loss: 1.0981602668762207\n  Batch: 301 Loss: 1.093410611152649\n  Batch: 302 Loss: 1.0990846157073975\n  Batch: 303 Loss: 1.0979113578796387\n  Batch: 304 Loss: 1.098785161972046\n  Batch: 305 Loss: 1.0982122421264648\n  Batch: 306 Loss: 1.0981920957565308\n  Batch: 307 Loss: 1.098803162574768\n  Batch: 308 Loss: 1.0974822044372559\n  Batch: 309 Loss: 1.0983566045761108\n  Batch: 310 Loss: 1.0973750352859497\n  Batch: 311 Loss: 1.0969446897506714\n  Batch: 312 Loss: 1.100087285041809\n  Batch: 313 Loss: 1.0966225862503052\n  Batch: 314 Loss: 1.0973161458969116\n  Batch: 315 Loss: 1.096909999847412\n  Batch: 316 Loss: 1.097194790840149\n  Batch: 317 Loss: 1.096413493156433\n  Batch: 318 Loss: 1.097204566001892\n  Batch: 319 Loss: 1.0964299440383911\n  Batch: 320 Loss: 1.095737338066101\n  Batch: 321 Loss: 1.0973777770996094\n  Batch: 322 Loss: 1.0961118936538696\n  Batch: 323 Loss: 1.0963683128356934\n  Batch: 324 Loss: 1.0965070724487305\n  Batch: 325 Loss: 1.0957589149475098\n  Batch: 326 Loss: 1.0964834690093994\n  Batch: 327 Loss: 1.0962159633636475\n  Batch: 328 Loss: 1.0945168733596802\n  Batch: 329 Loss: 1.0956568717956543\n  Batch: 330 Loss: 1.0959079265594482\n  Batch: 331 Loss: 1.0951505899429321\n  Batch: 332 Loss: 1.0954790115356445\n  Batch: 333 Loss: 1.095503568649292\n  Batch: 334 Loss: 1.0956687927246094\n  Batch: 335 Loss: 1.0951471328735352\n  Batch: 336 Loss: 1.094740867614746\nEpoch: 5 Loss: 1.111659360428651\n  Batch: 1 Loss: 1.092882513999939\n  Batch: 2 Loss: 1.094739556312561\n  Batch: 3 Loss: 1.0946557521820068\n  Batch: 4 Loss: 1.0939185619354248\n  Batch: 5 Loss: 1.0944947004318237\n  Batch: 6 Loss: 1.0942763090133667\n  Batch: 7 Loss: 1.0944254398345947\n  Batch: 8 Loss: 1.0822381973266602\n  Batch: 9 Loss: 1.0940167903900146\n  Batch: 10 Loss: 1.097693920135498\n  Batch: 11 Loss: 1.0938327312469482\n  Batch: 12 Loss: 1.0935165882110596\n  Batch: 13 Loss: 1.0949180126190186\n  Batch: 14 Loss: 1.0943793058395386\n  Batch: 15 Loss: 1.093851089477539\n  Batch: 16 Loss: 1.0936945676803589\n  Batch: 17 Loss: 1.0935205221176147\n  Batch: 18 Loss: 1.0932483673095703\n  Batch: 19 Loss: 1.0936286449432373\n  Batch: 20 Loss: 1.0938771963119507\n  Batch: 21 Loss: 1.0928559303283691\n  Batch: 22 Loss: 1.0927622318267822\n  Batch: 23 Loss: 1.0928436517715454\n  Batch: 24 Loss: 1.0925638675689697\n  Batch: 25 Loss: 1.085091471672058\n  Batch: 26 Loss: 1.0937097072601318\n  Batch: 27 Loss: 1.0925606489181519\n  Batch: 28 Loss: 1.0924098491668701\n  Batch: 29 Loss: 1.0924304723739624\n  Batch: 30 Loss: 1.0924147367477417\n  Batch: 31 Loss: 1.0925166606903076\n  Batch: 32 Loss: 1.0923711061477661\n  Batch: 33 Loss: 1.0938084125518799\n  Batch: 34 Loss: 1.0923280715942383\n  Batch: 35 Loss: 1.0957212448120117\n  Batch: 36 Loss: 1.0934913158416748\n  Batch: 37 Loss: 1.092299461364746\n  Batch: 38 Loss: 1.092559576034546\n  Batch: 39 Loss: 1.0914698839187622\n  Batch: 40 Loss: 1.0916633605957031\n  Batch: 41 Loss: 1.0876139402389526\n  Batch: 42 Loss: 1.0917489528656006\n  Batch: 43 Loss: 1.0914709568023682\n  Batch: 44 Loss: 1.091701626777649\n  Batch: 45 Loss: 1.0914156436920166\n  Batch: 46 Loss: 1.0923471450805664\n  Batch: 47 Loss: 1.0920286178588867\n  Batch: 48 Loss: 1.091524600982666\n  Batch: 49 Loss: 1.09138822555542\n  Batch: 50 Loss: 1.0951975584030151\n  Batch: 51 Loss: 1.090754747390747\n  Batch: 52 Loss: 1.0904706716537476\n  Batch: 53 Loss: 1.091001272201538\n  Batch: 54 Loss: 1.0909404754638672\n  Batch: 55 Loss: 1.0921790599822998\n  Batch: 56 Loss: 1.090928077697754\n  Batch: 57 Loss: 1.0939760208129883\n  Batch: 58 Loss: 1.0904752016067505\n  Batch: 59 Loss: 1.090951919555664\n  Batch: 60 Loss: 1.0904377698898315\n  Batch: 61 Loss: 1.09259033203125\n  Batch: 62 Loss: 1.0906072854995728\n  Batch: 63 Loss: 1.0902369022369385\n  Batch: 64 Loss: 1.0878515243530273\n  Batch: 65 Loss: 1.08925461769104\n  Batch: 66 Loss: 1.089273452758789\n  Batch: 67 Loss: 1.0892993211746216\n  Batch: 68 Loss: 1.088494896888733\n  Batch: 69 Loss: 1.0897800922393799\n  Batch: 70 Loss: 1.0890247821807861\n  Batch: 71 Loss: 1.0879977941513062\n  Batch: 72 Loss: 1.0889188051223755\n  Batch: 73 Loss: 1.089457392692566\n  Batch: 74 Loss: 1.0888051986694336\n  Batch: 75 Loss: 1.0889638662338257\n  Batch: 76 Loss: 1.0885131359100342\n  Batch: 77 Loss: 1.077795147895813\n  Batch: 78 Loss: 1.0843746662139893\n  Batch: 79 Loss: 1.08824622631073\n  Batch: 80 Loss: 1.0900604724884033\n  Batch: 81 Loss: 1.0912697315216064\n  Batch: 82 Loss: 1.0882587432861328\n  Batch: 83 Loss: 1.0884668827056885\n  Batch: 84 Loss: 1.0883022546768188\n  Batch: 85 Loss: 1.0884830951690674\n  Batch: 86 Loss: 1.087965965270996\n  Batch: 87 Loss: 1.0873019695281982\n  Batch: 88 Loss: 1.0881427526474\n  Batch: 89 Loss: 1.0876494646072388\n  Batch: 90 Loss: 1.0859951972961426\n  Batch: 91 Loss: 1.0809643268585205\n  Batch: 92 Loss: 1.0874665975570679\n  Batch: 93 Loss: 1.0878797769546509\n  Batch: 94 Loss: 1.0877554416656494\n  Batch: 95 Loss: 1.0887278318405151\n  Batch: 96 Loss: 1.0877373218536377\n  Batch: 97 Loss: 1.0867527723312378\n  Batch: 98 Loss: 1.087156891822815\n  Batch: 99 Loss: 1.0887552499771118\n  Batch: 100 Loss: 1.0873064994812012\n  Batch: 101 Loss: 1.0866068601608276\n  Batch: 102 Loss: 1.0851613283157349\n  Batch: 103 Loss: 1.0865118503570557\n  Batch: 104 Loss: 1.0864622592926025\n  Batch: 105 Loss: 1.0861835479736328\n  Batch: 106 Loss: 1.086795449256897\n  Batch: 107 Loss: 1.0876022577285767\n  Batch: 108 Loss: 1.1072263717651367\n  Batch: 109 Loss: 1.0864853858947754\n  Batch: 110 Loss: 1.0855424404144287\n  Batch: 111 Loss: 1.084434986114502\n  Batch: 112 Loss: 1.0886106491088867\n  Batch: 113 Loss: 1.0920486450195312\n  Batch: 114 Loss: 1.0882232189178467\n  Batch: 115 Loss: 1.0901377201080322\n  Batch: 116 Loss: 1.0899995565414429\n  Batch: 117 Loss: 1.0852487087249756\n  Batch: 118 Loss: 1.086169958114624\n  Batch: 119 Loss: 1.0854592323303223\n  Batch: 120 Loss: 1.0861706733703613\n  Batch: 121 Loss: 1.0868926048278809\n  Batch: 122 Loss: 1.0850099325180054\n  Batch: 123 Loss: 1.0853558778762817\n  Batch: 124 Loss: 1.0850276947021484\n  Batch: 125 Loss: 1.0851140022277832\n  Batch: 126 Loss: 1.0859477519989014\n  Batch: 127 Loss: 1.0838935375213623\n  Batch: 128 Loss: 1.0846219062805176\n  Batch: 129 Loss: 1.0848106145858765\n  Batch: 130 Loss: 1.0845236778259277\n  Batch: 131 Loss: 1.0845857858657837\n  Batch: 132 Loss: 1.0841851234436035\n  Batch: 133 Loss: 1.0848121643066406\n  Batch: 134 Loss: 1.0848960876464844\n  Batch: 135 Loss: 1.0846343040466309\n  Batch: 136 Loss: 1.0870649814605713\n  Batch: 137 Loss: 1.0839240550994873\n  Batch: 138 Loss: 1.085033893585205\n  Batch: 139 Loss: 1.084498643875122\n  Batch: 140 Loss: 1.0846526622772217\n  Batch: 141 Loss: 1.0848658084869385\n  Batch: 142 Loss: 1.083787202835083\n  Batch: 143 Loss: 1.084215521812439\n  Batch: 144 Loss: 1.0837428569793701\n  Batch: 145 Loss: 1.0756425857543945\n  Batch: 146 Loss: 1.0835437774658203\n  Batch: 147 Loss: 1.0833319425582886\n  Batch: 148 Loss: 1.0838149785995483\n  Batch: 149 Loss: 1.085525393486023\n  Batch: 150 Loss: 1.0836825370788574\n  Batch: 151 Loss: 1.0832443237304688\n  Batch: 152 Loss: 1.0832550525665283\n  Batch: 153 Loss: 1.0830891132354736\n  Batch: 154 Loss: 1.0831584930419922\n  Batch: 155 Loss: 1.0826278924942017\n  Batch: 156 Loss: 1.0766984224319458\n  Batch: 157 Loss: 1.0837846994400024\n  Batch: 158 Loss: 1.082411289215088\n  Batch: 159 Loss: 1.0830504894256592\n  Batch: 160 Loss: 1.082390308380127\n  Batch: 161 Loss: 1.0829601287841797\n  Batch: 162 Loss: 1.0822296142578125\n  Batch: 163 Loss: 1.082775354385376\n  Batch: 164 Loss: 1.0672720670700073\n  Batch: 165 Loss: 1.0822936296463013\n  Batch: 166 Loss: 1.0831266641616821\n  Batch: 167 Loss: 1.0853357315063477\n  Batch: 168 Loss: 1.0706733465194702\n  Batch: 169 Loss: 1.0842353105545044\n  Batch: 170 Loss: 1.0900568962097168\n  Batch: 171 Loss: 1.086021065711975\n  Batch: 172 Loss: 1.0698562860488892\n  Batch: 173 Loss: 1.0823266506195068\n  Batch: 174 Loss: 1.083853840827942\n  Batch: 175 Loss: 1.0833721160888672\n  Batch: 176 Loss: 1.0812681913375854\n  Batch: 177 Loss: 1.0822021961212158\n  Batch: 178 Loss: 1.0826184749603271\n  Batch: 179 Loss: 1.0819025039672852\n  Batch: 180 Loss: 1.0811984539031982\n  Batch: 181 Loss: 1.081412434577942\n  Batch: 182 Loss: 1.082669734954834\n  Batch: 183 Loss: 1.0836093425750732\n  Batch: 184 Loss: 1.080927848815918\n  Batch: 185 Loss: 1.0827350616455078\n  Batch: 186 Loss: 1.0825140476226807\n  Batch: 187 Loss: 1.0717620849609375\n  Batch: 188 Loss: 1.081941843032837\n  Batch: 189 Loss: 1.082035779953003\n  Batch: 190 Loss: 1.0821565389633179\n  Batch: 191 Loss: 1.0730390548706055\n  Batch: 192 Loss: 1.0841069221496582\n  Batch: 193 Loss: 1.0807240009307861\n  Batch: 194 Loss: 1.0148602724075317\n  Batch: 195 Loss: 1.0884778499603271\n  Batch: 196 Loss: 1.083122968673706\n  Batch: 197 Loss: 1.0742771625518799\n  Batch: 198 Loss: 1.0804635286331177\n  Batch: 199 Loss: 1.0815041065216064\n  Batch: 200 Loss: 1.0802229642868042\n  Batch: 201 Loss: 1.0842509269714355\n  Batch: 202 Loss: 1.0504685640335083\n  Batch: 203 Loss: 1.0805978775024414\n  Batch: 204 Loss: 1.0802397727966309\n  Batch: 205 Loss: 1.0689665079116821\n  Batch: 206 Loss: 1.0810127258300781\n  Batch: 207 Loss: 1.080282211303711\n  Batch: 208 Loss: 1.0802958011627197\n  Batch: 209 Loss: 1.0806734561920166\n  Batch: 210 Loss: 1.0792731046676636\n  Batch: 211 Loss: 1.0803605318069458\n  Batch: 212 Loss: 1.0800989866256714\n  Batch: 213 Loss: 1.0811457633972168\n  Batch: 214 Loss: 1.0787997245788574\n  Batch: 215 Loss: 1.0792615413665771\n  Batch: 216 Loss: 1.0791702270507812\n  Batch: 217 Loss: 1.0788462162017822\n  Batch: 218 Loss: 1.078305959701538\n  Batch: 219 Loss: 1.0423089265823364\n  Batch: 220 Loss: 1.0788185596466064\n  Batch: 221 Loss: 1.0795269012451172\n  Batch: 222 Loss: 1.078839898109436\n  Batch: 223 Loss: 1.0789448022842407\n  Batch: 224 Loss: 1.0786068439483643\n  Batch: 225 Loss: 1.0778172016143799\n  Batch: 226 Loss: 1.088350772857666\n  Batch: 227 Loss: 1.0785523653030396\n  Batch: 228 Loss: 1.0780854225158691\n  Batch: 229 Loss: 1.0789432525634766\n  Batch: 230 Loss: 1.0770611763000488\n  Batch: 231 Loss: 1.074432134628296\n  Batch: 232 Loss: 1.077599287033081\n  Batch: 233 Loss: 1.0788214206695557\n  Batch: 234 Loss: 1.0793366432189941\n  Batch: 235 Loss: 1.0799424648284912\n  Batch: 236 Loss: 1.0783679485321045\n  Batch: 237 Loss: 1.0769177675247192\n  Batch: 238 Loss: 1.0773332118988037\n  Batch: 239 Loss: 1.076903223991394\n  Batch: 240 Loss: 1.078606128692627\n  Batch: 241 Loss: 1.0773653984069824\n  Batch: 242 Loss: 1.0774285793304443\n  Batch: 243 Loss: 1.0775370597839355\n  Batch: 244 Loss: 1.0774354934692383\n  Batch: 245 Loss: 1.0763026475906372\n  Batch: 246 Loss: 1.0765182971954346\n  Batch: 247 Loss: 1.076622486114502\n  Batch: 248 Loss: 1.0733084678649902\n  Batch: 249 Loss: 1.0765063762664795\n  Batch: 250 Loss: 1.0764119625091553\n  Batch: 251 Loss: 1.0758224725723267\n  Batch: 252 Loss: 1.0771976709365845\n  Batch: 253 Loss: 1.0760830640792847\n  Batch: 254 Loss: 1.0758827924728394\n  Batch: 255 Loss: 1.0758001804351807\n  Batch: 256 Loss: 1.0757043361663818\n  Batch: 257 Loss: 1.0804659128189087\n  Batch: 258 Loss: 1.0754468441009521\n  Batch: 259 Loss: 1.069358468055725\n  Batch: 260 Loss: 1.06659734249115\n  Batch: 261 Loss: 1.0753741264343262\n  Batch: 262 Loss: 1.0752099752426147\n  Batch: 263 Loss: 1.0754536390304565\n  Batch: 264 Loss: 1.0727843046188354\n  Batch: 265 Loss: 1.0761122703552246\n  Batch: 266 Loss: 1.0776723623275757\n  Batch: 267 Loss: 1.0753600597381592\n  Batch: 268 Loss: 1.076249122619629\n  Batch: 269 Loss: 1.0756734609603882\n  Batch: 270 Loss: 1.0751994848251343\n  Batch: 271 Loss: 1.0746846199035645\n  Batch: 272 Loss: 1.070817232131958\n  Batch: 273 Loss: 1.0590225458145142\n  Batch: 274 Loss: 1.0745882987976074\n  Batch: 275 Loss: 1.075011968612671\n  Batch: 276 Loss: 1.0749038457870483\n  Batch: 277 Loss: 1.0743991136550903\n  Batch: 278 Loss: 1.0744401216506958\n  Batch: 279 Loss: 1.0731933116912842\n  Batch: 280 Loss: 1.076314091682434\n  Batch: 281 Loss: 1.0751252174377441\n  Batch: 282 Loss: 1.0806944370269775\n  Batch: 283 Loss: 1.0772035121917725\n  Batch: 284 Loss: 1.0743417739868164\n  Batch: 285 Loss: 1.07217538356781\n  Batch: 286 Loss: 1.0740917921066284\n  Batch: 287 Loss: 1.0733245611190796\n  Batch: 288 Loss: 1.076157808303833\n  Batch: 289 Loss: 1.07315993309021\n  Batch: 290 Loss: 1.0736992359161377\n  Batch: 291 Loss: 1.0806876420974731\n  Batch: 292 Loss: 1.0730335712432861\n  Batch: 293 Loss: 1.0784912109375\n  Batch: 294 Loss: 1.0738372802734375\n  Batch: 295 Loss: 1.072816014289856\n  Batch: 296 Loss: 1.0740209817886353\n  Batch: 297 Loss: 1.0689340829849243\n  Batch: 298 Loss: 1.0726902484893799\n  Batch: 299 Loss: 1.0732667446136475\n  Batch: 300 Loss: 1.0714441537857056\n  Batch: 301 Loss: 1.0726797580718994\n  Batch: 302 Loss: 1.07240629196167\n  Batch: 303 Loss: 1.0729809999465942\n  Batch: 304 Loss: 1.0728791952133179\n  Batch: 305 Loss: 1.072341799736023\n  Batch: 306 Loss: 1.0712882280349731\n  Batch: 307 Loss: 1.0730599164962769\n  Batch: 308 Loss: 1.0716837644577026\n  Batch: 309 Loss: 1.072278618812561\n  Batch: 310 Loss: 1.0718950033187866\n  Batch: 311 Loss: 1.072780966758728\n  Batch: 312 Loss: 1.0718390941619873\n  Batch: 313 Loss: 1.0721884965896606\n  Batch: 314 Loss: 1.071645975112915\n  Batch: 315 Loss: 1.072076439857483\n  Batch: 316 Loss: 1.0721604824066162\n  Batch: 317 Loss: 1.072825312614441\n  Batch: 318 Loss: 1.071330189704895\n  Batch: 319 Loss: 1.0721287727355957\n  Batch: 320 Loss: 1.071437954902649\n  Batch: 321 Loss: 1.0711337327957153\n  Batch: 322 Loss: 1.0693936347961426\n  Batch: 323 Loss: 1.0739866495132446\n  Batch: 324 Loss: 1.071707844734192\n  Batch: 325 Loss: 1.0718648433685303\n  Batch: 326 Loss: 1.0709798336029053\n  Batch: 327 Loss: 1.0707391500473022\n  Batch: 328 Loss: 1.0727131366729736\n  Batch: 329 Loss: 1.0707244873046875\n  Batch: 330 Loss: 1.0698668956756592\n  Batch: 331 Loss: 1.0708614587783813\n  Batch: 332 Loss: 1.0710943937301636\n  Batch: 333 Loss: 1.0707392692565918\n  Batch: 334 Loss: 1.070556879043579\n  Batch: 335 Loss: 1.07038152217865\n  Batch: 336 Loss: 1.071094274520874\nEpoch: 6 Loss: 1.0819287371067774\n  Batch: 1 Loss: 1.0701690912246704\n  Batch: 2 Loss: 1.0703402757644653\n  Batch: 3 Loss: 1.0702173709869385\n  Batch: 4 Loss: 1.070139765739441\n  Batch: 5 Loss: 1.067494511604309\n  Batch: 6 Loss: 1.0698230266571045\n  Batch: 7 Loss: 1.0701167583465576\n  Batch: 8 Loss: 1.0663156509399414\n  Batch: 9 Loss: 1.0697437524795532\n  Batch: 10 Loss: 1.067298412322998\n  Batch: 11 Loss: 1.0702612400054932\n  Batch: 12 Loss: 1.0697264671325684\n  Batch: 13 Loss: 1.054962158203125\n  Batch: 14 Loss: 1.0726754665374756\n  Batch: 15 Loss: 1.0718536376953125\n  Batch: 16 Loss: 1.0727862119674683\n  Batch: 17 Loss: 1.0696665048599243\n  Batch: 18 Loss: 1.0743249654769897\n  Batch: 19 Loss: 1.0697301626205444\n  Batch: 20 Loss: 1.068471074104309\n  Batch: 21 Loss: 1.0692875385284424\n  Batch: 22 Loss: 1.0562511682510376\n  Batch: 23 Loss: 1.0694524049758911\n  Batch: 24 Loss: 1.0701799392700195\n  Batch: 25 Loss: 1.0695492029190063\n  Batch: 26 Loss: 1.0692695379257202\n  Batch: 27 Loss: 1.0693960189819336\n  Batch: 28 Loss: 1.0687222480773926\n  Batch: 29 Loss: 1.0730316638946533\n  Batch: 30 Loss: 1.0688320398330688\n  Batch: 31 Loss: 1.0684614181518555\n  Batch: 32 Loss: 1.0683503150939941\n  Batch: 33 Loss: 1.0691642761230469\n  Batch: 34 Loss: 1.0679763555526733\n  Batch: 35 Loss: 1.0682932138442993\n  Batch: 36 Loss: 1.0695173740386963\n  Batch: 37 Loss: 1.0680935382843018\n  Batch: 38 Loss: 1.0693305730819702\n  Batch: 39 Loss: 1.0618839263916016\n  Batch: 40 Loss: 1.0686392784118652\n  Batch: 41 Loss: 1.0687649250030518\n  Batch: 42 Loss: 1.066800594329834\n  Batch: 43 Loss: 1.0684527158737183\n  Batch: 44 Loss: 1.0677496194839478\n  Batch: 45 Loss: 1.068772315979004\n  Batch: 46 Loss: 1.06819486618042\n  Batch: 47 Loss: 1.0677547454833984\n  Batch: 48 Loss: 1.0679876804351807\n  Batch: 49 Loss: 1.0652459859848022\n  Batch: 50 Loss: 1.0679569244384766\n  Batch: 51 Loss: 1.0677038431167603\n  Batch: 52 Loss: 1.0685756206512451\n  Batch: 53 Loss: 1.067339301109314\n  Batch: 54 Loss: 1.067949891090393\n  Batch: 55 Loss: 1.0651215314865112\n  Batch: 56 Loss: 1.068366527557373\n  Batch: 57 Loss: 1.0672743320465088\n  Batch: 58 Loss: 1.0673446655273438\n  Batch: 59 Loss: 1.0675373077392578\n  Batch: 60 Loss: 1.066965937614441\n  Batch: 61 Loss: 1.0649545192718506\n  Batch: 62 Loss: 1.0627567768096924\n  Batch: 63 Loss: 1.0665793418884277\n  Batch: 64 Loss: 1.066441297531128\n  Batch: 65 Loss: 1.0633041858673096\n  Batch: 66 Loss: 1.066828966140747\n  Batch: 67 Loss: 1.066632628440857\n  Batch: 68 Loss: 1.0673365592956543\n  Batch: 69 Loss: 1.066179633140564\n  Batch: 70 Loss: 1.074456810951233\n  Batch: 71 Loss: 1.0666897296905518\n  Batch: 72 Loss: 1.066076636314392\n  Batch: 73 Loss: 1.0659582614898682\n  Batch: 74 Loss: 1.0658438205718994\n  Batch: 75 Loss: 1.0662665367126465\n  Batch: 76 Loss: 1.0638033151626587\n  Batch: 77 Loss: 1.0661801099777222\n  Batch: 78 Loss: 1.0640755891799927\n  Batch: 79 Loss: 1.0665167570114136\n  Batch: 80 Loss: 1.0653736591339111\n  Batch: 81 Loss: 1.0492944717407227\n  Batch: 82 Loss: 1.0659743547439575\n  Batch: 83 Loss: 1.0664433240890503\n  Batch: 84 Loss: 1.0797111988067627\n  Batch: 85 Loss: 1.0676264762878418\n  Batch: 86 Loss: 1.0671747922897339\n  Batch: 87 Loss: 1.0616744756698608\n  Batch: 88 Loss: 1.0672792196273804\n  Batch: 89 Loss: 1.0650229454040527\n  Batch: 90 Loss: 1.073294997215271\n  Batch: 91 Loss: 1.0655345916748047\n  Batch: 92 Loss: 1.067143201828003\n  Batch: 93 Loss: 1.0578454732894897\n  Batch: 94 Loss: 1.0647599697113037\n  Batch: 95 Loss: 1.0647392272949219\n  Batch: 96 Loss: 1.0651142597198486\n  Batch: 97 Loss: 1.065112829208374\n  Batch: 98 Loss: 1.0789145231246948\n  Batch: 99 Loss: 1.0662891864776611\n  Batch: 100 Loss: 1.064432144165039\n  Batch: 101 Loss: 1.0650590658187866\n  Batch: 102 Loss: 1.016213059425354\n  Batch: 103 Loss: 1.0652625560760498\n  Batch: 104 Loss: 1.0680662393569946\n  Batch: 105 Loss: 1.0643566846847534\n  Batch: 106 Loss: 1.0664551258087158\n  Batch: 107 Loss: 1.066442608833313\n  Batch: 108 Loss: 1.0665513277053833\n  Batch: 109 Loss: 1.0648388862609863\n  Batch: 110 Loss: 1.0670539140701294\n  Batch: 111 Loss: 1.0730798244476318\n  Batch: 112 Loss: 1.0639506578445435\n  Batch: 113 Loss: 1.0635418891906738\n  Batch: 114 Loss: 1.0739142894744873\n  Batch: 115 Loss: 1.039313554763794\n  Batch: 116 Loss: 1.0644176006317139\n  Batch: 117 Loss: 1.0645743608474731\n  Batch: 118 Loss: 1.064864993095398\n  Batch: 119 Loss: 1.0652167797088623\n  Batch: 120 Loss: 1.0498650074005127\n  Batch: 121 Loss: 1.0641978979110718\n  Batch: 122 Loss: 1.0645447969436646\n  Batch: 123 Loss: 1.0707039833068848\n  Batch: 124 Loss: 1.0647878646850586\n  Batch: 125 Loss: 1.0636930465698242\n  Batch: 126 Loss: 1.0636684894561768\n  Batch: 127 Loss: 1.063437819480896\n  Batch: 129 Loss: 1.0634558200836182\n  Batch: 130 Loss: 1.0631535053253174\n  Batch: 131 Loss: 1.063377022743225\n  Batch: 132 Loss: 1.0633143186569214\n  Batch: 133 Loss: 1.0627379417419434\n  Batch: 134 Loss: 1.0627825260162354\n  Batch: 135 Loss: 1.0633609294891357\n  Batch: 136 Loss: 1.0626499652862549\n  Batch: 137 Loss: 1.0634372234344482\n  Batch: 138 Loss: 1.0650416612625122\n  Batch: 139 Loss: 1.0592042207717896\n  Batch: 140 Loss: 1.0623338222503662\n  Batch: 141 Loss: 1.0630574226379395\n  Batch: 142 Loss: 1.0622831583023071\n  Batch: 143 Loss: 1.0633740425109863\n  Batch: 144 Loss: 1.0598325729370117\n  Batch: 145 Loss: 1.0617684125900269\n  Batch: 146 Loss: 1.0627765655517578\n  Batch: 147 Loss: 1.0622344017028809\n  Batch: 148 Loss: 1.062537431716919\n  Batch: 149 Loss: 1.0620238780975342\n  Batch: 150 Loss: 1.0622583627700806\n  Batch: 151 Loss: 1.0580252408981323\n  Batch: 152 Loss: 1.062295913696289\n  Batch: 153 Loss: 1.0619966983795166\n  Batch: 154 Loss: 1.0617785453796387\n  Batch: 155 Loss: 1.0618711709976196\n  Batch: 156 Loss: 1.063417911529541\n  Batch: 157 Loss: 1.0615395307540894\n  Batch: 158 Loss: 1.0612857341766357\n  Batch: 159 Loss: 1.0394147634506226\n  Batch: 160 Loss: 1.062718152999878\n  Batch: 161 Loss: 1.061269998550415\n  Batch: 162 Loss: 1.0607808828353882\n  Batch: 163 Loss: 1.061232089996338\n  Batch: 164 Loss: 1.0245310068130493\n  Batch: 165 Loss: 1.0611505508422852\n  Batch: 166 Loss: 1.060791015625\n  Batch: 167 Loss: 1.0620425939559937\n  Batch: 168 Loss: 1.0620827674865723\n  Batch: 169 Loss: 1.0617371797561646\n  Batch: 170 Loss: 1.0381834506988525\n  Batch: 171 Loss: 1.049681544303894\n  Batch: 172 Loss: 1.0651171207427979\n  Batch: 173 Loss: 1.0690414905548096\n  Batch: 174 Loss: 1.0548291206359863\n  Batch: 175 Loss: 1.0614036321640015\n  Batch: 176 Loss: 1.0605905055999756\n  Batch: 177 Loss: 1.0607013702392578\n  Batch: 178 Loss: 1.0610448122024536\n  Batch: 179 Loss: 1.0610074996948242\n  Batch: 180 Loss: 1.0648127794265747\n  Batch: 181 Loss: 1.0609550476074219\n  Batch: 182 Loss: 1.0621956586837769\n  Batch: 183 Loss: 1.0626994371414185\n  Batch: 184 Loss: 1.0609040260314941\n  Batch: 185 Loss: 1.0640766620635986\n  Batch: 186 Loss: 1.0671429634094238\n  Batch: 187 Loss: 1.0630083084106445\n  Batch: 188 Loss: 1.0613564252853394\n  Batch: 189 Loss: 1.0630265474319458\n  Batch: 190 Loss: 1.0612543821334839\n  Batch: 191 Loss: 1.0507433414459229\n  Batch: 192 Loss: 1.060194730758667\n  Batch: 193 Loss: 1.0601916313171387\n  Batch: 194 Loss: 1.056506872177124\n  Batch: 195 Loss: 1.0470017194747925\n  Batch: 196 Loss: 0.9920200109481812\n  Batch: 197 Loss: 1.0567662715911865\n  Batch: 198 Loss: 1.0502065420150757\n  Batch: 199 Loss: 1.0608092546463013\n  Batch: 200 Loss: 1.0601087808609009\n  Batch: 201 Loss: 1.0466889142990112\n  Batch: 202 Loss: 1.0643140077590942\n  Batch: 203 Loss: 1.066627860069275\n  Batch: 204 Loss: 1.0665607452392578\n  Batch: 205 Loss: 1.0645068883895874\n  Batch: 206 Loss: 1.0424909591674805\n  Batch: 207 Loss: 1.0464375019073486\n  Batch: 208 Loss: 1.0622851848602295\n  Batch: 209 Loss: 1.0548344850540161\n  Batch: 210 Loss: 1.0596474409103394\n  Batch: 211 Loss: 1.0557531118392944\n  Batch: 212 Loss: 1.0594509840011597\n  Batch: 213 Loss: 1.0601541996002197\n  Batch: 214 Loss: 1.058889389038086\n  Batch: 215 Loss: 1.0573513507843018\n  Batch: 216 Loss: 1.058552622795105\n  Batch: 217 Loss: 1.0543603897094727\n  Batch: 218 Loss: 1.0589759349822998\n  Batch: 219 Loss: 1.0583709478378296\n  Batch: 220 Loss: 1.0570687055587769\n  Batch: 221 Loss: 1.0580356121063232\n  Batch: 222 Loss: 1.0598357915878296\n  Batch: 223 Loss: 1.0565277338027954\n  Batch: 224 Loss: 1.058147668838501\n  Batch: 225 Loss: 1.0581146478652954\n  Batch: 226 Loss: 1.0582391023635864\n  Batch: 227 Loss: 1.05793297290802\n  Batch: 228 Loss: 1.0581951141357422\n  Batch: 229 Loss: 1.0589690208435059\n  Batch: 230 Loss: 1.0582650899887085\n  Batch: 231 Loss: 1.058117151260376\n  Batch: 232 Loss: 1.057661533355713\n  Batch: 233 Loss: 1.0578850507736206\n  Batch: 234 Loss: 1.0579922199249268\n  Batch: 235 Loss: 1.0578393936157227\n  Batch: 236 Loss: 1.0582274198532104\n  Batch: 237 Loss: 1.0578639507293701\n  Batch: 238 Loss: 1.0608911514282227\n  Batch: 239 Loss: 1.0580337047576904\n  Batch: 240 Loss: 1.0599380731582642\n  Batch: 241 Loss: 1.0572115182876587\n  Batch: 242 Loss: 1.057518720626831\n  Batch: 243 Loss: 1.057599663734436\n  Batch: 244 Loss: 1.0575796365737915\n  Batch: 245 Loss: 1.0573186874389648\n  Batch: 246 Loss: 1.0552406311035156\n  Batch: 247 Loss: 1.051069736480713\n  Batch: 248 Loss: 1.063254952430725\n  Batch: 249 Loss: 1.058017373085022\n  Batch: 250 Loss: 1.0591295957565308\n  Batch: 251 Loss: 1.058316946029663\n  Batch: 252 Loss: 1.0566715002059937\n  Batch: 253 Loss: 1.0566108226776123\n  Batch: 254 Loss: 1.0500694513320923\n  Batch: 255 Loss: 1.0570083856582642\n  Batch: 256 Loss: 1.0582118034362793\n  Batch: 257 Loss: 1.056392788887024\n  Batch: 258 Loss: 1.056355595588684\n  Batch: 259 Loss: 1.0554877519607544\n  Batch: 260 Loss: 1.0531715154647827\n  Batch: 261 Loss: 1.056115984916687\n  Batch: 262 Loss: 1.0562399625778198\n  Batch: 263 Loss: 1.0565170049667358\n  Batch: 264 Loss: 1.0562087297439575\n  Batch: 265 Loss: 1.0560790300369263\n  Batch: 266 Loss: 1.0579079389572144\n  Batch: 267 Loss: 1.0559217929840088\n  Batch: 268 Loss: 1.0420265197753906\n  Batch: 269 Loss: 1.0456424951553345\n  Batch: 270 Loss: 1.0567729473114014\n  Batch: 271 Loss: 1.0558617115020752\n  Batch: 272 Loss: 1.0558617115020752\n  Batch: 273 Loss: 1.0564106702804565\n  Batch: 274 Loss: 1.0562986135482788\n  Batch: 275 Loss: 1.055639624595642\n  Batch: 276 Loss: 1.046058177947998\n  Batch: 277 Loss: 1.0564403533935547\n  Batch: 278 Loss: 1.0560133457183838\n  Batch: 279 Loss: 1.037278413772583\n  Batch: 280 Loss: 1.0540372133255005\n  Batch: 281 Loss: 1.0552756786346436\n  Batch: 282 Loss: 1.0185476541519165\n  Batch: 283 Loss: 1.058263897895813\n  Batch: 284 Loss: 1.056928277015686\n  Batch: 285 Loss: 1.055311679840088\n  Batch: 286 Loss: 1.0558651685714722\n  Batch: 287 Loss: 1.0544368028640747\n  Batch: 288 Loss: 1.0564621686935425\n  Batch: 289 Loss: 1.0551998615264893\n  Batch: 290 Loss: 1.0549051761627197\n  Batch: 291 Loss: 1.0565357208251953\n  Batch: 292 Loss: 1.0580925941467285\n  Batch: 293 Loss: 1.047999382019043\n  Batch: 294 Loss: 1.0548003911972046\n  Batch: 295 Loss: 1.0546271800994873\n  Batch: 296 Loss: 1.0543975830078125\n  Batch: 297 Loss: 1.0544713735580444\n  Batch: 298 Loss: 1.0571168661117554\n  Batch: 299 Loss: 1.0545235872268677\n  Batch: 300 Loss: 1.0544573068618774\n  Batch: 301 Loss: 1.0542716979980469\n  Batch: 302 Loss: 1.054687261581421\n  Batch: 303 Loss: 1.055109977722168\n  Batch: 304 Loss: 1.0548609495162964\n  Batch: 305 Loss: 1.0560935735702515\n  Batch: 306 Loss: 1.0555042028427124\n  Batch: 307 Loss: 1.0541622638702393\n  Batch: 308 Loss: 1.0548738241195679\n  Batch: 309 Loss: 1.0550543069839478\n  Batch: 310 Loss: 1.0905134677886963\n  Batch: 311 Loss: 1.0541590452194214\n  Batch: 312 Loss: 1.0538771152496338\n  Batch: 313 Loss: 1.0541646480560303\n  Batch: 314 Loss: 1.0551116466522217\n  Batch: 315 Loss: 1.0543850660324097\n  Batch: 316 Loss: 1.0548672676086426\n  Batch: 317 Loss: 1.0583155155181885\n  Batch: 318 Loss: 1.0308654308319092\n  Batch: 319 Loss: 1.056213140487671\n  Batch: 320 Loss: 1.0543889999389648\n  Batch: 321 Loss: 1.0518462657928467\n  Batch: 322 Loss: 1.058641791343689\n  Batch: 323 Loss: 1.0556288957595825\n  Batch: 324 Loss: 1.0518790483474731\n  Batch: 325 Loss: 1.0665968656539917\n  Batch: 326 Loss: 1.0547734498977661\n  Batch: 327 Loss: 1.0540995597839355\n  Batch: 328 Loss: 1.0560417175292969\n  Batch: 329 Loss: 1.0535696744918823\n  Batch: 330 Loss: 1.0545459985733032\n  Batch: 331 Loss: 1.0565381050109863\n  Batch: 332 Loss: 1.053348183631897\n  Batch: 333 Loss: 1.053183913230896\n  Batch: 334 Loss: 1.0206092596054077\n  Batch: 335 Loss: 1.0530297756195068\n  Batch: 336 Loss: 1.0508381128311157\nEpoch: 7 Loss: 1.0604149826935358\n  Batch: 1 Loss: 1.045729160308838\n  Batch: 2 Loss: 1.0534147024154663\n  Batch: 3 Loss: 1.0542134046554565\n  Batch: 4 Loss: 1.0537896156311035\n  Batch: 5 Loss: 1.0536975860595703\n  Batch: 6 Loss: 1.052993893623352\n  Batch: 7 Loss: 1.0539820194244385\n  Batch: 8 Loss: 1.0561232566833496\n  Batch: 9 Loss: 1.055408000946045\n  Batch: 10 Loss: 1.0546259880065918\n  Batch: 11 Loss: 1.0530061721801758\n  Batch: 12 Loss: 1.0544261932373047\n  Batch: 13 Loss: 1.0529385805130005\n  Batch: 14 Loss: 1.052376627922058\n  Batch: 15 Loss: 1.0412970781326294\n  Batch: 16 Loss: 1.0467228889465332\n  Batch: 17 Loss: 1.0526955127716064\n  Batch: 18 Loss: 1.051674723625183\n  Batch: 19 Loss: 1.0524684190750122\n  Batch: 20 Loss: 1.0386770963668823\n  Batch: 21 Loss: 1.051842451095581\n  Batch: 22 Loss: 1.052423119544983\n  Batch: 23 Loss: 1.031907320022583\n  Batch: 24 Loss: 1.051148533821106\n  Batch: 25 Loss: 1.0527945756912231\n  Batch: 26 Loss: 1.0447866916656494\n  Batch: 27 Loss: 1.0548516511917114\n  Batch: 28 Loss: 1.0523630380630493\n  Batch: 29 Loss: 1.053942084312439\n  Batch: 30 Loss: 1.0583723783493042\n  Batch: 31 Loss: 1.0523842573165894\n  Batch: 32 Loss: 1.0519325733184814\n  Batch: 33 Loss: 1.0520503520965576\n  Batch: 34 Loss: 1.0565335750579834\n  Batch: 35 Loss: 1.052291750907898\n  Batch: 36 Loss: 1.0522382259368896\n  Batch: 37 Loss: 1.0514068603515625\n  Batch: 38 Loss: 1.0520750284194946\n  Batch: 39 Loss: 1.0606921911239624\n  Batch: 40 Loss: 1.046775460243225\n  Batch: 41 Loss: 1.055695652961731\n  Batch: 42 Loss: 1.0514976978302002\n  Batch: 43 Loss: 1.0516009330749512\n  Batch: 44 Loss: 1.05295729637146\n  Batch: 45 Loss: 1.0524663925170898\n  Batch: 46 Loss: 1.0537840127944946\n  Batch: 47 Loss: 1.0254276990890503\n  Batch: 48 Loss: 1.0514205694198608\n  Batch: 49 Loss: 1.0520926713943481\n  Batch: 50 Loss: 1.0559678077697754\n  Batch: 51 Loss: 1.0511378049850464\n  Batch: 52 Loss: 1.0515416860580444\n  Batch: 53 Loss: 1.0509674549102783\n  Batch: 54 Loss: 1.0512109994888306\n  Batch: 55 Loss: 1.0509206056594849\n  Batch: 56 Loss: 1.05055570602417\n  Batch: 57 Loss: 1.0514605045318604\n  Batch: 58 Loss: 1.0507831573486328\n  Batch: 59 Loss: 1.0514389276504517\n  Batch: 60 Loss: 1.0515451431274414\n  Batch: 61 Loss: 1.0503756999969482\n  Batch: 62 Loss: 1.0474839210510254\n  Batch: 63 Loss: 1.0502499341964722\n  Batch: 64 Loss: 1.0504122972488403\n  Batch: 65 Loss: 1.050645351409912\n  Batch: 66 Loss: 1.0454059839248657\n  Batch: 67 Loss: 1.0498589277267456\n  Batch: 68 Loss: 1.0472999811172485\n  Batch: 69 Loss: 1.0501819849014282\n  Batch: 70 Loss: 1.0388522148132324\n  Batch: 71 Loss: 1.0497710704803467\n  Batch: 72 Loss: 1.052036166191101\n  Batch: 73 Loss: 1.0522040128707886\n  Batch: 74 Loss: 1.034035086631775\n  Batch: 75 Loss: 1.0504556894302368\n  Batch: 76 Loss: 1.0497102737426758\n  Batch: 77 Loss: 1.0528558492660522\n  Batch: 78 Loss: 1.041303038597107\n  Batch: 79 Loss: 1.0503685474395752\n  Batch: 80 Loss: 1.0494239330291748\n  Batch: 81 Loss: 1.0510495901107788\n  Batch: 82 Loss: 1.0533857345581055\n  Batch: 83 Loss: 1.0509593486785889\n  Batch: 84 Loss: 1.050920844078064\n  Batch: 85 Loss: 1.0071167945861816\n  Batch: 86 Loss: 1.0441986322402954\n  Batch: 87 Loss: 1.0494319200515747\n  Batch: 88 Loss: 1.0475064516067505\n  Batch: 89 Loss: 1.0490341186523438\n  Batch: 90 Loss: 1.0500832796096802\n  Batch: 91 Loss: 1.0492949485778809\n  Batch: 92 Loss: 1.0432462692260742\n  Batch: 93 Loss: 1.0509889125823975\n  Batch: 94 Loss: 1.0451476573944092\n  Batch: 95 Loss: 1.0511679649353027\n  Batch: 96 Loss: 1.0489379167556763\n  Batch: 97 Loss: 1.0481383800506592\n  Batch: 98 Loss: 1.0487769842147827\n  Batch: 99 Loss: 1.0353513956069946\n  Batch: 100 Loss: 1.0494745969772339\n  Batch: 101 Loss: 1.0504474639892578\n  Batch: 102 Loss: 1.0492076873779297\n  Batch: 103 Loss: 1.048587441444397\n  Batch: 104 Loss: 1.0587592124938965\n  Batch: 105 Loss: 1.0455046892166138\n  Batch: 106 Loss: 1.050586223602295\n  Batch: 107 Loss: 1.0492182970046997\n  Batch: 108 Loss: 1.0429786443710327\n  Batch: 109 Loss: 1.0467973947525024\n  Batch: 110 Loss: 1.0482041835784912\n  Batch: 111 Loss: 1.0495600700378418\n  Batch: 112 Loss: 1.0508836507797241\n  Batch: 113 Loss: 1.048699975013733\n  Batch: 114 Loss: 1.049952507019043\n  Batch: 115 Loss: 1.0488511323928833\n  Batch: 116 Loss: 1.0528967380523682\n  Batch: 117 Loss: 1.0504263639450073\n  Batch: 118 Loss: 1.0601305961608887\n  Batch: 119 Loss: 1.049202561378479\n  Batch: 120 Loss: 1.0498846769332886\n  Batch: 121 Loss: 1.0492706298828125\n  Batch: 122 Loss: 1.048973560333252\n  Batch: 123 Loss: 1.0481189489364624\n  Batch: 124 Loss: 1.048763632774353\n  Batch: 125 Loss: 1.0477626323699951\n  Batch: 126 Loss: 0.9533178210258484\n  Batch: 127 Loss: 1.0488038063049316\n  Batch: 128 Loss: 1.0489665269851685\n  Batch: 129 Loss: 1.049883484840393\n  Batch: 130 Loss: 1.0496762990951538\n  Batch: 131 Loss: 1.0405399799346924\n  Batch: 132 Loss: 1.0481617450714111\n  Batch: 133 Loss: 1.0550960302352905\n  Batch: 134 Loss: 1.0486286878585815\n  Batch: 135 Loss: 1.0491151809692383\n  Batch: 136 Loss: 1.0499907732009888\n  Batch: 137 Loss: 1.0487005710601807\n  Batch: 138 Loss: 1.0492483377456665\n  Batch: 139 Loss: 1.025626540184021\n  Batch: 140 Loss: 1.0461699962615967\n  Batch: 141 Loss: 1.0469998121261597\n  Batch: 142 Loss: 1.0473148822784424\n  Batch: 143 Loss: 1.0476303100585938\n  Batch: 144 Loss: 1.0515470504760742\n  Batch: 145 Loss: 1.0450800657272339\n  Batch: 146 Loss: 1.045046329498291\n  Batch: 147 Loss: 1.0479974746704102\n  Batch: 148 Loss: 1.0473440885543823\n  Batch: 149 Loss: 1.049964427947998\n  Batch: 150 Loss: 1.0466573238372803\n  Batch: 151 Loss: 1.047377347946167\n  Batch: 152 Loss: 1.0468271970748901\n  Batch: 153 Loss: 1.0472642183303833\n  Batch: 154 Loss: 1.046460509300232\n  Batch: 155 Loss: 1.0462217330932617\n  Batch: 156 Loss: 1.0466915369033813\n  Batch: 157 Loss: 1.046228051185608\n  Batch: 158 Loss: 1.0471521615982056\n  Batch: 159 Loss: 1.0447293519973755\n  Batch: 160 Loss: 1.0456819534301758\n  Batch: 161 Loss: 1.0463495254516602\n  Batch: 162 Loss: 1.046316146850586\n  Batch: 163 Loss: 1.0454092025756836\n  Batch: 164 Loss: 1.0463790893554688\n  Batch: 165 Loss: 1.0522565841674805\n  Batch: 166 Loss: 1.0214978456497192\n  Batch: 167 Loss: 1.046970248222351\n  Batch: 168 Loss: 1.0462982654571533\n  Batch: 169 Loss: 1.046793818473816\n  Batch: 170 Loss: 1.0463063716888428\n  Batch: 171 Loss: 1.0466269254684448\n  Batch: 172 Loss: 1.0459699630737305\n  Batch: 173 Loss: 1.0332520008087158\n  Batch: 174 Loss: 1.0470011234283447\n  Batch: 175 Loss: 1.0461102724075317\n  Batch: 176 Loss: 1.050307273864746\n  Batch: 177 Loss: 1.042587399482727\n  Batch: 178 Loss: 1.0477149486541748\n  Batch: 179 Loss: 1.0461701154708862\n  Batch: 180 Loss: 1.0463175773620605\n  Batch: 181 Loss: 1.0430777072906494\n  Batch: 182 Loss: 1.0458509922027588\n  Batch: 183 Loss: 1.04562509059906\n  Batch: 184 Loss: 1.045920491218567\n  Batch: 185 Loss: 1.0468158721923828\n  Batch: 186 Loss: 1.0457046031951904\n  Batch: 187 Loss: 1.046324372291565\n  Batch: 188 Loss: 1.0449565649032593\n  Batch: 189 Loss: 1.045790672302246\n  Batch: 190 Loss: 1.0264438390731812\n  Batch: 191 Loss: 1.0501742362976074\n  Batch: 192 Loss: 1.045163631439209\n  Batch: 193 Loss: 1.0456452369689941\n  Batch: 194 Loss: 1.0463224649429321\n  Batch: 195 Loss: 1.041056513786316\n  Batch: 196 Loss: 1.0449085235595703\n  Batch: 197 Loss: 1.0122565031051636\n  Batch: 198 Loss: 1.0473703145980835\n  Batch: 199 Loss: 1.0447862148284912\n  Batch: 200 Loss: 1.0308201313018799\n  Batch: 201 Loss: 1.04501473903656\n  Batch: 202 Loss: 1.0415289402008057\n  Batch: 203 Loss: 1.0459396839141846\n  Batch: 204 Loss: 1.044723629951477\n  Batch: 205 Loss: 1.0446441173553467\n  Batch: 206 Loss: 1.0306251049041748\n  Batch: 207 Loss: 1.0449570417404175\n  Batch: 208 Loss: 1.0452561378479004\n  Batch: 209 Loss: 1.0238150358200073\n  Batch: 210 Loss: 1.0423102378845215\n  Batch: 211 Loss: 1.0471657514572144\n  Batch: 212 Loss: 1.0442301034927368\n  Batch: 213 Loss: 1.0470013618469238\n  Batch: 214 Loss: 1.0452167987823486\n  Batch: 215 Loss: 1.0471364259719849\n  Batch: 216 Loss: 1.0452851057052612\n  Batch: 217 Loss: 1.0456703901290894\n  Batch: 218 Loss: 1.0461186170578003\n  Batch: 219 Loss: 1.0275297164916992\n  Batch: 220 Loss: 1.0442484617233276\n  Batch: 221 Loss: 1.0444750785827637\n  Batch: 222 Loss: 1.1215846538543701\n  Batch: 223 Loss: 1.0407776832580566\n  Batch: 224 Loss: 1.0248016119003296\n  Batch: 225 Loss: 1.0213184356689453\n  Batch: 226 Loss: 1.047135591506958\n  Batch: 227 Loss: 1.044259786605835\n  Batch: 228 Loss: 1.04413902759552\n  Batch: 229 Loss: 1.0439766645431519\n  Batch: 230 Loss: 1.0443707704544067\n  Batch: 231 Loss: 0.9742778539657593\n  Batch: 232 Loss: 1.0444848537445068\n  Batch: 233 Loss: 1.0525158643722534\n  Batch: 234 Loss: 1.03662109375\n  Batch: 235 Loss: 1.0447412729263306\n  Batch: 236 Loss: 1.0499441623687744\n  Batch: 237 Loss: 1.0516397953033447\n  Batch: 238 Loss: 1.046623706817627\n  Batch: 239 Loss: 1.0589964389801025\n  Batch: 240 Loss: 1.0444313287734985\n  Batch: 241 Loss: 1.044000506401062\n  Batch: 242 Loss: 1.0438553094863892\n  Batch: 243 Loss: 1.0452919006347656\n  Batch: 244 Loss: 1.029537320137024\n  Batch: 245 Loss: 1.0432841777801514\n  Batch: 246 Loss: 1.0459705591201782\n  Batch: 247 Loss: 1.0443180799484253\n  Batch: 248 Loss: 1.0437040328979492\n  Batch: 249 Loss: 1.0432422161102295\n  Batch: 250 Loss: 1.0436742305755615\n  Batch: 251 Loss: 1.0433845520019531\n  Batch: 252 Loss: 1.0431371927261353\n  Batch: 253 Loss: 1.0447983741760254\n  Batch: 254 Loss: 1.0432853698730469\n  Batch: 255 Loss: 1.0481278896331787\n  Batch: 256 Loss: 1.0427920818328857\n  Batch: 257 Loss: 1.0450239181518555\n  Batch: 258 Loss: 1.029981255531311\n  Batch: 259 Loss: 1.04265558719635\n  Batch: 260 Loss: 1.0426405668258667\n  Batch: 261 Loss: 1.0432507991790771\n  Batch: 262 Loss: 1.0406326055526733\n  Batch: 263 Loss: 1.0449626445770264\n  Batch: 264 Loss: 1.0075132846832275\n  Batch: 265 Loss: 1.0428240299224854\n  Batch: 266 Loss: 1.0444833040237427\n  Batch: 267 Loss: 1.015078067779541\n  Batch: 268 Loss: 1.043115258216858\n  Batch: 269 Loss: 1.0520588159561157\n  Batch: 270 Loss: 1.0282319784164429\n  Batch: 271 Loss: 1.039903163909912\n  Batch: 272 Loss: 1.0471317768096924\n  Batch: 273 Loss: 1.0451915264129639\n  Batch: 274 Loss: 1.0452672243118286\n  Batch: 275 Loss: 1.0309886932373047\n  Batch: 276 Loss: 1.0433173179626465\n  Batch: 277 Loss: 1.042654275894165\n  Batch: 278 Loss: 1.0412951707839966\n  Batch: 279 Loss: 1.0470460653305054\n  Batch: 280 Loss: 1.0424659252166748\n  Batch: 281 Loss: 0.9934849739074707\n  Batch: 282 Loss: 1.0421478748321533\n  Batch: 283 Loss: 1.046546220779419\n  Batch: 284 Loss: 1.0428305864334106\n  Batch: 285 Loss: 1.0136449337005615\n  Batch: 286 Loss: 1.0419813394546509\n  Batch: 287 Loss: 1.042266845703125\n  Batch: 288 Loss: 1.041944146156311\n  Batch: 289 Loss: 1.0413920879364014\n  Batch: 290 Loss: 1.0391696691513062\n  Batch: 291 Loss: 1.0418994426727295\n  Batch: 292 Loss: 1.0421910285949707\n  Batch: 293 Loss: 1.034060001373291\n  Batch: 294 Loss: 1.0453795194625854\n  Batch: 295 Loss: 1.0430030822753906\n  Batch: 296 Loss: 1.0364718437194824\n  Batch: 297 Loss: 1.0391074419021606\n  Batch: 298 Loss: 1.042261004447937\n  Batch: 299 Loss: 1.0417336225509644\n  Batch: 300 Loss: 1.0414109230041504\n  Batch: 301 Loss: 1.040190577507019\n  Batch: 302 Loss: 1.0414586067199707\n  Batch: 303 Loss: 1.0350005626678467\n  Batch: 304 Loss: 1.0362722873687744\n  Batch: 305 Loss: 1.0310670137405396\n  Batch: 306 Loss: 1.041731357574463\n  Batch: 307 Loss: 1.042801856994629\n  Batch: 308 Loss: 1.0410315990447998\n  Batch: 309 Loss: 1.0410797595977783\n  Batch: 310 Loss: 1.0426751375198364\n  Batch: 311 Loss: 1.0379679203033447\n  Batch: 312 Loss: 1.0390452146530151\n  Batch: 313 Loss: 1.0416547060012817\n  Batch: 314 Loss: 0.9667717218399048\n  Batch: 315 Loss: 1.0416345596313477\n  Batch: 316 Loss: 1.0381906032562256\n  Batch: 317 Loss: 1.0411978960037231\n  Batch: 318 Loss: 1.0409036874771118\n  Batch: 319 Loss: 1.041020393371582\n  Batch: 320 Loss: 1.0417534112930298\n  Batch: 321 Loss: 1.0503673553466797\n  Batch: 322 Loss: 1.0410138368606567\n  Batch: 323 Loss: 1.0407731533050537\n  Batch: 324 Loss: 1.040687084197998\n  Batch: 325 Loss: 1.0439789295196533\n  Batch: 326 Loss: 1.0423691272735596\n  Batch: 327 Loss: 1.0405651330947876\n  Batch: 328 Loss: 1.0403051376342773\n  Batch: 329 Loss: 1.040345549583435\n  Batch: 330 Loss: 1.040343165397644\n  Batch: 331 Loss: 1.0401883125305176\n  Batch: 332 Loss: 1.0401499271392822\n  Batch: 333 Loss: 1.0409634113311768\n  Batch: 334 Loss: 1.040411114692688\n  Batch: 335 Loss: 1.0401428937911987\n  Batch: 336 Loss: 1.0402613878250122\nEpoch: 8 Loss: 1.0446757076396829\n  Batch: 1 Loss: 1.0400446653366089\n  Batch: 2 Loss: 1.0165119171142578\n  Batch: 3 Loss: 1.0406522750854492\n  Batch: 4 Loss: 1.040848970413208\n  Batch: 5 Loss: 1.0399889945983887\n  Batch: 6 Loss: 1.0405529737472534\n  Batch: 7 Loss: 1.0398293733596802\n  Batch: 8 Loss: 1.0338188409805298\n  Batch: 9 Loss: 1.047661304473877\n  Batch: 10 Loss: 1.0400891304016113\n  Batch: 11 Loss: 1.0399590730667114\n  Batch: 12 Loss: 1.037909984588623\n  Batch: 13 Loss: 1.025219440460205\n  Batch: 14 Loss: 1.0406218767166138\n  Batch: 15 Loss: 1.0412908792495728\n  Batch: 16 Loss: 1.0414180755615234\n  Batch: 17 Loss: 1.0400422811508179\n  Batch: 18 Loss: 1.034266471862793\n  Batch: 19 Loss: 1.0394515991210938\n  Batch: 20 Loss: 1.0287768840789795\n  Batch: 21 Loss: 1.0392245054244995\n  Batch: 22 Loss: 1.0403549671173096\n  Batch: 23 Loss: 1.0372921228408813\n  Batch: 24 Loss: 1.039768099784851\n  Batch: 25 Loss: 1.0391982793807983\n  Batch: 26 Loss: 0.929804801940918\n  Batch: 27 Loss: 1.0382181406021118\n  Batch: 28 Loss: 1.0396215915679932\n  Batch: 29 Loss: 1.039548635482788\n  Batch: 30 Loss: 1.0403172969818115\n  Batch: 31 Loss: 1.0401190519332886\n  Batch: 32 Loss: 1.023314356803894\n  Batch: 33 Loss: 1.040439486503601\n  Batch: 34 Loss: 1.019460678100586\n  Batch: 35 Loss: 1.039859414100647\n  Batch: 36 Loss: 1.0483591556549072\n  Batch: 37 Loss: 1.0617141723632812\n  Batch: 38 Loss: 1.0397918224334717\n  Batch: 39 Loss: 1.0400413274765015\n  Batch: 40 Loss: 1.0390578508377075\n  Batch: 41 Loss: 1.038930892944336\n  Batch: 42 Loss: 1.0390568971633911\n  Batch: 43 Loss: 1.0391819477081299\n  Batch: 44 Loss: 1.0388964414596558\n  Batch: 45 Loss: 1.0401736497879028\n  Batch: 46 Loss: 1.0388931035995483\n  Batch: 47 Loss: 1.0387687683105469\n  Batch: 48 Loss: 1.0386033058166504\n  Batch: 49 Loss: 1.0387760400772095\n  Batch: 50 Loss: 1.039749264717102\n  Batch: 51 Loss: 1.0382840633392334\n  Batch: 52 Loss: 1.0428167581558228\n  Batch: 53 Loss: 1.0386791229248047\n  Batch: 54 Loss: 1.0382583141326904\n  Batch: 55 Loss: 1.1039800643920898\n  Batch: 56 Loss: 1.0402660369873047\n  Batch: 57 Loss: 1.03830885887146\n  Batch: 58 Loss: 1.0375244617462158\n  Batch: 59 Loss: 1.0392719507217407\n  Batch: 60 Loss: 1.0397429466247559\n  Batch: 61 Loss: 1.0549668073654175\n  Batch: 62 Loss: 1.0398838520050049\n  Batch: 63 Loss: 1.0389373302459717\n  Batch: 64 Loss: 1.036115050315857\n  Batch: 65 Loss: 1.0403814315795898\n  Batch: 66 Loss: 1.0387088060379028\n  Batch: 67 Loss: 0.9852455854415894\n  Batch: 68 Loss: 1.0320698022842407\n  Batch: 69 Loss: 1.0380462408065796\n  Batch: 70 Loss: 1.017654538154602\n  Batch: 71 Loss: 1.0381561517715454\n  Batch: 72 Loss: 1.0220814943313599\n  Batch: 73 Loss: 1.0548723936080933\n  Batch: 74 Loss: 1.0390894412994385\n  Batch: 75 Loss: 1.0397049188613892\n  Batch: 76 Loss: 1.0392239093780518\n  Batch: 77 Loss: 1.0451743602752686\n  Batch: 78 Loss: 1.0396019220352173\n  Batch: 79 Loss: 1.039493203163147\n  Batch: 80 Loss: 1.0378804206848145\n  Batch: 81 Loss: 1.0111349821090698\n  Batch: 82 Loss: 1.0362571477890015\n  Batch: 83 Loss: 1.0299328565597534\n  Batch: 84 Loss: 1.0381041765213013\n  Batch: 85 Loss: 1.0374655723571777\n  Batch: 86 Loss: 1.0434110164642334\n  Batch: 87 Loss: 1.0381362438201904\n  Batch: 88 Loss: 1.037327527999878\n  Batch: 89 Loss: 1.0372943878173828\n  Batch: 90 Loss: 1.037458062171936\n  Batch: 91 Loss: 1.0372450351715088\n  Batch: 92 Loss: 1.0434472560882568\n  Batch: 93 Loss: 1.0474742650985718\n  Batch: 94 Loss: 1.0377088785171509\n  Batch: 95 Loss: 1.0371265411376953\n  Batch: 96 Loss: 1.0370147228240967\n  Batch: 97 Loss: 1.0384209156036377\n  Batch: 98 Loss: 1.0369784832000732\n  Batch: 99 Loss: 1.0387392044067383\n  Batch: 100 Loss: 1.0355247259140015\n  Batch: 101 Loss: 1.0079900026321411\n  Batch: 102 Loss: 1.03680419921875\n  Batch: 103 Loss: 1.0369071960449219\n  Batch: 104 Loss: 1.0357897281646729\n  Batch: 105 Loss: 1.0356528759002686\n  Batch: 106 Loss: 1.0348536968231201\n  Batch: 107 Loss: 1.0369391441345215\n  Batch: 108 Loss: 1.0371037721633911\n  Batch: 109 Loss: 1.036903977394104\n  Batch: 110 Loss: 1.0366390943527222\n  Batch: 111 Loss: 1.0370486974716187\n  Batch: 112 Loss: 1.031790018081665\n  Batch: 113 Loss: 1.0368638038635254\n  Batch: 114 Loss: 1.0368709564208984\n  Batch: 115 Loss: 1.0188078880310059\n  Batch: 116 Loss: 1.0390874147415161\n  Batch: 117 Loss: 1.0372138023376465\n  Batch: 118 Loss: 0.9947475790977478\n  Batch: 119 Loss: 1.037000060081482\n  Batch: 120 Loss: 1.03675377368927\n  Batch: 121 Loss: 1.0365015268325806\n  Batch: 122 Loss: 1.0389784574508667\n  Batch: 123 Loss: 1.0370804071426392\n  Batch: 124 Loss: 1.0363484621047974\n  Batch: 125 Loss: 1.0349966287612915\n  Batch: 126 Loss: 0.9444090127944946\n  Batch: 127 Loss: 1.0361591577529907\n  Batch: 128 Loss: 1.0368512868881226\n  Batch: 129 Loss: 1.0409754514694214\n  Batch: 130 Loss: 1.0404267311096191\n  Batch: 131 Loss: 1.0390912294387817\n  Batch: 132 Loss: 1.03620183467865\n  Batch: 133 Loss: 1.0543794631958008\n  Batch: 134 Loss: 1.0381444692611694\n  Batch: 135 Loss: 1.0385311841964722\n  Batch: 136 Loss: 0.9479539394378662\n  Batch: 137 Loss: 1.036091685295105\n  Batch: 138 Loss: 1.029863715171814\n  Batch: 139 Loss: 1.0361014604568481\n  Batch: 140 Loss: 1.0230365991592407\n  Batch: 141 Loss: 1.0241308212280273\n  Batch: 142 Loss: 1.0376852750778198\n  Batch: 143 Loss: 1.035965085029602\n  Batch: 144 Loss: 1.030888319015503\n  Batch: 145 Loss: 1.0352376699447632\n  Batch: 146 Loss: 1.0393636226654053\n  Batch: 147 Loss: 1.0376231670379639\n  Batch: 148 Loss: 1.008423924446106\n  Batch: 149 Loss: 1.0357863903045654\n  Batch: 150 Loss: 1.0353978872299194\n  Batch: 151 Loss: 1.0358803272247314\n  Batch: 152 Loss: 1.0335055589675903\n  Batch: 153 Loss: 0.9915427565574646\n  Batch: 154 Loss: 1.0372588634490967\n  Batch: 155 Loss: 1.0361173152923584\n  Batch: 156 Loss: 1.0464816093444824\n  Batch: 157 Loss: 1.0441384315490723\n  Batch: 158 Loss: 0.9899125695228577\n  Batch: 159 Loss: 1.0377376079559326\n  Batch: 160 Loss: 1.0297852754592896\n  Batch: 161 Loss: 1.038174033164978\n  Batch: 162 Loss: 1.0354499816894531\n  Batch: 163 Loss: 1.0361896753311157\n  Batch: 164 Loss: 1.0357435941696167\n  Batch: 165 Loss: 1.0350617170333862\n  Batch: 166 Loss: 1.0381137132644653\n  Batch: 167 Loss: 1.0350478887557983\n  Batch: 168 Loss: 1.0396977663040161\n  Batch: 169 Loss: 1.0370804071426392\n  Batch: 170 Loss: 1.0384767055511475\n  Batch: 171 Loss: 1.0358906984329224\n  Batch: 172 Loss: 1.0253437757492065\n  Batch: 173 Loss: 1.027530550956726\n  Batch: 174 Loss: 1.0363006591796875\n  Batch: 175 Loss: 1.0357649326324463\n  Batch: 176 Loss: 1.0350786447525024\n  Batch: 177 Loss: 1.0347068309783936\n  Batch: 178 Loss: 1.0347100496292114\n  Batch: 179 Loss: 1.0149898529052734\n  Batch: 180 Loss: 1.0373725891113281\n  Batch: 181 Loss: 1.036126971244812\n  Batch: 182 Loss: 1.0284730195999146\n  Batch: 183 Loss: 1.0242794752120972\n  Batch: 184 Loss: 1.0348241329193115\n  Batch: 185 Loss: 1.0344806909561157\n  Batch: 186 Loss: 0.9950653314590454\n  Batch: 187 Loss: 1.0165821313858032\n  Batch: 188 Loss: 1.0392515659332275\n  Batch: 189 Loss: 1.0154964923858643\n  Batch: 190 Loss: 1.0349209308624268\n  Batch: 191 Loss: 1.0342631340026855\n  Batch: 192 Loss: 1.0347281694412231\n  Batch: 193 Loss: 1.0347317457199097\n  Batch: 194 Loss: 1.0325708389282227\n  Batch: 195 Loss: 1.0347095727920532\n  Batch: 196 Loss: 1.0371570587158203\n  Batch: 197 Loss: 1.0330816507339478\n  Batch: 198 Loss: 1.0360934734344482\n  Batch: 199 Loss: 1.0345834493637085\n  Batch: 200 Loss: 1.0378013849258423\n  Batch: 201 Loss: 1.0174649953842163\n  Batch: 202 Loss: 1.034305214881897\n  Batch: 203 Loss: 1.0381507873535156\n  Batch: 204 Loss: 1.0339897871017456\n  Batch: 205 Loss: 1.038601279258728\n  Batch: 206 Loss: 1.0395094156265259\n  Batch: 207 Loss: 1.0339382886886597\n  Batch: 208 Loss: 1.03507661819458\n  Batch: 209 Loss: 1.0342601537704468\n  Batch: 210 Loss: 1.0061914920806885\n  Batch: 211 Loss: 1.03474760055542\n  Batch: 212 Loss: 1.033629059791565\n  Batch: 213 Loss: 1.0187996625900269\n  Batch: 214 Loss: 1.0290279388427734\n  Batch: 215 Loss: 1.0339752435684204\n  Batch: 216 Loss: 1.0337824821472168\n  Batch: 217 Loss: 1.033847689628601\n  Batch: 218 Loss: 1.0334937572479248\n  Batch: 219 Loss: 1.036302924156189\n  Batch: 220 Loss: 1.0337448120117188\n  Batch: 221 Loss: 1.0346812009811401\n  Batch: 222 Loss: 1.0333524942398071\n  Batch: 223 Loss: 1.0357955694198608\n  Batch: 224 Loss: 1.0340440273284912\n  Batch: 225 Loss: 0.988671600818634\n  Batch: 226 Loss: 1.033470869064331\n  Batch: 227 Loss: 1.030174732208252\n  Batch: 228 Loss: 0.9841395616531372\n  Batch: 229 Loss: 1.0341376066207886\n  Batch: 230 Loss: 1.025612235069275\n  Batch: 231 Loss: 1.0348283052444458\n  Batch: 232 Loss: 1.0329992771148682\n  Batch: 233 Loss: 1.0291483402252197\n  Batch: 234 Loss: 0.990176796913147\n  Batch: 235 Loss: 1.035169005393982\n  Batch: 236 Loss: 1.0335392951965332\n  Batch: 237 Loss: 1.033479928970337\n  Batch: 238 Loss: 1.0334223508834839\n  Batch: 239 Loss: 1.035520076751709\n  Batch: 240 Loss: 1.0398789644241333\n  Batch: 241 Loss: 1.0329219102859497\n  Batch: 242 Loss: 1.0335228443145752\n  Batch: 243 Loss: 1.034845232963562\n  Batch: 244 Loss: 1.0350778102874756\n  Batch: 245 Loss: 1.033687710762024\n  Batch: 246 Loss: 1.0367101430892944\n  Batch: 247 Loss: 1.0329619646072388\n  Batch: 248 Loss: 1.0328824520111084\n  Batch: 249 Loss: 1.033097743988037\n  Batch: 250 Loss: 1.0326776504516602\n  Batch: 251 Loss: 1.0347436666488647\n  Batch: 252 Loss: 1.0330331325531006\n  Batch: 253 Loss: 1.0375317335128784\n  Batch: 254 Loss: 1.0311551094055176\n  Batch: 255 Loss: 1.0331133604049683\n  Batch: 256 Loss: 1.0326346158981323\n  Batch: 257 Loss: 1.0287628173828125\n  Batch: 258 Loss: 1.0329135656356812\n  Batch: 259 Loss: 1.0326967239379883\n  Batch: 260 Loss: 1.0315525531768799\n  Batch: 261 Loss: 1.0330595970153809\n  Batch: 262 Loss: 1.0189179182052612\n  Batch: 263 Loss: 1.0221797227859497\n  Batch: 264 Loss: 1.0488475561141968\n  Batch: 265 Loss: 1.0336374044418335\n  Batch: 266 Loss: 1.0342668294906616\n  Batch: 267 Loss: 1.014769434928894\n  Batch: 268 Loss: 1.0325015783309937\n  Batch: 269 Loss: 1.0322396755218506\n  Batch: 270 Loss: 1.0324102640151978\n  Batch: 271 Loss: 1.0336652994155884\n  Batch: 272 Loss: 0.9713344573974609\n  Batch: 273 Loss: 0.890252947807312\n  Batch: 274 Loss: 0.9903472065925598\n  Batch: 275 Loss: 1.0059598684310913\n  Batch: 276 Loss: 0.9902374148368835\n  Batch: 277 Loss: 1.042712688446045\n  Batch: 278 Loss: 1.0400179624557495\n  Batch: 279 Loss: 1.0390052795410156\n  Batch: 280 Loss: 1.034380555152893\n  Batch: 281 Loss: 1.035373330116272\n  Batch: 282 Loss: 1.0323066711425781\n  Batch: 283 Loss: 1.0452975034713745\n  Batch: 284 Loss: 1.0339113473892212\n  Batch: 285 Loss: 1.0321837663650513\n  Batch: 286 Loss: 1.0324335098266602\n  Batch: 287 Loss: 1.0328558683395386\n  Batch: 288 Loss: 1.022934913635254\n  Batch: 289 Loss: 1.0328080654144287\n  Batch: 290 Loss: 1.0319404602050781\n  Batch: 291 Loss: 1.0320769548416138\n  Batch: 292 Loss: 1.0320155620574951\n  Batch: 293 Loss: 1.0314165353775024\n  Batch: 294 Loss: 1.0322391986846924\n  Batch: 295 Loss: 1.0316251516342163\n  Batch: 296 Loss: 1.0066934823989868\n  Batch: 297 Loss: 1.0320229530334473\n  Batch: 298 Loss: 1.0322883129119873\n  Batch: 299 Loss: 1.0314580202102661\n  Batch: 300 Loss: 1.0319346189498901\n  Batch: 301 Loss: 1.0313502550125122\n  Batch: 302 Loss: 1.0295363664627075\n  Batch: 303 Loss: 1.0324029922485352\n  Batch: 304 Loss: 1.0315828323364258\n  Batch: 305 Loss: 1.0319185256958008\n  Batch: 306 Loss: 1.0180258750915527\n  Batch: 307 Loss: 1.0320311784744263\n  Batch: 308 Loss: 1.0348535776138306\n  Batch: 309 Loss: 1.0246098041534424\n  Batch: 310 Loss: 1.032116413116455\n  Batch: 311 Loss: 1.0320930480957031\n  Batch: 312 Loss: 1.034432053565979\n  Batch: 313 Loss: 1.0071444511413574\n  Batch: 314 Loss: 1.0319337844848633\n  Batch: 315 Loss: 1.0332738161087036\n  Batch: 316 Loss: 1.0324856042861938\n  Batch: 317 Loss: 1.0375216007232666\n  Batch: 318 Loss: 1.0335997343063354\n  Batch: 319 Loss: 1.032355546951294\n  Batch: 320 Loss: 1.03286874294281\n  Batch: 321 Loss: 1.031437635421753\n  Batch: 322 Loss: 1.0326274633407593\n  Batch: 323 Loss: 1.0309746265411377\n  Batch: 324 Loss: 1.0308313369750977\n  Batch: 325 Loss: 1.0426719188690186\n  Batch: 326 Loss: 1.030868649482727\n  Batch: 327 Loss: 1.031154990196228\n  Batch: 328 Loss: 1.0308095216751099\n  Batch: 329 Loss: 1.030772089958191\n  Batch: 330 Loss: 1.031388759613037\n  Batch: 331 Loss: 1.03153395652771\n  Batch: 332 Loss: 1.0246130228042603\n  Batch: 333 Loss: 1.0217422246932983\n  Batch: 334 Loss: 1.0306270122528076\n  Batch: 335 Loss: 1.0315452814102173\n  Batch: 336 Loss: 1.0320472717285156\nEpoch: 9 Loss: 1.0319308258947872\n  Batch: 1 Loss: 1.0323843955993652\n  Batch: 2 Loss: 1.036128044128418\n  Batch: 3 Loss: 1.0302236080169678\n  Batch: 4 Loss: 1.0305169820785522\n  Batch: 5 Loss: 1.0110565423965454\n  Batch: 6 Loss: 1.0304676294326782\n  Batch: 7 Loss: 1.0321071147918701\n  Batch: 8 Loss: 1.0306729078292847\n  Batch: 9 Loss: 1.0307435989379883\n  Batch: 10 Loss: 1.031795859336853\n  Batch: 11 Loss: 1.030351161956787\n  Batch: 12 Loss: 1.031384825706482\n  Batch: 13 Loss: 1.0336517095565796\n  Batch: 14 Loss: 1.0304185152053833\n  Batch: 15 Loss: 1.0165444612503052\n  Batch: 16 Loss: 1.0302577018737793\n  Batch: 17 Loss: 0.9935154914855957\n  Batch: 18 Loss: 1.02083158493042\n  Batch: 19 Loss: 0.966989278793335\n  Batch: 20 Loss: 1.0386353731155396\n  Batch: 21 Loss: 1.0314081907272339\n  Batch: 22 Loss: 1.0278879404067993\n  Batch: 23 Loss: 1.0324745178222656\n  Batch: 24 Loss: 1.0314974784851074\n  Batch: 25 Loss: 1.0327972173690796\n  Batch: 26 Loss: 1.0312371253967285\n  Batch: 27 Loss: 1.0301364660263062\n  Batch: 28 Loss: 1.0161468982696533\n  Batch: 29 Loss: 1.030585527420044\n  Batch: 30 Loss: 1.030871868133545\n  Batch: 31 Loss: 0.9720214605331421\n  Batch: 32 Loss: 1.0266350507736206\n  Batch: 33 Loss: 1.0361614227294922\n  Batch: 34 Loss: 1.019452452659607\n  Batch: 35 Loss: 1.0305359363555908\n  Batch: 36 Loss: 1.030700445175171\n  Batch: 37 Loss: 1.0327526330947876\n  Batch: 38 Loss: 1.0391335487365723\n  Batch: 39 Loss: 1.031615972518921\n  Batch: 40 Loss: 1.0296460390090942\n  Batch: 41 Loss: 1.0295206308364868\n  Batch: 42 Loss: 1.0295045375823975\n  Batch: 43 Loss: 1.0255215167999268\n  Batch: 44 Loss: 1.0321325063705444\n  Batch: 45 Loss: 1.0294578075408936\n  Batch: 46 Loss: 1.0300847291946411\n  Batch: 47 Loss: 1.030450701713562\n  Batch: 48 Loss: 1.0209580659866333\n  Batch: 49 Loss: 1.029937744140625\n  Batch: 50 Loss: 1.021261215209961\n  Batch: 51 Loss: 1.0263406038284302\n  Batch: 52 Loss: 1.0303313732147217\n  Batch: 53 Loss: 1.0292983055114746\n  Batch: 54 Loss: 1.0295549631118774\n  Batch: 55 Loss: 1.0294965505599976\n  Batch: 56 Loss: 1.0297216176986694\n  Batch: 57 Loss: 1.0220328569412231\n  Batch: 58 Loss: 1.0290632247924805\n  Batch: 59 Loss: 1.029018521308899\n  Batch: 60 Loss: 1.0290411710739136\n  Batch: 61 Loss: 1.0289679765701294\n  Batch: 62 Loss: 1.0293512344360352\n  Batch: 63 Loss: 1.029052972793579\n  Batch: 64 Loss: 1.0140981674194336\n  Batch: 65 Loss: 1.009219765663147\n  Batch: 66 Loss: 1.022022008895874\n  Batch: 67 Loss: 1.015580177307129\n  Batch: 68 Loss: 1.0301748514175415\n  Batch: 69 Loss: 1.0294517278671265\n  Batch: 70 Loss: 1.0291837453842163\n  Batch: 71 Loss: 1.0300425291061401\n  Batch: 72 Loss: 1.0294914245605469\n  Batch: 73 Loss: 1.0306843519210815\n  Batch: 74 Loss: 1.0316591262817383\n  Batch: 75 Loss: 1.0295674800872803\n  Batch: 76 Loss: 1.0311157703399658\n  Batch: 77 Loss: 1.0288463830947876\n  Batch: 78 Loss: 1.0290377140045166\n  Batch: 79 Loss: 1.0251381397247314\n  Batch: 80 Loss: 0.9737675786018372\n  Batch: 81 Loss: 1.014190912246704\n  Batch: 82 Loss: 1.0152175426483154\n  Batch: 83 Loss: 1.0286307334899902\n  Batch: 84 Loss: 1.0292621850967407\n  Batch: 85 Loss: 1.0282448530197144\n  Batch: 86 Loss: 0.9895201325416565\n  Batch: 87 Loss: 1.0320419073104858\n  Batch: 88 Loss: 0.9811356067657471\n  Batch: 89 Loss: 0.9122923612594604\n  Batch: 90 Loss: 1.0368579626083374\n  Batch: 91 Loss: 1.0303951501846313\n  Batch: 92 Loss: 1.029744267463684\n  Batch: 93 Loss: 0.9718133807182312\n  Batch: 94 Loss: 0.9798148274421692\n  Batch: 95 Loss: 1.0289421081542969\n  Batch: 96 Loss: 1.0296666622161865\n  Batch: 97 Loss: 1.0327658653259277\n  Batch: 98 Loss: 1.031359314918518\n  Batch: 99 Loss: 1.025309681892395\n  Batch: 100 Loss: 1.0353178977966309\n  Batch: 101 Loss: 1.028069019317627\n  Batch: 102 Loss: 0.9898005127906799\n  Batch: 103 Loss: 1.0290800333023071\n  Batch: 104 Loss: 1.0287725925445557\n  Batch: 105 Loss: 1.0219119787216187\n  Batch: 106 Loss: 1.0358010530471802\n  Batch: 107 Loss: 1.0300885438919067\n  Batch: 108 Loss: 1.0291327238082886\n  Batch: 109 Loss: 1.0260189771652222\n  Batch: 110 Loss: 1.0205222368240356\n  Batch: 111 Loss: 1.028062105178833\n  Batch: 112 Loss: 1.0279972553253174\n  Batch: 113 Loss: 1.0289019346237183\n  Batch: 114 Loss: 1.027981162071228\n  Batch: 115 Loss: 1.028685212135315\n  Batch: 116 Loss: 1.0277454853057861\n  Batch: 117 Loss: 1.027904987335205\n  Batch: 118 Loss: 1.028037428855896\n  Batch: 119 Loss: 1.0079989433288574\n  Batch: 120 Loss: 1.027729868888855\n  Batch: 121 Loss: 1.028891682624817\n  Batch: 122 Loss: 1.0282706022262573\n  Batch: 123 Loss: 1.0236784219741821\n  Batch: 124 Loss: 1.027746319770813\n  Batch: 125 Loss: 1.030125617980957\n  Batch: 126 Loss: 1.0249783992767334\n  Batch: 127 Loss: 1.028565526008606\n  Batch: 128 Loss: 1.0264042615890503\n  Batch: 129 Loss: 1.026341438293457\n  Batch: 130 Loss: 1.0078043937683105\n  Batch: 131 Loss: 1.012473702430725\n  Batch: 132 Loss: 1.0273523330688477\n  Batch: 133 Loss: 1.0274550914764404\n  Batch: 134 Loss: 1.0282584428787231\n  Batch: 135 Loss: 1.0260614156723022\n  Batch: 136 Loss: 1.0276302099227905\n  Batch: 137 Loss: 1.031591534614563\n  Batch: 138 Loss: 0.9839931726455688\n  Batch: 139 Loss: 1.0276800394058228\n  Batch: 140 Loss: 1.0274896621704102\n  Batch: 141 Loss: 0.9308557510375977\n  Batch: 142 Loss: 1.0276644229888916\n  Batch: 143 Loss: 1.0275700092315674\n  Batch: 144 Loss: 0.9714590907096863\n  Batch: 145 Loss: 1.027807593345642\n  Batch: 146 Loss: 1.0311481952667236\n  Batch: 147 Loss: 1.027536392211914\n  Batch: 148 Loss: 1.027451753616333\n  Batch: 149 Loss: 1.0256551504135132\n  Batch: 150 Loss: 1.0356816053390503\n  Batch: 151 Loss: 1.0273724794387817\n  Batch: 152 Loss: 1.0294897556304932\n  Batch: 153 Loss: 1.0433045625686646\n  Batch: 154 Loss: 1.014917016029358\n  Batch: 155 Loss: 1.0272399187088013\n  Batch: 156 Loss: 1.012607455253601\n  Batch: 157 Loss: 1.0280495882034302\n  Batch: 158 Loss: 1.0271079540252686\n  Batch: 159 Loss: 1.027220606803894\n  Batch: 160 Loss: 1.0239057540893555\n  Batch: 161 Loss: 1.0289415121078491\n  Batch: 162 Loss: 1.0258885622024536\n  Batch: 163 Loss: 0.9744032025337219\n  Batch: 164 Loss: 1.0279350280761719\n  Batch: 165 Loss: 1.0286592245101929\n  Batch: 166 Loss: 1.0268973112106323\n  Batch: 167 Loss: 1.0282329320907593\n  Batch: 168 Loss: 1.0266910791397095\n  Batch: 169 Loss: 0.9816322326660156\n  Batch: 170 Loss: 1.0266823768615723\n  Batch: 171 Loss: 1.0277458429336548\n  Batch: 172 Loss: 1.0260449647903442\n  Batch: 173 Loss: 1.0237972736358643\n  Batch: 174 Loss: 1.026457667350769\n  Batch: 175 Loss: 1.0267733335494995\n  Batch: 176 Loss: 1.0236345529556274\n  Batch: 177 Loss: 1.0266107320785522\n  Batch: 178 Loss: 1.0265967845916748\n  Batch: 179 Loss: 1.0111453533172607\n  Batch: 180 Loss: 1.0303553342819214\n  Batch: 181 Loss: 1.027398705482483\n  Batch: 182 Loss: 1.0267490148544312\n  Batch: 183 Loss: 1.0264527797698975\n  Batch: 184 Loss: 1.0305047035217285\n  Batch: 185 Loss: 1.0267342329025269\n  Batch: 186 Loss: 1.0262763500213623\n  Batch: 187 Loss: 1.02842378616333\n  Batch: 188 Loss: 1.0268839597702026\n  Batch: 189 Loss: 1.0262715816497803\n  Batch: 190 Loss: 1.0263476371765137\n  Batch: 191 Loss: 1.0259641408920288\n  Batch: 192 Loss: 1.0262091159820557\n  Batch: 193 Loss: 1.02620530128479\n  Batch: 194 Loss: 1.022371768951416\n  Batch: 195 Loss: 1.0260506868362427\n  Batch: 196 Loss: 1.0267976522445679\n  Batch: 197 Loss: 1.0266335010528564\n  Batch: 198 Loss: 1.0274028778076172\n  Batch: 199 Loss: 0.9763008952140808\n  Batch: 200 Loss: 0.9950450658798218\n  Batch: 201 Loss: 1.0243911743164062\n  Batch: 202 Loss: 1.0276813507080078\n  Batch: 203 Loss: 1.0182734727859497\n  Batch: 204 Loss: 1.0281541347503662\n  Batch: 205 Loss: 1.0275956392288208\n  Batch: 206 Loss: 1.0272356271743774\n  Batch: 207 Loss: 1.024738073348999\n  Batch: 208 Loss: 1.0439788103103638\n  Batch: 209 Loss: 1.0307295322418213\n  Batch: 210 Loss: 1.0220801830291748\n  Batch: 211 Loss: 1.0332990884780884\n  Batch: 212 Loss: 1.0267084836959839\n  Batch: 213 Loss: 1.0256843566894531\n  Batch: 214 Loss: 1.0264467000961304\n  Batch: 215 Loss: 1.0256974697113037\n  Batch: 216 Loss: 1.025784969329834\n  Batch: 217 Loss: 1.0257254838943481\n  Batch: 218 Loss: 1.0336105823516846\n  Batch: 219 Loss: 1.0266202688217163\n  Batch: 220 Loss: 1.0259456634521484\n  Batch: 221 Loss: 1.0258241891860962\n  Batch: 222 Loss: 1.0258232355117798\n  Batch: 223 Loss: 1.0261955261230469\n  Batch: 224 Loss: 1.0257724523544312\n  Batch: 225 Loss: 1.00703763961792\n  Batch: 226 Loss: 1.0259891748428345\n  Batch: 227 Loss: 1.0259356498718262\n  Batch: 228 Loss: 1.0260039567947388\n  Batch: 229 Loss: 1.0312514305114746\n  Batch: 230 Loss: 1.0258114337921143\n  Batch: 231 Loss: 0.9915909171104431\n  Batch: 232 Loss: 1.0227926969528198\n  Batch: 233 Loss: 1.025521159172058\n  Batch: 234 Loss: 1.0406150817871094\n  Batch: 235 Loss: 1.0204771757125854\n  Batch: 236 Loss: 1.026589274406433\n  Batch: 237 Loss: 1.0388027429580688\n  Batch: 238 Loss: 1.0054339170455933\n  Batch: 239 Loss: 1.0616929531097412\n  Batch: 240 Loss: 1.0316191911697388\n  Batch: 241 Loss: 1.0273600816726685\n  Batch: 242 Loss: 1.0412864685058594\n  Batch: 243 Loss: 1.0234260559082031\n  Batch: 244 Loss: 1.0140399932861328\n  Batch: 245 Loss: 1.0165692567825317\n  Batch: 246 Loss: 1.0414056777954102\n  Batch: 247 Loss: 1.027809977531433\n  Batch: 248 Loss: 1.0266804695129395\n  Batch: 249 Loss: 1.0298867225646973\n  Batch: 250 Loss: 1.0274535417556763\n  Batch: 251 Loss: 1.027146816253662\n  Batch: 252 Loss: 0.9982113838195801\n  Batch: 253 Loss: 1.025331974029541\n  Batch: 254 Loss: 1.0285381078720093\n  Batch: 255 Loss: 1.026143193244934\n  Batch: 256 Loss: 1.0318771600723267\n  Batch: 257 Loss: 1.026951789855957\n  Batch: 258 Loss: 1.0246121883392334\n  Batch: 259 Loss: 1.0270241498947144\n  Batch: 260 Loss: 1.0263500213623047\n  Batch: 261 Loss: 1.0265859365463257\n  Batch: 262 Loss: 1.0258852243423462\n  Batch: 263 Loss: 1.0257222652435303\n  Batch: 264 Loss: 1.0529195070266724\n  Batch: 265 Loss: 1.0163540840148926\n  Batch: 266 Loss: 1.0253816843032837\n  Batch: 267 Loss: 1.0251832008361816\n  Batch: 268 Loss: 1.0263665914535522\n  Batch: 269 Loss: 1.026185154914856\n  Batch: 270 Loss: 1.025273323059082\n  Batch: 271 Loss: 1.0139257907867432\n  Batch: 272 Loss: 1.0252310037612915\n  Batch: 273 Loss: 1.0255190134048462\n  Batch: 274 Loss: 1.0383391380310059\n  Batch: 275 Loss: 1.0130500793457031\n  Batch: 276 Loss: 1.0253171920776367\n  Batch: 277 Loss: 1.0254334211349487\n  Batch: 278 Loss: 1.02524995803833\n  Batch: 279 Loss: 1.0248322486877441\n  Batch: 280 Loss: 1.025356650352478\n  Batch: 281 Loss: 1.0251421928405762\n  Batch: 282 Loss: 1.0249577760696411\n  Batch: 283 Loss: 1.0251723527908325\n  Batch: 284 Loss: 1.0248744487762451\n  Batch: 285 Loss: 1.0254161357879639\n  Batch: 286 Loss: 1.024576187133789\n  Batch: 287 Loss: 1.0246634483337402\n  Batch: 288 Loss: 1.0204154253005981\n  Batch: 289 Loss: 1.0245426893234253\n  Batch: 290 Loss: 1.0244721174240112\n  Batch: 291 Loss: 1.0204389095306396\n  Batch: 292 Loss: 1.025046706199646\n  Batch: 293 Loss: 1.0148309469223022\n  Batch: 294 Loss: 1.0240840911865234\n  Batch: 295 Loss: 1.0157358646392822\n  Batch: 296 Loss: 1.0241409540176392\n  Batch: 297 Loss: 1.0242761373519897\n  Batch: 298 Loss: 1.0244110822677612\n  Batch: 299 Loss: 1.024658203125\n  Batch: 300 Loss: 1.0303492546081543\n  Batch: 301 Loss: 1.0231223106384277\n  Batch: 302 Loss: 1.0233726501464844\n  Batch: 303 Loss: 1.0240545272827148\n  Batch: 304 Loss: 1.0240637063980103\n  Batch: 305 Loss: 1.0249814987182617\n  Batch: 306 Loss: 1.0244550704956055\n  Batch: 307 Loss: 1.0221525430679321\n  Batch: 308 Loss: 1.0240215063095093\n  Batch: 309 Loss: 1.0247478485107422\n  Batch: 310 Loss: 1.0241833925247192\n  Batch: 311 Loss: 1.0246247053146362\n  Batch: 312 Loss: 1.0391539335250854\n  Batch: 313 Loss: 1.0217012166976929\n  Batch: 314 Loss: 0.9990952014923096\n  Batch: 315 Loss: 1.0052231550216675\n  Batch: 316 Loss: 1.0198688507080078\n  Batch: 317 Loss: 1.0254936218261719\n  Batch: 318 Loss: 1.0206074714660645\n  Batch: 319 Loss: 1.0092803239822388\n  Batch: 320 Loss: 1.0242289304733276\n  Batch: 321 Loss: 1.024168848991394\n  Batch: 322 Loss: 0.901420533657074\n  Batch: 323 Loss: 0.9908799529075623\n  Batch: 324 Loss: 1.0257701873779297\n  Batch: 325 Loss: 1.0279388427734375\n  Batch: 326 Loss: 1.0210390090942383\n  Batch: 327 Loss: 0.8782519698143005\n  Batch: 328 Loss: 1.0366804599761963\n  Batch: 329 Loss: 1.0196579694747925\n  Batch: 330 Loss: 1.016467571258545\n  Batch: 331 Loss: 0.8330206274986267\n  Batch: 332 Loss: 1.039044737815857\n  Batch: 333 Loss: 1.0276501178741455\n  Batch: 334 Loss: 1.0179017782211304\n  Batch: 335 Loss: 1.0535186529159546\n  Batch: 336 Loss: 1.0242581367492676\nEpoch: 10 Loss: 1.0223600972621214\n  Batch: 1 Loss: 1.0250188112258911\n  Batch: 2 Loss: 1.0247899293899536\n  Batch: 3 Loss: 1.0285992622375488\n  Batch: 4 Loss: 1.024240493774414\n  Batch: 5 Loss: 1.023655652999878\n  Batch: 6 Loss: 1.0241622924804688\n  Batch: 7 Loss: 0.9939406514167786\n  Batch: 8 Loss: 0.8500933051109314\n  Batch: 9 Loss: 1.014654517173767\n  Batch: 10 Loss: 1.024731993675232\n  Batch: 11 Loss: 1.023687481880188\n  Batch: 12 Loss: 1.0245527029037476\n  Batch: 13 Loss: 1.023820161819458\n  Batch: 14 Loss: 0.9992049932479858\n  Batch: 15 Loss: 1.0321842432022095\n  Batch: 16 Loss: 1.0250983238220215\n  Batch: 17 Loss: 1.0275216102600098\n  Batch: 18 Loss: 1.0245012044906616\n  Batch: 19 Loss: 1.0509536266326904\n  Batch: 20 Loss: 1.0272879600524902\n  Batch: 21 Loss: 1.0657130479812622\n  Batch: 22 Loss: 1.018368124961853\n  Batch: 23 Loss: 0.9872631430625916\n  Batch: 24 Loss: 1.0234768390655518\n  Batch: 25 Loss: 1.0250439643859863\n  Batch: 26 Loss: 1.0227490663528442\n  Batch: 27 Loss: 1.0248080492019653\n  Batch: 28 Loss: 1.0235499143600464\n  Batch: 29 Loss: 1.023073434829712\n  Batch: 30 Loss: 1.0244340896606445\n  Batch: 31 Loss: 1.0231313705444336\n  Batch: 32 Loss: 1.0233820676803589\n  Batch: 33 Loss: 1.0248963832855225\n  Batch: 34 Loss: 1.024679183959961\n  Batch: 35 Loss: 1.023600459098816\n  Batch: 36 Loss: 1.0228649377822876\n  Batch: 37 Loss: 1.0229042768478394\n  Batch: 38 Loss: 1.0231525897979736\n  Batch: 39 Loss: 1.0135829448699951\n  Batch: 40 Loss: 1.0228397846221924\n  Batch: 41 Loss: 1.0231655836105347\n  Batch: 42 Loss: 1.0231716632843018\n  Batch: 43 Loss: 1.0244582891464233\n  Batch: 44 Loss: 1.0233527421951294\n  Batch: 45 Loss: 0.9939701557159424\n  Batch: 46 Loss: 1.0230222940444946\n  Batch: 47 Loss: 1.022950291633606\n  Batch: 48 Loss: 1.0220448970794678\n  Batch: 49 Loss: 1.020923376083374\n  Batch: 50 Loss: 1.023817777633667\n  Batch: 51 Loss: 1.023856282234192\n  Batch: 52 Loss: 1.024207353591919\n  Batch: 53 Loss: 1.0227406024932861\n  Batch: 54 Loss: 1.0280698537826538\n  Batch: 55 Loss: 0.9410303235054016\n  Batch: 56 Loss: 1.008113145828247\n  Batch: 57 Loss: 1.0145924091339111\n  Batch: 58 Loss: 1.019243597984314\n  Batch: 59 Loss: 1.0253016948699951\n  Batch: 60 Loss: 1.0225887298583984\n  Batch: 61 Loss: 1.0236400365829468\n  Batch: 62 Loss: 1.022675633430481\n  Batch: 63 Loss: 1.0232635736465454\n  Batch: 64 Loss: 1.0228502750396729\n  Batch: 65 Loss: 1.0246039628982544\n  Batch: 66 Loss: 1.0229440927505493\n  Batch: 67 Loss: 1.0224689245224\n  Batch: 68 Loss: 1.0226777791976929\n  Batch: 69 Loss: 1.0225106477737427\n  Batch: 70 Loss: 1.0138955116271973\n  Batch: 71 Loss: 1.023504376411438\n  Batch: 72 Loss: 1.023930311203003\n  Batch: 73 Loss: 1.0242981910705566\n  Batch: 74 Loss: 1.0229895114898682\n  Batch: 75 Loss: 1.0358270406723022\n  Batch: 76 Loss: 1.0221270322799683\n  Batch: 77 Loss: 1.0193760395050049\n  Batch: 78 Loss: 1.0224319696426392\n  Batch: 79 Loss: 1.0215181112289429\n  Batch: 80 Loss: 1.0171785354614258\n  Batch: 81 Loss: 1.0235016345977783\n  Batch: 82 Loss: 1.0220321416854858\n  Batch: 83 Loss: 1.0219182968139648\n  Batch: 84 Loss: 1.0220409631729126\n  Batch: 85 Loss: 1.0230175256729126\n  Batch: 86 Loss: 1.0241950750350952\n  Batch: 87 Loss: 1.022121548652649\n  Batch: 88 Loss: 1.0255872011184692\n  Batch: 89 Loss: 1.0225290060043335\n  Batch: 90 Loss: 1.038797378540039\n  Batch: 91 Loss: 1.0223658084869385\n  Batch: 92 Loss: 1.0221754312515259\n  Batch: 93 Loss: 1.0217328071594238\n  Batch: 94 Loss: 0.9586585760116577\n  Batch: 95 Loss: 1.0043281316757202\n  Batch: 96 Loss: 1.0237470865249634\n  Batch: 97 Loss: 1.0196452140808105\n  Batch: 98 Loss: 1.0225657224655151\n  Batch: 99 Loss: 0.9869733452796936\n  Batch: 100 Loss: 1.021658182144165\n  Batch: 101 Loss: 1.0245354175567627\n  Batch: 102 Loss: 1.0216459035873413\n  Batch: 103 Loss: 1.0174216032028198\n  Batch: 104 Loss: 1.0126798152923584\n  Batch: 105 Loss: 0.9952990412712097\n  Batch: 106 Loss: 1.0217359066009521\n  Batch: 107 Loss: 1.0229198932647705\n  Batch: 108 Loss: 1.026423454284668\n  Batch: 109 Loss: 1.0278629064559937\n  Batch: 110 Loss: 1.0059746503829956\n  Batch: 111 Loss: 1.021865963935852\n  Batch: 112 Loss: 1.0216996669769287\n  Batch: 113 Loss: 1.0261179208755493\n  Batch: 114 Loss: 1.0219104290008545\n  Batch: 115 Loss: 1.0218400955200195\n  Batch: 116 Loss: 1.025588035583496\n  Batch: 117 Loss: 1.028554916381836\n  Batch: 118 Loss: 1.022002100944519\n  Batch: 119 Loss: 1.0234532356262207\n  Batch: 120 Loss: 1.022642731666565\n  Batch: 121 Loss: 0.922959566116333\n  Batch: 122 Loss: 1.0250293016433716\n  Batch: 123 Loss: 1.037089467048645\n  Batch: 124 Loss: 0.950539231300354\n  Batch: 125 Loss: 1.0218826532363892\n  Batch: 126 Loss: 1.0225090980529785\n  Batch: 127 Loss: 1.0224298238754272\n  Batch: 128 Loss: 1.018412709236145\n  Batch: 129 Loss: 0.9909591674804688\n  Batch: 130 Loss: 1.0330485105514526\n  Batch: 131 Loss: 1.0074907541275024\n  Batch: 132 Loss: 1.0216648578643799\n  Batch: 133 Loss: 1.0298110246658325\n  Batch: 134 Loss: 1.0185387134552002\n  Batch: 135 Loss: 0.9843030571937561\n  Batch: 136 Loss: 1.0249701738357544\n  Batch: 137 Loss: 1.02399480342865\n  Batch: 138 Loss: 1.0239543914794922\n  Batch: 139 Loss: 1.024207592010498\n  Batch: 140 Loss: 1.0203096866607666\n  Batch: 141 Loss: 0.9976252913475037\n  Batch: 142 Loss: 1.0146678686141968\n  Batch: 143 Loss: 1.0218461751937866\n  Batch: 144 Loss: 1.0222817659378052\n  Batch: 145 Loss: 0.9611026644706726\n  Batch: 146 Loss: 1.022661805152893\n  Batch: 147 Loss: 0.9835110902786255\n  Batch: 148 Loss: 1.023632287979126\n  Batch: 149 Loss: 1.020623803138733\n  Batch: 150 Loss: 1.0210157632827759\n  Batch: 151 Loss: 0.9851289987564087\n  Batch: 152 Loss: 0.9937078952789307\n  Batch: 153 Loss: 1.0192455053329468\n  Batch: 154 Loss: 1.0236551761627197\n  Batch: 155 Loss: 1.0237621068954468\n  Batch: 156 Loss: 1.0220422744750977\n  Batch: 157 Loss: 1.022922158241272\n  Batch: 158 Loss: 1.020950198173523\n  Batch: 159 Loss: 1.0212552547454834\n  Batch: 160 Loss: 1.0209388732910156\n  Batch: 161 Loss: 1.0196727514266968\n  Batch: 162 Loss: 1.0215797424316406\n  Batch: 163 Loss: 1.0209846496582031\n  Batch: 164 Loss: 1.0237661600112915\n  Batch: 165 Loss: 1.0237008333206177\n  Batch: 166 Loss: 1.0223479270935059\n  Batch: 167 Loss: 1.0215535163879395\n  Batch: 168 Loss: 1.0129894018173218\n  Batch: 169 Loss: 1.020782470703125\n  Batch: 170 Loss: 1.0222928524017334\n  Batch: 171 Loss: 1.0214521884918213\n  Batch: 172 Loss: 1.0173699855804443\n  Batch: 173 Loss: 0.9882106184959412\n  Batch: 174 Loss: 1.0206300020217896\n  Batch: 175 Loss: 1.0117253065109253\n  Batch: 176 Loss: 1.0214043855667114\n  Batch: 177 Loss: 1.0225814580917358\n  Batch: 178 Loss: 1.0178472995758057\n  Batch: 179 Loss: 1.0207901000976562\n  Batch: 180 Loss: 1.0313498973846436\n  Batch: 181 Loss: 1.0213667154312134\n  Batch: 182 Loss: 1.0210959911346436\n  Batch: 183 Loss: 1.022297978401184\n  Batch: 184 Loss: 1.020429253578186\n  Batch: 185 Loss: 1.0205005407333374\n  Batch: 186 Loss: 1.0209269523620605\n  Batch: 187 Loss: 1.020084023475647\n  Batch: 188 Loss: 1.0135300159454346\n  Batch: 189 Loss: 1.0203399658203125\n  Batch: 190 Loss: 1.020286202430725\n  Batch: 191 Loss: 1.0203948020935059\n  Batch: 192 Loss: 1.0203251838684082\n  Batch: 193 Loss: 1.0090340375900269\n  Batch: 194 Loss: 0.9992861747741699\n  Batch: 195 Loss: 1.0201401710510254\n  Batch: 196 Loss: 1.0213496685028076\n  Batch: 197 Loss: 1.0174368619918823\n  Batch: 198 Loss: 1.0207302570343018\n  Batch: 199 Loss: 0.9970215559005737\n  Batch: 200 Loss: 1.0204201936721802\n  Batch: 201 Loss: 1.0217736959457397\n  Batch: 202 Loss: 1.0203818082809448\n  Batch: 203 Loss: 1.0210461616516113\n  Batch: 204 Loss: 1.022487759590149\n  Batch: 205 Loss: 1.0203391313552856\n  Batch: 206 Loss: 1.0225555896759033\n  Batch: 207 Loss: 1.0213727951049805\n  Batch: 208 Loss: 1.0161607265472412\n  Batch: 209 Loss: 1.0188581943511963\n  Batch: 210 Loss: 1.0199111700057983\n  Batch: 211 Loss: 0.993216335773468\n  Batch: 212 Loss: 1.0205857753753662\n  Batch: 213 Loss: 1.0200005769729614\n  Batch: 214 Loss: 0.9983581900596619\n  Batch: 215 Loss: 1.0187801122665405\n  Batch: 216 Loss: 1.021665334701538\n  Batch: 217 Loss: 0.964114248752594\n  Batch: 218 Loss: 1.0156874656677246\n  Batch: 219 Loss: 1.0206942558288574\n  Batch: 220 Loss: 1.0035985708236694\n  Batch: 221 Loss: 1.02244234085083\n  Batch: 222 Loss: 1.0207178592681885\n  Batch: 223 Loss: 1.0153083801269531\n  Batch: 224 Loss: 1.0117394924163818\n  Batch: 225 Loss: 1.0229564905166626\n  Batch: 226 Loss: 0.9783433675765991\n  Batch: 227 Loss: 1.0237964391708374\n  Batch: 228 Loss: 1.0064018964767456\n  Batch: 229 Loss: 1.0191882848739624\n  Batch: 230 Loss: 1.0110318660736084\n  Batch: 231 Loss: 1.0028678178787231\n  Batch: 232 Loss: 0.9883627891540527\n  Batch: 233 Loss: 1.019712209701538\n  Batch: 234 Loss: 1.020352840423584\n  Batch: 235 Loss: 0.9948859810829163\n  Batch: 236 Loss: 0.9307643175125122\n  Batch: 237 Loss: 1.0159980058670044\n  Batch: 238 Loss: 1.0206501483917236\n  Batch: 239 Loss: 1.0209107398986816\n  Batch: 240 Loss: 1.0206997394561768\n  Batch: 241 Loss: 1.0081021785736084\n  Batch: 242 Loss: 1.0204567909240723\n  Batch: 243 Loss: 1.019857406616211\n  Batch: 244 Loss: 0.9269064664840698\n  Batch: 245 Loss: 1.0202988386154175\n  Batch: 246 Loss: 1.0209320783615112\n  Batch: 247 Loss: 1.0129843950271606\n  Batch: 248 Loss: 1.025400996208191\n  Batch: 249 Loss: 1.0197056531906128\n  Batch: 250 Loss: 1.0254513025283813\n  Batch: 251 Loss: 1.0202585458755493\n  Batch: 252 Loss: 0.8645626306533813\n  Batch: 253 Loss: 1.019757628440857\n  Batch: 254 Loss: 0.982373058795929\n  Batch: 255 Loss: 1.0214532613754272\n  Batch: 256 Loss: 1.0202252864837646\n  Batch: 257 Loss: 1.0285207033157349\n  Batch: 258 Loss: 1.0284714698791504\n  Batch: 259 Loss: 1.022231936454773\n  Batch: 260 Loss: 1.014377474784851\n  Batch: 261 Loss: 1.0207912921905518\n  Batch: 262 Loss: 1.022692084312439\n  Batch: 263 Loss: 1.022515058517456\n  Batch: 264 Loss: 1.0216323137283325\n  Batch: 265 Loss: 1.000916838645935\n  Batch: 266 Loss: 0.9926598072052002\n  Batch: 267 Loss: 1.0171483755111694\n  Batch: 268 Loss: 1.0183367729187012\n  Batch: 269 Loss: 0.9957203269004822\n  Batch: 270 Loss: 0.9718303680419922\n  Batch: 271 Loss: 0.9765684604644775\n  Batch: 272 Loss: 0.9806255102157593\n  Batch: 273 Loss: 1.0135999917984009\n  Batch: 274 Loss: 1.020000696182251\n  Batch: 275 Loss: 1.0214256048202515\n  Batch: 276 Loss: 1.01835298538208\n  Batch: 277 Loss: 0.8795281648635864\n  Batch: 278 Loss: 0.963490903377533\n  Batch: 279 Loss: 1.0192168951034546\n  Batch: 280 Loss: 1.019461750984192\n  Batch: 281 Loss: 1.0207349061965942\n  Batch: 282 Loss: 1.0193394422531128\n  Batch: 283 Loss: 1.0193397998809814\n  Batch: 284 Loss: 1.0193277597427368\n  Batch: 285 Loss: 1.0190107822418213\n  Batch: 286 Loss: 1.0238434076309204\n  Batch: 287 Loss: 1.019584059715271\n  Batch: 288 Loss: 0.9808862209320068\n  Batch: 289 Loss: 1.0202209949493408\n  Batch: 290 Loss: 0.9966646432876587\n  Batch: 291 Loss: 1.0179178714752197\n  Batch: 292 Loss: 1.0205049514770508\n  Batch: 293 Loss: 1.0187158584594727\n  Batch: 294 Loss: 1.0053822994232178\n  Batch: 295 Loss: 1.018814206123352\n  Batch: 296 Loss: 1.021813988685608\n  Batch: 297 Loss: 1.0190746784210205\n  Batch: 298 Loss: 0.9845060110092163\n  Batch: 299 Loss: 1.0196969509124756\n  Batch: 300 Loss: 0.9700763821601868\n  Batch: 301 Loss: 1.0188181400299072\n  Batch: 302 Loss: 1.0193206071853638\n  Batch: 303 Loss: 1.0168145895004272\n  Batch: 304 Loss: 1.01047682762146\n  Batch: 305 Loss: 1.0186965465545654\n  Batch: 306 Loss: 1.0184385776519775\n  Batch: 307 Loss: 1.0187500715255737\n  Batch: 308 Loss: 1.0193182229995728\n  Batch: 309 Loss: 1.0190352201461792\n  Batch: 310 Loss: 1.0185630321502686\n  Batch: 311 Loss: 1.0185601711273193\n  Batch: 312 Loss: 1.0186306238174438\n  Batch: 313 Loss: 1.0187180042266846\n  Batch: 314 Loss: 1.0188641548156738\n  Batch: 315 Loss: 1.0181738138198853\n  Batch: 316 Loss: 1.0037188529968262\n  Batch: 317 Loss: 0.9657021164894104\n  Batch: 318 Loss: 0.9247298240661621\n  Batch: 319 Loss: 1.0307623147964478\n  Batch: 320 Loss: 1.0185794830322266\n  Batch: 321 Loss: 1.0180768966674805\n  Batch: 322 Loss: 1.0176723003387451\n  Batch: 323 Loss: 1.0157339572906494\n  Batch: 324 Loss: 1.0180996656417847\n  Batch: 325 Loss: 1.019171953201294\n  Batch: 326 Loss: 1.0294017791748047\n  Batch: 327 Loss: 1.0213885307312012\n  Batch: 328 Loss: 1.0188343524932861\n  Batch: 329 Loss: 0.9930530786514282\n  Batch: 330 Loss: 1.0185407400131226\n  Batch: 331 Loss: 0.9518154263496399\n  Batch: 332 Loss: 1.0260902643203735\n  Batch: 333 Loss: 1.0185163021087646\n  Batch: 334 Loss: 1.0239862203598022\n  Batch: 335 Loss: 1.0187060832977295\n  Batch: 336 Loss: 0.9741119742393494\nEpoch: 11 Loss: 1.0139907947963192\n  Batch: 1 Loss: 1.018035888671875\n  Batch: 2 Loss: 1.0185374021530151\n  Batch: 3 Loss: 1.0083917379379272\n  Batch: 4 Loss: 0.911089301109314\n  Batch: 5 Loss: 1.0179768800735474\n  Batch: 6 Loss: 1.0085716247558594\n  Batch: 7 Loss: 1.0180847644805908\n  Batch: 8 Loss: 1.0178320407867432\n  Batch: 9 Loss: 0.9980918765068054\n  Batch: 10 Loss: 1.0179539918899536\n  Batch: 11 Loss: 1.0240083932876587\n  Batch: 12 Loss: 0.9713101983070374\n  Batch: 13 Loss: 1.0196759700775146\n  Batch: 14 Loss: 1.0158904790878296\n  Batch: 15 Loss: 1.0195212364196777\n  Batch: 16 Loss: 1.0191986560821533\n  Batch: 17 Loss: 1.0179197788238525\n  Batch: 18 Loss: 1.018688678741455\n  Batch: 19 Loss: 0.9880507588386536\n  Batch: 20 Loss: 1.0199384689331055\n  Batch: 21 Loss: 1.0180628299713135\n  Batch: 22 Loss: 0.9884127378463745\n  Batch: 23 Loss: 1.017788290977478\n  Batch: 24 Loss: 1.0189623832702637\n  Batch: 25 Loss: 1.0166798830032349\n  Batch: 26 Loss: 1.0178817510604858\n  Batch: 27 Loss: 1.0202031135559082\n  Batch: 28 Loss: 0.980375349521637\n  Batch: 29 Loss: 1.0178062915802002\n  Batch: 30 Loss: 1.0048836469650269\n  Batch: 31 Loss: 1.0182703733444214\n  Batch: 32 Loss: 1.0178160667419434\n  Batch: 33 Loss: 1.0175572633743286\n  Batch: 34 Loss: 1.017533779144287\n  Batch: 35 Loss: 1.0177111625671387\n  Batch: 36 Loss: 1.0178685188293457\n  Batch: 37 Loss: 0.9542422890663147\n  Batch: 38 Loss: 1.0174466371536255\n  Batch: 39 Loss: 0.9483627676963806\n  Batch: 40 Loss: 1.0168675184249878\n  Batch: 41 Loss: 1.0173389911651611\n  Batch: 42 Loss: 1.017225980758667\n  Batch: 43 Loss: 1.0169707536697388\n  Batch: 44 Loss: 1.0173606872558594\n  Batch: 45 Loss: 1.0196818113327026\n  Batch: 46 Loss: 1.016425609588623\n  Batch: 47 Loss: 1.0174986124038696\n  Batch: 48 Loss: 1.0171393156051636\n  Batch: 49 Loss: 1.0173479318618774\n  Batch: 50 Loss: 1.0191845893859863\n  Batch: 51 Loss: 1.020293116569519\n  Batch: 52 Loss: 1.017861008644104\n  Batch: 53 Loss: 1.0173145532608032\n  Batch: 54 Loss: 1.0171277523040771\n  Batch: 55 Loss: 1.0376427173614502\n  Batch: 56 Loss: 0.9992198944091797\n  Batch: 57 Loss: 1.0097265243530273\n  Batch: 58 Loss: 1.0171815156936646\n  Batch: 59 Loss: 1.0152045488357544\n  Batch: 60 Loss: 1.0172395706176758\n  Batch: 61 Loss: 1.016924500465393\n  Batch: 62 Loss: 1.0171170234680176\n  Batch: 63 Loss: 0.9788139462471008\n  Batch: 64 Loss: 1.0023674964904785\n  Batch: 65 Loss: 1.0171253681182861\n  Batch: 66 Loss: 1.0174589157104492\n  Batch: 67 Loss: 1.016850471496582\n  Batch: 68 Loss: 0.9645493626594543\n  Batch: 69 Loss: 0.9405714273452759\n  Batch: 70 Loss: 1.0170966386795044\n  Batch: 71 Loss: 1.0005205869674683\n  Batch: 72 Loss: 1.0097434520721436\n  Batch: 73 Loss: 1.0175118446350098\n  Batch: 74 Loss: 1.0023356676101685\n  Batch: 75 Loss: 1.0171711444854736\n  Batch: 76 Loss: 1.0172232389450073\n  Batch: 77 Loss: 1.0176112651824951\n  Batch: 78 Loss: 0.9838608503341675\n  Batch: 79 Loss: 1.0150282382965088\n  Batch: 80 Loss: 1.0171785354614258\n  Batch: 81 Loss: 1.0165687799453735\n  Batch: 82 Loss: 1.0177992582321167\n  Batch: 83 Loss: 0.999634325504303\n  Batch: 84 Loss: 1.019210934638977\n  Batch: 85 Loss: 1.0278315544128418\n  Batch: 86 Loss: 1.0205307006835938\n  Batch: 87 Loss: 0.9777654409408569\n  Batch: 88 Loss: 1.019059181213379\n  Batch: 89 Loss: 1.0246177911758423\n  Batch: 90 Loss: 1.0167527198791504\n  Batch: 91 Loss: 1.0171422958374023\n  Batch: 92 Loss: 1.0203722715377808\n  Batch: 93 Loss: 1.016666054725647\n  Batch: 94 Loss: 0.9978361129760742\n  Batch: 95 Loss: 0.9465053677558899\n  Batch: 96 Loss: 1.0254656076431274\n  Batch: 97 Loss: 1.0174064636230469\n  Batch: 98 Loss: 1.0176234245300293\n  Batch: 99 Loss: 1.0170679092407227\n  Batch: 100 Loss: 1.0174211263656616\n  Batch: 101 Loss: 1.0170187950134277\n  Batch: 102 Loss: 1.0175434350967407\n  Batch: 103 Loss: 1.0167526006698608\n  Batch: 104 Loss: 1.0336098670959473\n  Batch: 105 Loss: 0.9515866041183472\n  Batch: 106 Loss: 1.0164859294891357\n  Batch: 107 Loss: 1.0166224241256714\n  Batch: 108 Loss: 1.024525761604309\n  Batch: 109 Loss: 1.0135982036590576\n  Batch: 110 Loss: 1.0187413692474365\n  Batch: 111 Loss: 1.0347871780395508\n  Batch: 112 Loss: 1.0179319381713867\n  Batch: 113 Loss: 1.024185061454773\n  Batch: 114 Loss: 1.062028169631958\n  Batch: 115 Loss: 1.0217278003692627\n  Batch: 116 Loss: 1.0168160200119019\n  Batch: 117 Loss: 1.0169854164123535\n  Batch: 118 Loss: 1.016736388206482\n  Batch: 119 Loss: 1.0166493654251099\n  Batch: 120 Loss: 1.020544171333313\n  Batch: 121 Loss: 1.0173081159591675\n  Batch: 122 Loss: 1.0203362703323364\n  Batch: 123 Loss: 1.0170336961746216\n  Batch: 124 Loss: 1.0180411338806152\n  Batch: 125 Loss: 1.017071008682251\n  Batch: 126 Loss: 1.0350974798202515\n  Batch: 127 Loss: 1.0174150466918945\n  Batch: 128 Loss: 1.0144710540771484\n  Batch: 129 Loss: 1.0170179605484009\n  Batch: 130 Loss: 1.0170254707336426\n  Batch: 131 Loss: 1.0171340703964233\n  Batch: 132 Loss: 1.0063273906707764\n  Batch: 133 Loss: 0.9998358488082886\n  Batch: 134 Loss: 1.0156354904174805\n  Batch: 135 Loss: 1.0172590017318726\n  Batch: 136 Loss: 1.016229510307312\n  Batch: 137 Loss: 1.0180469751358032\n  Batch: 138 Loss: 1.0169742107391357\n  Batch: 139 Loss: 1.0161916017532349\n  Batch: 140 Loss: 1.0165257453918457\n  Batch: 141 Loss: 1.0139459371566772\n  Batch: 142 Loss: 1.0161867141723633\n  Batch: 143 Loss: 0.9870413541793823\n  Batch: 144 Loss: 1.0162289142608643\n  Batch: 145 Loss: 1.01643967628479\n  Batch: 146 Loss: 1.0160679817199707\n  Batch: 147 Loss: 1.0161110162734985\n  Batch: 148 Loss: 0.9204807877540588\n  Batch: 149 Loss: 1.0160889625549316\n  Batch: 150 Loss: 1.0171384811401367\n  Batch: 151 Loss: 1.016268014907837\n  Batch: 152 Loss: 0.9797869920730591\n  Batch: 153 Loss: 1.016932487487793\n  Batch: 154 Loss: 1.003440260887146\n  Batch: 155 Loss: 1.0164780616760254\n  Batch: 156 Loss: 1.0159354209899902\n  Batch: 157 Loss: 1.0168906450271606\n  Batch: 158 Loss: 1.0167341232299805\n  Batch: 159 Loss: 1.0220940113067627\n  Batch: 160 Loss: 1.016586422920227\n  Batch: 161 Loss: 1.0159562826156616\n  Batch: 162 Loss: 1.0063562393188477\n  Batch: 163 Loss: 1.0166982412338257\n  Batch: 164 Loss: 1.0186128616333008\n  Batch: 165 Loss: 0.9816246032714844\n  Batch: 166 Loss: 1.0007320642471313\n  Batch: 167 Loss: 1.0123339891433716\n  Batch: 168 Loss: 1.0164234638214111\n  Batch: 169 Loss: 0.9555097818374634\n  Batch: 170 Loss: 0.9489349126815796\n  Batch: 171 Loss: 1.0155384540557861\n  Batch: 172 Loss: 1.0179412364959717\n  Batch: 173 Loss: 1.023808240890503\n  Batch: 174 Loss: 0.9996125102043152\n  Batch: 175 Loss: 1.0111854076385498\n  Batch: 176 Loss: 1.016061544418335\n  Batch: 177 Loss: 1.0160493850708008\n  Batch: 178 Loss: 1.004289984703064\n  Batch: 179 Loss: 1.004274606704712\n  Batch: 180 Loss: 1.020707607269287\n  Batch: 181 Loss: 1.016180157661438\n  Batch: 182 Loss: 1.0011612176895142\n  Batch: 183 Loss: 1.0183618068695068\n  Batch: 184 Loss: 1.0086944103240967\n  Batch: 185 Loss: 1.0159581899642944\n  Batch: 186 Loss: 1.0214942693710327\n  Batch: 187 Loss: 1.0081424713134766\n  Batch: 188 Loss: 1.0002433061599731\n  Batch: 189 Loss: 1.0170423984527588\n  Batch: 190 Loss: 1.0158675909042358\n  Batch: 191 Loss: 1.0155035257339478\n  Batch: 192 Loss: 0.9535419940948486\n  Batch: 193 Loss: 1.0154331922531128\n  Batch: 194 Loss: 1.0162041187286377\n  Batch: 195 Loss: 1.015913724899292\n  Batch: 196 Loss: 1.0127391815185547\n  Batch: 197 Loss: 1.0153101682662964\n  Batch: 198 Loss: 1.013797640800476\n  Batch: 199 Loss: 1.0172945261001587\n  Batch: 200 Loss: 0.9430328607559204\n  Batch: 201 Loss: 0.9823533296585083\n  Batch: 202 Loss: 1.0146424770355225\n  Batch: 203 Loss: 1.0145288705825806\n  Batch: 204 Loss: 1.0165691375732422\n  Batch: 205 Loss: 1.015876054763794\n  Batch: 206 Loss: 1.0080790519714355\n  Batch: 207 Loss: 1.0158584117889404\n  Batch: 208 Loss: 1.0164289474487305\n  Batch: 209 Loss: 0.9854655861854553\n  Batch: 210 Loss: 1.0164189338684082\n  Batch: 211 Loss: 1.0158227682113647\n  Batch: 212 Loss: 1.0157945156097412\n  Batch: 213 Loss: 0.9940060973167419\n  Batch: 214 Loss: 1.0177347660064697\n  Batch: 215 Loss: 1.0179262161254883\n  Batch: 216 Loss: 1.0170702934265137\n  Batch: 217 Loss: 1.0130879878997803\n  Batch: 218 Loss: 1.0168205499649048\n  Batch: 219 Loss: 1.0161970853805542\n  Batch: 220 Loss: 0.985565185546875\n  Batch: 221 Loss: 1.0159571170806885\n  Batch: 222 Loss: 1.0154308080673218\n  Batch: 223 Loss: 1.0164178609848022\n  Batch: 224 Loss: 1.0154106616973877\n  Batch: 225 Loss: 0.9773293733596802\n  Batch: 226 Loss: 1.0016459226608276\n  Batch: 227 Loss: 0.9938119053840637\n  Batch: 228 Loss: 1.0155178308486938\n  Batch: 229 Loss: 1.0108786821365356\n  Batch: 230 Loss: 1.018560528755188\n  Batch: 231 Loss: 1.0162692070007324\n  Batch: 232 Loss: 1.015694499015808\n  Batch: 233 Loss: 1.0102895498275757\n  Batch: 234 Loss: 1.0150994062423706\n  Batch: 235 Loss: 1.0165611505508423\n  Batch: 236 Loss: 1.0151169300079346\n  Batch: 237 Loss: 0.9835981726646423\n  Batch: 238 Loss: 0.9137402772903442\n  Batch: 239 Loss: 1.0157719850540161\n  Batch: 240 Loss: 1.0160441398620605\n  Batch: 241 Loss: 1.0186107158660889\n  Batch: 242 Loss: 1.0269180536270142\n  Batch: 243 Loss: 0.8399782180786133\n  Batch: 244 Loss: 0.964022159576416\n  Batch: 245 Loss: 1.0213438272476196\n  Batch: 246 Loss: 1.0407495498657227\n  Batch: 247 Loss: 1.0138394832611084\n  Batch: 248 Loss: 1.0162098407745361\n  Batch: 249 Loss: 1.0132936239242554\n  Batch: 250 Loss: 1.000775933265686\n  Batch: 251 Loss: 0.9835559129714966\n  Batch: 252 Loss: 1.0166093111038208\n  Batch: 253 Loss: 1.0115609169006348\n  Batch: 254 Loss: 1.0170776844024658\n  Batch: 255 Loss: 1.0174275636672974\n  Batch: 256 Loss: 1.0152783393859863\n  Batch: 257 Loss: 1.0172252655029297\n  Batch: 258 Loss: 1.011597752571106\n  Batch: 259 Loss: 1.0155043601989746\n  Batch: 260 Loss: 1.0154632329940796\n  Batch: 261 Loss: 1.0262423753738403\n  Batch: 262 Loss: 1.0146256685256958\n  Batch: 263 Loss: 1.015052080154419\n  Batch: 264 Loss: 1.0119147300720215\n  Batch: 265 Loss: 0.8025855422019958\n  Batch: 266 Loss: 1.0027077198028564\n  Batch: 267 Loss: 1.0150598287582397\n  Batch: 268 Loss: 1.0150426626205444\n  Batch: 269 Loss: 1.0158264636993408\n  Batch: 270 Loss: 1.01500403881073\n  Batch: 271 Loss: 1.013056755065918\n  Batch: 272 Loss: 1.017324686050415\n  Batch: 273 Loss: 1.0180232524871826\n  Batch: 274 Loss: 1.0165449380874634\n  Batch: 275 Loss: 0.9543250203132629\n  Batch: 276 Loss: 0.976082980632782\n  Batch: 277 Loss: 0.9934388399124146\n  Batch: 278 Loss: 0.990827202796936\n  Batch: 279 Loss: 1.004931092262268\n  Batch: 280 Loss: 1.0180559158325195\n  Batch: 281 Loss: 1.017335057258606\n  Batch: 282 Loss: 1.0155072212219238\n  Batch: 283 Loss: 1.0164039134979248\n  Batch: 284 Loss: 1.0296989679336548\n  Batch: 285 Loss: 1.0183632373809814\n  Batch: 286 Loss: 1.0145723819732666\n  Batch: 287 Loss: 1.0152018070220947\n  Batch: 288 Loss: 1.0107336044311523\n  Batch: 289 Loss: 1.0107053518295288\n  Batch: 290 Loss: 0.998517632484436\n  Batch: 291 Loss: 1.0194042921066284\n  Batch: 292 Loss: 1.0151450634002686\n  Batch: 293 Loss: 1.0186221599578857\n  Batch: 294 Loss: 1.0147018432617188\n  Batch: 295 Loss: 1.0162687301635742\n  Batch: 296 Loss: 1.0143870115280151\n  Batch: 297 Loss: 1.0164129734039307\n  Batch: 298 Loss: 1.0133527517318726\n  Batch: 299 Loss: 1.014543056488037\n  Batch: 300 Loss: 0.933607816696167\n  Batch: 301 Loss: 1.0147730112075806\n  Batch: 302 Loss: 1.0009381771087646\n  Batch: 303 Loss: 1.015299916267395\n  Batch: 304 Loss: 0.9535558223724365\n  Batch: 305 Loss: 1.0145546197891235\n  Batch: 306 Loss: 0.9892524480819702\n  Batch: 307 Loss: 0.9353339076042175\n  Batch: 308 Loss: 0.9535204768180847\n  Batch: 309 Loss: 1.012133002281189\n  Batch: 310 Loss: 1.0308126211166382\n  Batch: 311 Loss: 0.9725581407546997\n  Batch: 312 Loss: 1.0147652626037598\n  Batch: 313 Loss: 0.982176661491394\n  Batch: 314 Loss: 1.0141671895980835\n  Batch: 315 Loss: 1.0167597532272339\n  Batch: 316 Loss: 0.9995079636573792\n  Batch: 317 Loss: 1.016149640083313\n  Batch: 318 Loss: 0.9872572422027588\n  Batch: 319 Loss: 1.0165653228759766\n  Batch: 320 Loss: 1.0042468309402466\n  Batch: 321 Loss: 1.0159592628479004\n  Batch: 322 Loss: 1.0150692462921143\n  Batch: 323 Loss: 0.9566903710365295\n  Batch: 324 Loss: 1.0143771171569824\n  Batch: 325 Loss: 1.013898253440857\n  Batch: 326 Loss: 1.014194369316101\n  Batch: 327 Loss: 1.0139553546905518\n  Batch: 328 Loss: 1.0150291919708252\n  Batch: 329 Loss: 0.9627493619918823\n  Batch: 330 Loss: 0.9923925399780273\n  Batch: 331 Loss: 1.0140188932418823\n  Batch: 332 Loss: 1.0028115510940552\n  Batch: 333 Loss: 0.9810997247695923\n  Batch: 334 Loss: 1.0059767961502075\n  Batch: 335 Loss: 0.9656996130943298\n  Batch: 336 Loss: 1.0145282745361328\nEpoch: 12 Loss: 1.0075639589201837\n  Batch: 1 Loss: 1.0142436027526855\n  Batch: 2 Loss: 1.0139992237091064\n  Batch: 3 Loss: 0.9760497808456421\n  Batch: 4 Loss: 1.0144498348236084\n  Batch: 5 Loss: 1.0143311023712158\n  Batch: 6 Loss: 1.0170115232467651\n  Batch: 7 Loss: 1.014428734779358\n  Batch: 8 Loss: 1.0143415927886963\n  Batch: 9 Loss: 1.0146496295928955\n  Batch: 10 Loss: 1.0143388509750366\n  Batch: 11 Loss: 1.014326572418213\n  Batch: 12 Loss: 1.0141220092773438\n  Batch: 13 Loss: 1.0138516426086426\n  Batch: 14 Loss: 1.014028549194336\n  Batch: 15 Loss: 1.0160642862319946\n  Batch: 16 Loss: 0.9755062460899353\n  Batch: 17 Loss: 1.0095925331115723\n  Batch: 18 Loss: 1.0023354291915894\n  Batch: 19 Loss: 1.0142626762390137\n  Batch: 20 Loss: 1.0139691829681396\n  Batch: 21 Loss: 1.0027878284454346\n  Batch: 22 Loss: 1.0035929679870605\n  Batch: 23 Loss: 1.0139405727386475\n  Batch: 24 Loss: 1.0137808322906494\n  Batch: 25 Loss: 0.9921358823776245\n  Batch: 26 Loss: 1.01372492313385\n  Batch: 27 Loss: 1.0152102708816528\n  Batch: 28 Loss: 1.014222264289856\n  Batch: 29 Loss: 0.9269981384277344\n  Batch: 30 Loss: 0.9494861960411072\n  Batch: 31 Loss: 0.9379876852035522\n  Batch: 32 Loss: 0.9841182827949524\n  Batch: 33 Loss: 1.0124987363815308\n  Batch: 34 Loss: 0.9851177930831909\n  Batch: 35 Loss: 1.0172662734985352\n  Batch: 36 Loss: 0.958159327507019\n  Batch: 37 Loss: 1.0054703950881958\n  Batch: 38 Loss: 1.0144869089126587\n  Batch: 39 Loss: 1.0142004489898682\n  Batch: 40 Loss: 1.010775089263916\n  Batch: 41 Loss: 1.014204502105713\n  Batch: 42 Loss: 1.0124918222427368\n  Batch: 43 Loss: 1.0146175622940063\n  Batch: 44 Loss: 1.0144145488739014\n  Batch: 45 Loss: 0.9943360090255737\n  Batch: 46 Loss: 0.9866058826446533\n  Batch: 47 Loss: 0.9868439435958862\n  Batch: 48 Loss: 1.0138590335845947\n  Batch: 49 Loss: 0.9838679432868958\n  Batch: 50 Loss: 0.9938346147537231\n  Batch: 51 Loss: 1.0155868530273438\n  Batch: 52 Loss: 0.9159709811210632\n  Batch: 53 Loss: 1.0366566181182861\n  Batch: 54 Loss: 0.9583145380020142\n  Batch: 55 Loss: 1.0066519975662231\n  Batch: 56 Loss: 1.013584852218628\n  Batch: 57 Loss: 0.9734286069869995\n  Batch: 58 Loss: 1.0135706663131714\n  Batch: 59 Loss: 1.009148120880127\n  Batch: 60 Loss: 0.9265206456184387\n  Batch: 61 Loss: 1.0142768621444702\n  Batch: 62 Loss: 1.013806939125061\n  Batch: 63 Loss: 1.0146147012710571\n  Batch: 64 Loss: 0.8910583853721619\n  Batch: 65 Loss: 1.0197519063949585\n  Batch: 66 Loss: 1.0146113634109497\n  Batch: 67 Loss: 0.9908596277236938\n  Batch: 68 Loss: 1.0153244733810425\n  Batch: 69 Loss: 1.0171020030975342\n  Batch: 70 Loss: 1.0081676244735718\n  Batch: 71 Loss: 0.9535725712776184\n  Batch: 72 Loss: 1.0128535032272339\n  Batch: 73 Loss: 1.0055640935897827\n  Batch: 74 Loss: 1.0060381889343262\n  Batch: 75 Loss: 1.0131231546401978\n  Batch: 76 Loss: 1.0078409910202026\n  Batch: 77 Loss: 1.0173598527908325\n  Batch: 78 Loss: 1.0139167308807373\n  Batch: 79 Loss: 1.01144278049469\n  Batch: 80 Loss: 1.0132286548614502\n  Batch: 81 Loss: 1.014451265335083\n  Batch: 82 Loss: 0.985289454460144\n  Batch: 83 Loss: 1.0144879817962646\n  Batch: 84 Loss: 1.0130836963653564\n  Batch: 85 Loss: 1.0129280090332031\n  Batch: 86 Loss: 1.0136394500732422\n  Batch: 87 Loss: 1.013135552406311\n  Batch: 88 Loss: 1.0151519775390625\n  Batch: 89 Loss: 0.9895699620246887\n  Batch: 90 Loss: 1.0004734992980957\n  Batch: 91 Loss: 1.0130501985549927\n  Batch: 92 Loss: 1.013546109199524\n  Batch: 93 Loss: 0.9882960319519043\n  Batch: 94 Loss: 1.0305451154708862\n  Batch: 95 Loss: 0.9854564666748047\n  Batch: 96 Loss: 1.0130895376205444\n  Batch: 97 Loss: 0.9763056039810181\n  Batch: 98 Loss: 1.0142953395843506\n  Batch: 99 Loss: 1.0119023323059082\n  Batch: 100 Loss: 1.0152060985565186\n  Batch: 101 Loss: 1.0132083892822266\n  Batch: 102 Loss: 1.0156081914901733\n  Batch: 103 Loss: 1.0151187181472778\n  Batch: 104 Loss: 1.0007151365280151\n  Batch: 105 Loss: 0.9509026408195496\n  Batch: 106 Loss: 1.0136176347732544\n  Batch: 107 Loss: 1.0129128694534302\n  Batch: 108 Loss: 1.0149184465408325\n  Batch: 109 Loss: 1.0162705183029175\n  Batch: 110 Loss: 1.0136339664459229\n  Batch: 111 Loss: 1.0131113529205322\n  Batch: 112 Loss: 1.0145587921142578\n  Batch: 113 Loss: 1.0126829147338867\n  Batch: 114 Loss: 1.0055205821990967\n  Batch: 115 Loss: 1.0128068923950195\n  Batch: 116 Loss: 1.0153509378433228\n  Batch: 117 Loss: 1.0057380199432373\n  Batch: 118 Loss: 1.0059664249420166\n  Batch: 119 Loss: 1.0104297399520874\n  Batch: 120 Loss: 1.0127495527267456\n  Batch: 121 Loss: 1.013260841369629\n  Batch: 122 Loss: 1.013208270072937\n  Batch: 123 Loss: 1.0104349851608276\n  Batch: 124 Loss: 1.0125956535339355\n  Batch: 125 Loss: 0.9940352439880371\n  Batch: 126 Loss: 1.0125774145126343\n  Batch: 127 Loss: 0.980846107006073\n  Batch: 128 Loss: 1.0127683877944946\n  Batch: 129 Loss: 1.00202476978302\n  Batch: 130 Loss: 1.0124455690383911\n  Batch: 131 Loss: 1.0086026191711426\n  Batch: 132 Loss: 1.0123213529586792\n  Batch: 133 Loss: 0.9793893098831177\n  Batch: 134 Loss: 0.8600032329559326\n  Batch: 135 Loss: 1.008673071861267\n  Batch: 136 Loss: 1.0129446983337402\n  Batch: 137 Loss: 0.9396288394927979\n  Batch: 138 Loss: 1.0127074718475342\n  Batch: 139 Loss: 1.0126005411148071\n  Batch: 140 Loss: 1.0114543437957764\n  Batch: 141 Loss: 1.003715991973877\n  Batch: 142 Loss: 1.0262584686279297\n  Batch: 143 Loss: 1.0143790245056152\n  Batch: 144 Loss: 1.012446641921997\n  Batch: 145 Loss: 1.0124603509902954\n  Batch: 146 Loss: 1.0109798908233643\n  Batch: 147 Loss: 0.993637204170227\n  Batch: 148 Loss: 1.0126299858093262\n  Batch: 149 Loss: 1.01253080368042\n  Batch: 150 Loss: 1.0122524499893188\n  Batch: 151 Loss: 1.0132229328155518\n  Batch: 152 Loss: 1.0154805183410645\n  Batch: 153 Loss: 1.0133599042892456\n  Batch: 154 Loss: 0.9706901907920837\n  Batch: 155 Loss: 0.9845198392868042\n  Batch: 156 Loss: 1.0125296115875244\n  Batch: 157 Loss: 1.0100651979446411\n  Batch: 158 Loss: 0.8809910416603088\n  Batch: 159 Loss: 1.0140410661697388\n  Batch: 160 Loss: 1.0133346319198608\n  Batch: 161 Loss: 1.0125865936279297\n  Batch: 162 Loss: 1.0123099088668823\n  Batch: 163 Loss: 1.0124181509017944\n  Batch: 164 Loss: 1.0097923278808594\n  Batch: 165 Loss: 1.0123883485794067\n  Batch: 166 Loss: 0.9969104528427124\n  Batch: 167 Loss: 1.0125455856323242\n  Batch: 168 Loss: 1.0121363401412964\n  Batch: 169 Loss: 1.011515498161316\n  Batch: 170 Loss: 0.918666422367096\n  Batch: 171 Loss: 1.0192605257034302\n  Batch: 172 Loss: 1.0121989250183105\n  Batch: 173 Loss: 1.0116679668426514\n  Batch: 174 Loss: 1.0120632648468018\n  Batch: 175 Loss: 1.012333631515503\n  Batch: 176 Loss: 1.0128141641616821\n  Batch: 177 Loss: 1.0043532848358154\n  Batch: 178 Loss: 0.9926645159721375\n  Batch: 179 Loss: 1.0121536254882812\n  Batch: 180 Loss: 1.0212222337722778\n  Batch: 181 Loss: 1.0050493478775024\n  Batch: 182 Loss: 1.0070942640304565\n  Batch: 183 Loss: 1.0081371068954468\n  Batch: 184 Loss: 1.0126162767410278\n  Batch: 185 Loss: 1.0013422966003418\n  Batch: 186 Loss: 1.0119552612304688\n  Batch: 187 Loss: 0.9964187145233154\n  Batch: 188 Loss: 1.0118842124938965\n  Batch: 189 Loss: 1.0119857788085938\n  Batch: 190 Loss: 1.0107367038726807\n  Batch: 191 Loss: 0.9938040375709534\n  Batch: 192 Loss: 1.0122427940368652\n  Batch: 193 Loss: 0.9836467504501343\n  Batch: 194 Loss: 1.0139261484146118\n  Batch: 195 Loss: 1.0118359327316284\n  Batch: 196 Loss: 1.0043470859527588\n  Batch: 197 Loss: 0.9740844964981079\n  Batch: 198 Loss: 1.0124143362045288\n  Batch: 199 Loss: 0.9166104197502136\n  Batch: 200 Loss: 0.8454201221466064\n  Batch: 201 Loss: 1.0116705894470215\n  Batch: 202 Loss: 0.870997428894043\n  Batch: 203 Loss: 1.0136852264404297\n  Batch: 204 Loss: 1.0132777690887451\n  Batch: 205 Loss: 0.9709485769271851\n  Batch: 206 Loss: 1.0152498483657837\n  Batch: 207 Loss: 1.012890338897705\n  Batch: 208 Loss: 1.0120805501937866\n  Batch: 209 Loss: 1.0123372077941895\n  Batch: 210 Loss: 1.012080192565918\n  Batch: 211 Loss: 1.0089513063430786\n  Batch: 212 Loss: 1.0121080875396729\n  Batch: 213 Loss: 1.0117892026901245\n  Batch: 214 Loss: 1.0133172273635864\n  Batch: 215 Loss: 1.0120564699172974\n  Batch: 216 Loss: 1.01176917552948\n  Batch: 217 Loss: 0.948563814163208\n  Batch: 218 Loss: 1.0118721723556519\n  Batch: 219 Loss: 1.01222562789917\n  Batch: 220 Loss: 1.0127593278884888\n  Batch: 221 Loss: 1.0020864009857178\n  Batch: 222 Loss: 1.0117290019989014\n  Batch: 223 Loss: 0.9802523851394653\n  Batch: 224 Loss: 1.0119141340255737\n  Batch: 225 Loss: 1.0154612064361572\n  Batch: 226 Loss: 0.9945718050003052\n  Batch: 227 Loss: 1.0221078395843506\n  Batch: 228 Loss: 1.0249887704849243\n  Batch: 229 Loss: 0.962084949016571\n  Batch: 230 Loss: 1.0083026885986328\n  Batch: 231 Loss: 1.0127789974212646\n  Batch: 232 Loss: 1.0120530128479004\n  Batch: 233 Loss: 1.0123361349105835\n  Batch: 234 Loss: 0.9319738149642944\n  Batch: 235 Loss: 1.004249095916748\n  Batch: 236 Loss: 1.0130871534347534\n  Batch: 237 Loss: 1.0114281177520752\n  Batch: 238 Loss: 1.0115858316421509\n  Batch: 239 Loss: 1.0136685371398926\n  Batch: 240 Loss: 0.7967848181724548\n  Batch: 241 Loss: 1.0124987363815308\n  Batch: 242 Loss: 0.9802216291427612\n  Batch: 243 Loss: 1.0116989612579346\n  Batch: 244 Loss: 1.0121991634368896\n  Batch: 245 Loss: 0.9890087842941284\n  Batch: 246 Loss: 0.9607012271881104\n  Batch: 247 Loss: 1.0119292736053467\n  Batch: 248 Loss: 1.0119500160217285\n  Batch: 249 Loss: 1.0122073888778687\n  Batch: 250 Loss: 0.9961904287338257\n  Batch: 251 Loss: 1.0121663808822632\n  Batch: 252 Loss: 1.010265588760376\n  Batch: 253 Loss: 1.0121105909347534\n  Batch: 254 Loss: 1.0115673542022705\n  Batch: 255 Loss: 1.0122981071472168\n  Batch: 256 Loss: 1.0118120908737183\n  Batch: 257 Loss: 1.0114952325820923\n  Batch: 258 Loss: 1.0072145462036133\n  Batch: 259 Loss: 1.012000322341919\n  Batch: 260 Loss: 1.0126842260360718\n  Batch: 261 Loss: 0.9874632954597473\n  Batch: 262 Loss: 0.9673143029212952\n  Batch: 263 Loss: 1.0121873617172241\n  Batch: 264 Loss: 1.0059301853179932\n  Batch: 265 Loss: 1.011004090309143\n  Batch: 266 Loss: 1.0112501382827759\n  Batch: 267 Loss: 1.011745572090149\n  Batch: 268 Loss: 1.0103596448898315\n  Batch: 269 Loss: 0.9866217970848083\n  Batch: 270 Loss: 0.9623963236808777\n  Batch: 271 Loss: 1.0116575956344604\n  Batch: 272 Loss: 1.0111435651779175\n  Batch: 273 Loss: 0.986400842666626\n  Batch: 274 Loss: 1.0112890005111694\n  Batch: 275 Loss: 1.0027027130126953\n  Batch: 276 Loss: 1.0112977027893066\n  Batch: 277 Loss: 1.0127533674240112\n  Batch: 278 Loss: 0.9526746273040771\n  Batch: 279 Loss: 0.9662215709686279\n  Batch: 280 Loss: 1.0110938549041748\n  Batch: 281 Loss: 1.01494300365448\n  Batch: 282 Loss: 1.0114282369613647\n  Batch: 283 Loss: 0.9845696687698364\n  Batch: 284 Loss: 1.009656548500061\n  Batch: 285 Loss: 1.0136456489562988\n  Batch: 286 Loss: 0.9959328174591064\n  Batch: 287 Loss: 0.9510967135429382\n  Batch: 288 Loss: 1.0143541097640991\n  Batch: 289 Loss: 1.0116544961929321\n  Batch: 290 Loss: 0.9349759221076965\n  Batch: 291 Loss: 1.0124529600143433\n  Batch: 292 Loss: 1.0127463340759277\n  Batch: 293 Loss: 0.9386324286460876\n  Batch: 294 Loss: 1.0032944679260254\n  Batch: 295 Loss: 1.011971354484558\n  Batch: 296 Loss: 0.9878532886505127\n  Batch: 297 Loss: 1.0115481615066528\n  Batch: 298 Loss: 1.0114158391952515\n  Batch: 299 Loss: 1.011378288269043\n  Batch: 300 Loss: 1.0111057758331299\n  Batch: 301 Loss: 1.011096477508545\n  Batch: 302 Loss: 1.0114505290985107\n  Batch: 303 Loss: 1.0157092809677124\n  Batch: 304 Loss: 1.0109833478927612\n  Batch: 305 Loss: 1.0112491846084595\n  Batch: 306 Loss: 1.0062772035598755\n  Batch: 307 Loss: 1.0110530853271484\n  Batch: 308 Loss: 1.010868787765503\n  Batch: 309 Loss: 1.010248064994812\n  Batch: 310 Loss: 1.0114259719848633\n  Batch: 311 Loss: 1.0393701791763306\n  Batch: 312 Loss: 1.010827660560608\n  Batch: 313 Loss: 1.010918140411377\n  Batch: 314 Loss: 1.0109879970550537\n  Batch: 315 Loss: 0.980932354927063\n  Batch: 316 Loss: 0.9988223314285278\n  Batch: 317 Loss: 1.0119316577911377\n  Batch: 318 Loss: 1.0109267234802246\n  Batch: 319 Loss: 0.9356127381324768\n  Batch: 320 Loss: 1.0023542642593384\n  Batch: 321 Loss: 1.0112762451171875\n  Batch: 322 Loss: 1.0318174362182617\n  Batch: 323 Loss: 0.9737605452537537\n  Batch: 324 Loss: 1.0113425254821777\n  Batch: 325 Loss: 1.0125213861465454\n  Batch: 326 Loss: 0.962872326374054\n  Batch: 327 Loss: 1.0123062133789062\n  Batch: 328 Loss: 0.9724956750869751\n  Batch: 329 Loss: 1.0101048946380615\n  Batch: 330 Loss: 1.0145643949508667\n  Batch: 331 Loss: 1.012558937072754\n  Batch: 332 Loss: 1.0131202936172485\n  Batch: 333 Loss: 0.966319739818573\n  Batch: 334 Loss: 1.011476755142212\n  Batch: 335 Loss: 1.00192391872406\n  Batch: 336 Loss: 1.0107009410858154\nEpoch: 13 Loss: 1.000824054791814\n  Batch: 1 Loss: 1.0019032955169678\n  Batch: 2 Loss: 1.011230707168579\n  Batch: 3 Loss: 1.0046223402023315\n  Batch: 4 Loss: 0.9613515138626099\n  Batch: 5 Loss: 1.0141257047653198\n  Batch: 6 Loss: 0.9842984676361084\n  Batch: 7 Loss: 1.0106340646743774\n  Batch: 8 Loss: 1.0104825496673584\n  Batch: 9 Loss: 1.0106531381607056\n  Batch: 10 Loss: 1.0140223503112793\n  Batch: 11 Loss: 0.9998416900634766\n  Batch: 12 Loss: 0.9903340935707092\n  Batch: 13 Loss: 1.0021237134933472\n  Batch: 14 Loss: 1.0022507905960083\n  Batch: 15 Loss: 1.0081367492675781\n  Batch: 16 Loss: 0.9804332852363586\n  Batch: 17 Loss: 0.9614238739013672\n  Batch: 18 Loss: 1.012134313583374\n  Batch: 19 Loss: 1.0131046772003174\n  Batch: 20 Loss: 1.0110218524932861\n  Batch: 21 Loss: 1.010703206062317\n  Batch: 22 Loss: 1.003365397453308\n  Batch: 23 Loss: 1.01114821434021\n  Batch: 24 Loss: 1.0012969970703125\n  Batch: 25 Loss: 1.0049023628234863\n  Batch: 26 Loss: 1.011169195175171\n  Batch: 27 Loss: 1.0102620124816895\n  Batch: 28 Loss: 1.0085899829864502\n  Batch: 29 Loss: 1.0105761289596558\n  Batch: 30 Loss: 1.0117721557617188\n  Batch: 31 Loss: 0.9609898328781128\n  Batch: 32 Loss: 1.0111494064331055\n  Batch: 33 Loss: 1.010661244392395\n  Batch: 34 Loss: 1.011559247970581\n  Batch: 35 Loss: 0.9820095896720886\n  Batch: 36 Loss: 1.0141746997833252\n  Batch: 37 Loss: 1.0122917890548706\n  Batch: 38 Loss: 0.967707097530365\n  Batch: 39 Loss: 1.0088132619857788\n  Batch: 40 Loss: 1.010951042175293\n  Batch: 41 Loss: 0.9739935398101807\n  Batch: 42 Loss: 1.0102592706680298\n  Batch: 43 Loss: 1.008680820465088\n  Batch: 44 Loss: 1.0110394954681396\n  Batch: 45 Loss: 0.9953470230102539\n  Batch: 46 Loss: 1.0105807781219482\n  Batch: 47 Loss: 1.0103373527526855\n  Batch: 48 Loss: 1.0040773153305054\n  Batch: 49 Loss: 1.010545015335083\n  Batch: 50 Loss: 0.9314854145050049\n  Batch: 51 Loss: 0.9691866040229797\n  Batch: 52 Loss: 1.010973334312439\n  Batch: 53 Loss: 1.0104188919067383\n  Batch: 54 Loss: 1.0106415748596191\n  Batch: 55 Loss: 1.010855793952942\n  Batch: 56 Loss: 0.9807857871055603\n  Batch: 57 Loss: 0.9666792154312134\n  Batch: 58 Loss: 1.0120590925216675\n  Batch: 59 Loss: 0.9787517786026001\n  Batch: 60 Loss: 1.0101878643035889\n  Batch: 61 Loss: 1.010971188545227\n  Batch: 62 Loss: 1.0105053186416626\n  Batch: 63 Loss: 1.0107686519622803\n  Batch: 64 Loss: 0.9717101454734802\n  Batch: 65 Loss: 1.0083261728286743\n  Batch: 66 Loss: 0.9521979689598083\n  Batch: 67 Loss: 1.0108633041381836\n  Batch: 68 Loss: 0.8891627192497253\n  Batch: 69 Loss: 1.0116424560546875\n  Batch: 70 Loss: 0.9710738658905029\n  Batch: 71 Loss: 1.0035662651062012\n  Batch: 72 Loss: 1.0104577541351318\n  Batch: 73 Loss: 1.0120190382003784\n  Batch: 74 Loss: 0.9697695374488831\n  Batch: 75 Loss: 1.0090677738189697\n  Batch: 76 Loss: 1.0087037086486816\n  Batch: 77 Loss: 1.0105434656143188\n  Batch: 78 Loss: 1.0105456113815308\n  Batch: 79 Loss: 1.0115903615951538\n  Batch: 80 Loss: 1.0102006196975708\n  Batch: 81 Loss: 1.012489914894104\n  Batch: 82 Loss: 0.9822198152542114\n  Batch: 83 Loss: 0.9883145093917847\n  Batch: 84 Loss: 0.9629810452461243\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/3229177980.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mtrain_loss_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'  Batch: {b+1} Loss: {loss.item()}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":17},{"id":"2027MWqpENuJ","cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.plot(range(13), train_losses[:13], label='Training Loss')\nplt.plot(range(13), test_losses[:13], label='Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Training and Validation Loss')\nplt.legend()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":489},"id":"2027MWqpENuJ","executionInfo":{"status":"ok","timestamp":1750918098955,"user_tz":300,"elapsed":720,"user":{"displayName":"Yazan Bakdash","userId":"15411906646133184411"}},"outputId":"f29bf24b-f336-41c6-f793-c23aba094567","trusted":true,"execution":{"iopub.status.busy":"2025-07-03T04:53:59.347683Z","iopub.execute_input":"2025-07-03T04:53:59.348227Z","iopub.status.idle":"2025-07-03T04:53:59.540705Z","shell.execute_reply.started":"2025-07-03T04:53:59.348205Z","shell.execute_reply":"2025-07-03T04:53:59.540036Z"}},"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"<matplotlib.legend.Legend at 0x78c09aa56550>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":29},{"id":"62a48586-ffb5-496b-9d63-30ca1d2b6e93","cell_type":"code","source":"def iou_score(preds, targets, threshold=0.5, eps=1e-6):\n    preds = torch.sigmoid(preds)\n    preds = (preds > threshold).float()\n\n    intersection = (preds * targets).sum(dim=(2, 3))\n    union = ((preds + targets) > 0).float().sum(dim=(2, 3))\n\n    iou = (intersection + eps) / (union + eps)\n    return iou.mean().item()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T04:52:26.177412Z","iopub.execute_input":"2025-07-03T04:52:26.177687Z","iopub.status.idle":"2025-07-03T04:52:26.182483Z","shell.execute_reply.started":"2025-07-03T04:52:26.177669Z","shell.execute_reply":"2025-07-03T04:52:26.181606Z"}},"outputs":[],"execution_count":24},{"id":"z7pDp6QeeDxT","cell_type":"code","source":"def dice_score(preds, targets, threshold=0.5, eps=1e-6):\n    preds = torch.sigmoid(preds)\n    preds = (preds > threshold).float()\n\n    intersection = (preds * targets).sum(dim=(2, 3))\n    union = preds.sum(dim=(2, 3)) + targets.sum(dim=(2, 3))\n\n    dice = (2 * intersection + eps) / (union + eps)\n    return dice.mean().item()\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z7pDp6QeeDxT","executionInfo":{"status":"ok","timestamp":1750918114366,"user_tz":300,"elapsed":493,"user":{"displayName":"Yazan Bakdash","userId":"15411906646133184411"}},"outputId":"2df136e8-9f11-4946-87b5-d9a97734edd1","trusted":true,"execution":{"iopub.status.busy":"2025-07-03T04:52:28.139892Z","iopub.execute_input":"2025-07-03T04:52:28.140566Z","iopub.status.idle":"2025-07-03T04:52:28.144763Z","shell.execute_reply.started":"2025-07-03T04:52:28.140545Z","shell.execute_reply":"2025-07-03T04:52:28.143946Z"}},"outputs":[],"execution_count":25},{"id":"ed21cf47-8d72-4a2b-ba78-6dcb76c7d848","cell_type":"code","source":"model.eval()\nwith torch.no_grad():\n    iou_total = 0\n    dice_total = 0\n    for img_batch, mask_batch, _ in test_loader:\n        img_batch = img_batch.to(device)\n        mask_batch = mask_batch.to(device)\n\n        preds = model(img_batch)\n\n        iou_total += iou_score(preds, mask_batch)\n        dice_total += dice_score(preds, mask_batch)\n\n    print(\"Mean IoU:\", iou_total / len(test_loader))\n    print(\"Mean Dice Score:\", dice_total / len(test_loader))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T04:52:30.567552Z","iopub.execute_input":"2025-07-03T04:52:30.568081Z","iopub.status.idle":"2025-07-03T04:52:47.491804Z","shell.execute_reply.started":"2025-07-03T04:52:30.568062Z","shell.execute_reply":"2025-07-03T04:52:47.491074Z"}},"outputs":[{"name":"stdout","text":"Mean IoU: 0.8579969097461019\nMean Dice Score: 0.8624589102608817\n","output_type":"stream"}],"execution_count":26},{"id":"08605565-0897-4e83-a837-1fa124c72d2f","cell_type":"code","source":"# Save model weights\ntorch.save(model.state_dict(), \"/kaggle/working/unet3d_model.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T04:54:16.660991Z","iopub.execute_input":"2025-07-03T04:54:16.661433Z","iopub.status.idle":"2025-07-03T04:54:16.727165Z","shell.execute_reply.started":"2025-07-03T04:54:16.661410Z","shell.execute_reply":"2025-07-03T04:54:16.726634Z"}},"outputs":[],"execution_count":30},{"id":"88b0ec4e-8a75-49cb-8d68-4140b28f8096","cell_type":"code","source":"def sliding_window_inference(volume, model, patch_size=(128, 128, 128), stride=(64, 64, 64)):\n    model.eval()\n    device = next(model.parameters()).device\n    \n    D, H, W = volume.shape\n    C = 1  # assuming single-channel output\n\n    output = np.zeros((C, D, H, W), dtype=np.float32)\n    count_map = np.zeros((C, D, H, W), dtype=np.float32)\n\n    with torch.no_grad():\n        for z in range(0, D - patch_size[0] + 1, stride[0]):\n            for y in range(0, H - patch_size[1] + 1, stride[1]):\n                for x in range(0, W - patch_size[2] + 1, stride[2]):\n                    patch = volume[z:z+patch_size[0], y:y+patch_size[1], x:x+patch_size[2]]\n\n                    # normalize patch\n                    patch = (patch - patch.mean()) / (patch.std() + 1e-8)\n\n                    # predict\n                    input_tensor = torch.tensor(patch, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(device)\n                    pred = model(input_tensor)  # shape: [1, 1, D, H, W]\n                    pred = torch.sigmoid(pred).cpu().numpy()[0]\n\n                    output[:, z:z+patch_size[0], y:y+patch_size[1], x:x+patch_size[2]] += pred\n                    count_map[:, z:z+patch_size[0], y:y+patch_size[1], x:x+patch_size[2]] += 1\n\n    # Avoid divide-by-zero\n    count_map[count_map == 0] = 1\n    averaged_output = output / count_map\n    return averaged_output.squeeze()  # final prediction shape: (D, H, W)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T04:54:24.010623Z","iopub.execute_input":"2025-07-03T04:54:24.011193Z","iopub.status.idle":"2025-07-03T04:54:24.018327Z","shell.execute_reply.started":"2025-07-03T04:54:24.011173Z","shell.execute_reply":"2025-07-03T04:54:24.017511Z"}},"outputs":[],"execution_count":32},{"id":"d5f9b04f-87c2-49e4-b153-0ebf8fbeb294","cell_type":"code","source":"def evaluate_full_images(sample_dirs, model, threshold=0.5):\n    model.eval()\n    for sample_dir in sample_dirs:\n        image = np.load(sample_dir / \"image.npy\")  # (D, H, W)\n        mask = np.load(sample_dir / \"mask.npy\")    # (D, H, W)\n\n        # Normalize image\n        image_norm = (image - image.mean()) / (image.std() + 1e-8)\n\n        # Run patch-wise inference on full volume\n        pred = sliding_window_inference(image_norm, model, patch_size=(96, 96, 96), stride=(48, 48, 48))\n        pred = (pred > threshold).astype(float)\n\n        # Prepare for plotting\n        image_tensor = torch.tensor(image_norm).unsqueeze(0)  # [1, D, H, W]\n        mask_tensor = torch.tensor(mask).unsqueeze(0)\n        pred_tensor = torch.tensor(pred).unsqueeze(0)\n\n        plot_predictions_grid(\n            images=image_tensor.unsqueeze(0),    # [B, 1, D, H, W]\n            masks=mask_tensor.unsqueeze(0),\n            model=None,  # Not needed now\n            names=[sample_dir.name],\n            threshold=threshold\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T04:54:26.839776Z","iopub.execute_input":"2025-07-03T04:54:26.840013Z","iopub.status.idle":"2025-07-03T04:54:26.845793Z","shell.execute_reply.started":"2025-07-03T04:54:26.839997Z","shell.execute_reply":"2025-07-03T04:54:26.844990Z"}},"outputs":[],"execution_count":33},{"id":"15d8b267-e58f-41ac-96f1-8c6f63057fb2","cell_type":"code","source":"evaluate_full_images(test_dirs, model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T04:54:28.976158Z","iopub.execute_input":"2025-07-03T04:54:28.976921Z","iopub.status.idle":"2025-07-03T04:54:34.878247Z","shell.execute_reply.started":"2025-07-03T04:54:28.976897Z","shell.execute_reply":"2025-07-03T04:54:34.877536Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/1532164680.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mevaluate_full_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dirs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_35/338790148.py\u001b[0m in \u001b[0;36mevaluate_full_images\u001b[0;34m(sample_dirs, model, threshold)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mpred_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         plot_predictions_grid(\n\u001b[0m\u001b[1;32m     20\u001b[0m             \u001b[0mimages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m    \u001b[0;31m# [B, 1, D, H, W]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mmasks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'plot_predictions_grid' is not defined"],"ename":"NameError","evalue":"name 'plot_predictions_grid' is not defined","output_type":"error"}],"execution_count":34}]}